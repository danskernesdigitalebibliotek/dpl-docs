{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>This is the main documentation site for Danish Public Libraries (aka DPL) projects. These projects cover the software driving the website of (most of) Danish public libraries, and assorted projects to host and support the sites.</p> <p>This site contains general DPL and cross repository documentation and consolidates documentation from individual repositories.</p>"},{"location":"#major-components","title":"Major components","text":"<ul> <li>General   General and cross-repo documentation.</li> <li>DPL Design System Specifies the design used on   the sites, and contains a library of UI components implemented in   the CMS and React components.</li> <li>DPL CMS   The Drupal based CMS behind the sites.</li> <li>DPL Platform   The Lagoon and Kubernetes based hosting platform which hosts the production and development sites.</li> <li>DPL React   The React components used by the CMS.</li> <li>DPL Go   The library website focused on kids and teenagers.</li> </ul>"},{"location":"General/","title":"General","text":"<ul> <li>Documentation Site   How to add documentation to this site.</li> <li>DPL CMS release process   The steps needed to create a new release of the library sites.</li> <li>Extraordinary releases   Handling releases outside of the normal release process.</li> <li>Vendor updates   Handling updates to third party code used by our projects.</li> </ul>"},{"location":"General/documentation-site/","title":"Documentation Site","text":"<p>This project exists to consolidate and standardize documentation across DPL (Danish Public Libraries) repositories. It's based on mkdocs material and uses a handy multirepo-plugin to import markdown and build documentation from other DPL repositories dynamically.</p>"},{"location":"General/documentation-site/#adding-documentation","title":"Adding documentation","text":""},{"location":"General/documentation-site/#general-documentation","title":"General documentation","text":"<p>Documentation that isn't specific to any particular repository should be placed in this general section. </p>"},{"location":"General/documentation-site/#external-documentation","title":"External documentation","text":"<p>Any repository you want to add to the documentation site, must have a <code>docs/</code> folder in the project root on github and relevant markdown must be moved inside.</p> <p>If you wish to further divide your documentation into subfolders, mkdocs will accomodate this and build the site navigation accordingly. Here is an example:</p> <pre><code>docs/\n| README.md # Primary docs/index file for your navigation header\n\u2514\u2500\u2500\u2500folder1 # Left pane navigation category\n\u2502   \u2502 file01.md\n\u2502   \u2514\u2500\u2500\u2500subfolder1 # Expandable sub-navigation of folder1\n\u2502       \u2502  file02.md\n\u2502       \u2502  file03.md\n\u2514\u2500\u2500\u2500folder2 # Additional left pane navigation category\n    \u2502  file04.md\n</code></pre> <p>The <code>docs/</code> folder is self-contained, meaning that any relative links e.g. <code>\"../../\"</code> linking outside the docs/ directory, should use an absolute URL like: https://github.com/danskernesdigitalebibliotek/dpl-docs/blob/main/README.md</p> <p>Otherwise, following the link from the documentation site will cause a 404.</p>"},{"location":"General/documentation-site/#site-development","title":"Site development","text":"<p>The documentation project is hosted here.</p> <p>It uses a Github workflow to build and publish to github pages. Since documentation can be updated independently from other repos, a cron has been established to publish the site every 6 hours. While this covers most use cases, it's also possible to trigger it adhoc through Github actions with the workflow_dispatch event trigger.</p> <p>Customizations (colors, font, social links) and general site configuration can be configured in mkdocs.yml</p>"},{"location":"General/documentation-site/#requirements","title":"Requirements","text":"<ul> <li>mkdocs   material</li> <li>multirepo-plugin</li> </ul>"},{"location":"General/documentation-site/#docker-compose-setup","title":"Docker compose setup","text":"<p>For those not able or wanting to install Python packages on their machine, there's a rudimentary <code>docker-compose.yml</code> file that'll bring up a container running <code>mkdocs serve</code> on the usual HTTP port.</p>"},{"location":"General/extraordinary-releases/","title":"Extraordinary releases","text":"<p>Sometimes it may be necessary to create releases outside of the standard  release process. Typical situations involve critical security  updates or bug fixes. </p> <p>The important part of this work is that such releases only include a limited set of changes instead everything which has subsequently been </p>"},{"location":"General/extraordinary-releases/#creating-an-extraordinary-release","title":"Creating an extraordinary release","text":"<ol> <li>Identify the subproject for which to apply the change. This can be:</li> <li>Design system</li> <li>React components</li> <li>CMS</li> <li>Clone the project locally</li> <li>Identify the starting point for the extraordinary release. This will usually     be a previous release.</li> <li>Create a branch from the tag corresponding to the previous release. Replace     the patch part of the version with an <code>x</code>. This branch will be used for all    changes for the release. Example: <code>git checkout -b 2025.13.x 2025.13.0</code>.</li> <li>Push the branch.</li> <li>Create a new branch from this branch to contain an individual change.     Example: <code>git checkout -b important-change 2025.13.x</code></li> <li>Commit the necessary changes to this branch. This will often be in the form    of cherry-picked commits already applied to the <code>develop</code> branch of the    project.</li> <li>Push the new branch and create a pull request from the new branch against the    release branch.</li> <li>Wait for tests to complete and the pull request to be approved.</li> <li>Repeat steps 6-9 if the release should contain multiple changes.</li> <li>Create the release for the project<ol> <li>Enter the release version as \"Choose a tag\", and select to    create it on publish.</li> <li>Select the release branch as target.</li> <li>Click \"Generate release notes\" to automatically fill out title    and description.</li> </ol> </li> <li>Depending on the origin project for the extraordinary release follow through      with the release in the other projects using the same release name:<ol> <li>Design system: React components and CMS.</li> <li>React components: CMS only.</li> <li>CMS: No other projects.</li> </ol> </li> <li>Do not merge the release branch into <code>main</code> or <code>develop</code> branches for the     project.</li> </ol>"},{"location":"General/extraordinary-releases/#deploy-the-release","title":"Deploy the release","text":"<p>To deploy the release follow the standard deployment and annoncement process.</p>"},{"location":"General/release-process/","title":"Release process","text":"<p>Creating a new release involves creating a new release of the design system, the react components, the Go application and the CMS itself. It's important that it's done in that order, due to the dependencies between the projects.</p> <p>In the following you'll need to use the \"Bypass branch protection\" option when merging to skip having someone review the changes.</p>"},{"location":"General/release-process/#creating-the-release","title":"Creating the release","text":""},{"location":"General/release-process/#design-system","title":"Design system","text":"<ol> <li>Create a pull request from the <code>develop</code> branch against <code>main</code>.</li> <li>Wait for Github Actions to run and turn green.</li> <li>Merge the pull request.</li> <li>Create a new release<ol> <li>Enter the release version as \"Choose a tag\", and select to     create it on publish.</li> <li>Select <code>main</code> as target.</li> <li>Click \"Generate release notes\" to automatically fill out title     and description.</li> <li>Ensure that \"Set as the latest release\" is selected before     publishing the release.</li> </ol> </li> </ol>"},{"location":"General/release-process/#update-dpl-react-for-local-development","title":"Update DPL React for local development","text":"<p>These steps are not critical for release, but are required for developers to get the latest version of the design system when developing.</p> <p>In order to update the dependencies, you need to be authenticated with the GitHub NPM package repository.</p> <p>In https://github.com/danskernesdigitalebibliotek/dpl-react:</p> <ol> <li>Create a new branch locally from <code>develop</code> (remember to <code>git pull</code> first).</li> <li>Ensure you're using the right version of <code>node</code>. You can use <code>nvm    use</code> for this.</li> <li>Update the version of the design system used by running <code>yarn add    @danskernesdigitalebibliotek/dpl-design-system@[version]</code>, you can    find the latest version of the design system in the package on    Github    NOTE: You need to be certified for this.</li> <li>Commit and push the changes.</li> <li>Create a pull request from your branch against <code>develop</code>.</li> <li>Wait for actions to turn green.</li> <li>Merge the pull request.</li> </ol>"},{"location":"General/release-process/#dpl-react","title":"DPL React","text":"<ol> <li>Create a pull request from the <code>develop</code> against <code>main</code>.</li> <li>Wait for actions to go green.</li> <li>Merge pull request.</li> <li>Create a new    release    in the same way as with the Design system.</li> </ol>"},{"location":"General/release-process/#dpl-go","title":"DPL Go","text":"<ol> <li>Create a pull request from the <code>develop</code> against <code>main</code>.</li> <li>Wait for actions to go green.</li> <li>Merge pull request.</li> <li>Create a new    release    in the same way as with the Design system.</li> </ol>"},{"location":"General/release-process/#dpl-cms","title":"DPL CMS","text":"<ol> <li>Create a new branch locally from <code>develop</code> (remember to <code>git pull</code> first).</li> <li>Merge <code>main</code> into your branch. (remember to <code>git pull</code> main first).</li> <li>Update the version of the Design System used: <code>RELEASE=[version]    VERSION=[version] task dev:composer:update-design-system</code>. Refer to    <code>task dev:composer:update-design-system --summary</code> if needed.</li> <li>Update the version of React components used: <code>RELEASE=[version]    VERSION=[version] task dev:composer:update-react</code></li> <li>Commit, push your branch and create a pull request against <code>develop</code>.</li> <li>Wait for actions to go green.</li> <li>Merge your pull request.</li> <li>Create a new pull request from <code>develop</code> against <code>main</code>.</li> <li>Wait for actions to go green.</li> <li>Merge your pull request.</li> <li>Create a new     release     in the same way as with the Design system.</li> </ol> <p>Note: One might wonder where the Go application is handled in this process. The answer to that is: Go is not being handled here. The Go version is set in the sites.yaml file in the DPl Platform project. So the Go version of each site is determined by its <code>go-release</code> property in <code>sites.yaml</code></p>"},{"location":"General/release-process/#deployment-to-staging-and-announcement","title":"Deployment to staging and announcement","text":"<p>After creating a release, the torch passes to the DDF team to test and approve the release before the final deployment to production.</p>"},{"location":"General/release-process/#deployment-to-staging","title":"Deployment to staging","text":"<p>In order to deploy the new release to staging so that DDF can test it, refer to Platform runbook. This will require access to Azure in order to run the deployment.</p>"},{"location":"General/release-process/#jira-and-communication","title":"JIRA and communication","text":"<p>That was the technical part, now you need to tell somebody about your shiny new release.</p> <p>Do the following for both the Hermod and Brahma projecs in Jira (this obviously requires Jira access).</p> <ol> <li>Rename the <code>upcoming</code> release to the released version.</li> <li>Mark the release as released.</li> <li>Create a new unrelased <code>upcoming</code> release if it doesn't happen automatically.</li> <li>When the release has been deployed to    staging, tell DDF by creating a new topic    in the the <code>#DDF+</code> Zulip channel with the release name in the    subject and links to the release on staging and the release notes    for the projects that has contributed to the release. Take a look    at past release announcements in Zulip for an    example</li> </ol>"},{"location":"General/release-process/#deployment-to-production","title":"Deployment to production","text":"<p>When DDF has tested and approved the new release, they will annouce it in the above mentioned topic.</p> <p>The deployment to production is mostly handled by the platform team, but it's occasionally taken care of by the development team.</p> <p>The procedure is described in the runbook.</p> <p>As the deployment takes some time, it's important to post updates to the topic to keep DDF in the loop. In particular:</p> <ul> <li>Who's doing what.</li> <li>When they're doing it (when starting, with occasional status   updates).</li> <li>What they're doing (deploying sites, redeploying failed deploys,   syncing moduletest sites).</li> <li>When they're interrupted (taking a pause to go home to carry on, for   instance).</li> <li>When it's done.</li> </ul> <p>See Release 2025.4.0 for an example.</p>"},{"location":"General/vendor-updates/","title":"Vendor updates","text":"<p>All DPL projects make use of libraries and other tools managed by third parties (vendors). This describes our general process for handling updates to these:</p> <ol> <li>Vendored code must be kept up to date.</li> <li>Updating vendored is a continual process which is carried out as a part of     the normal release cycle.</li> <li>We trust our automated tools to identify updates and locate problems with     updates to vendored code.</li> <li>We rely on our knowledge about the projects and vendors to identify updates    which require further scrutiny.</li> </ol> <p>Projects have a more specific process for managing vendor updates can provide further information as part of the project-specific documentation.</p>"},{"location":"General/vendor-updates/#dependabot","title":"Dependabot","text":"<p>We use Dependabot  to monitor vendor code and create pull requests in case there are updates.</p>"},{"location":"General/vendor-updates/#handling-pull-requests","title":"Handling pull requests","text":"<p>When handling a pull request created by Dependabot the developer handling the pull requests does the following:</p> <ol> <li>Consider the risk of the update. High risk updates are less    likely to be merged directly. We consider the following factors when     determining risk:</li> <li>Update level: Major releases are more risky than minor and patch       releases as they contain breaking or other significant changes.</li> <li>Impact: Updates to core frameworks of the project are more risky than       updates to code supporting single features.</li> <li>Usage: Updates to dependencies used during development are considered      less risky than updates used in production.</li> <li>Test coverage: Updates to dependencies used in areas which are covered       by tests are considered less risky. </li> <li>Check the status of the GitHub Actions workflows for the pull request. If    one of these fail then we never apply the update directly.</li> <li>If the person decides that the update is safe to merge they merge the pull    request directly.</li> <li>If the person decides that the update should not be applied directly they:</li> <li>Update the Dependabot configuration to ignore the dependency version       in question. If a new major version is failing we may still want to        include minor and patch releases.</li> <li>Create an issue in JIRA with the following information:<ul> <li>Type: Task</li> <li>Title: Opgrader [vendor project name] ([DDF project name])</li> <li>Description:<ul> <li>A short description of the library, the role it plays    within the project, why you do not think the update requires    additional scrutiny and a link til the pull request created by    Dependabot. All is Danish. This provides context to the business and   the person handling the update in the future. </li> <li>Reference to the added ignore configuration (4.1) which must be    deleted after the update.</li> </ul> </li> <li>Project: DDFNEXT</li> <li>Sprint: Opdateringer</li> </ul> </li> <li>Close the Dependabot pull request.</li> </ol>"},{"location":"General/vendor-updates/#configuration","title":"Configuration","text":"<p>We configure Dependabot with the following properties:</p> <ul> <li>Schedule: Vendors should be checked weekly at night between sunday and    monday. This ensures updates occur frequently and on a regular basis across   all projetcs.</li> <li>Limit: By default we set a high limit for number of pull requests. e.g. </li> <li>We prefer many smaller updates arriving as quickly as possible.    Dependabot will use a limit of 5 if no limit is provided.    For some projects creating many pull requests risk affecting other resources.    This may warrant a lower limit on pull requests and/or additional grouping.</li> <li>Groups</li> <li>In general keep groups to a minimum. This avoids creating pull requests     which cannot be merged directly due to a single vendor causing status     checks to fail.</li> <li>Use groups to ensure that dependencies that are usually updated in      conjunction (and usually by the same vendor) are handled appropriately      without creating multiple pull requests. Examples: <ul> <li><code>react</code>, <code>react-dom</code> and their related <code>@types</code> packages</li> <li><code>drupal/core</code>, <code>drupal/core-recommended</code> and    <code>drupal/core-composer-scaffold</code></li> </ul> </li> <li>Versioning strategy: The minimum version should always be set to the    latest version.   This prevents accidental downgrades.</li> </ul>"},{"location":"General/vendor-updates/#updating-a-dependency","title":"Updating a Dependency","text":"<p>If we update a dependency manually we do the following:</p> <ol> <li>Create a ticket in JIRA. If a ticket for the update has already been created    through our Dependabot workflow we move it to the current project. If not    then we create a new one with similar information as when it is created     from a Dependabot update</li> <li>Update the dependency using our standard development process. The pull     request must not contain unrelated changes to ensure the integrity of the     project.</li> <li>If the change is not considered risky then we    merge the pull request after internal review without further testing.</li> </ol>"},{"location":"DPL-CMS/","title":"DPL CMS Documentation","text":"<p>The documentation in this folder describes how to develop DPL CMS.</p> <p>The focus of the documentation is to inform developers of how to develop the CMS, and give some background behind the various architectural choices.</p>"},{"location":"DPL-CMS/#layout","title":"Layout","text":"<p>The documentation falls into two categories:</p> <p>The Markdown-files in this directory document the system as it. Eg. you can read about how to add a new entry to the core configuration.</p> <p>The ./architecture folder contains our Architectural Decision Records that describes the reasoning behind key architecture decisions. Consult these records if you need background on some part of the CMS, or plan on making any modifications to the architecture.</p> <p>As for the remaining files and directories</p> <ul> <li>./diagrams contains diagram files like draw.io or PlantUML and   rendered diagrams in png/svg format. See the section for details.</li> <li>./images this is just plain images used by documentation files.</li> <li>./Taskfile.yml a go-task Taskfile,   run <code>task</code> to list available tasks.</li> </ul>"},{"location":"DPL-CMS/#diagrams","title":"Diagrams","text":"<p>We strive to keep the diagrams and illustrations used in the documentation as maintainable as possible. A big part of this is our use of programmatic diagramming via PlantUML and Open Source based manual diagramming via diagrams.net (formerly known as draw.io).</p> <p>When a change has been made to a <code>*.puml</code> or <code>*.drawio</code> file, you should re-render the diagrams using the command <code>task render</code> and commit the result.</p>"},{"location":"DPL-CMS/api-development/","title":"API Development","text":"<p>We use the RESTful Web Services and OpenAPI REST Drupal modules to expose endpoints from Drupal as an API to be consumed by external parties.</p>"},{"location":"DPL-CMS/api-development/#howtos","title":"Howtos","text":""},{"location":"DPL-CMS/api-development/#create-a-new-endpoint","title":"Create a new endpoint","text":"<ol> <li>Implement a new REST resource plugin by extending    <code>Drupal\\rest\\Plugin\\ResourceBase</code> and annotating it with <code>@RestResource</code></li> <li>Describe <code>uri_paths</code>, <code>route_parameters</code> and <code>responses</code> in the annotation as    detailed as possible to create a strong specification.</li> <li>Install the REST UI module <code>drush pm-enable restui</code></li> <li>Enable and configure the new REST resource. It is important to use the    <code>dpl_login_user_token</code> authentication provider for all resources which will    be used by the frontend this will provide a library or user token by default.</li> <li>Inspect the updated OpenAPI specification at <code>/openapi/rest?_format=json</code> to    ensure looks as intended</li> <li>Run <code>task ci:openapi:validate</code> to validate the updated OpenAPI specification</li> <li>Run <code>task ci:openapi:download</code> to download the updated OpenAPI specification</li> <li>Uninstall the REST UI module <code>drush pm-uninstall restui</code></li> <li>Export the updated configuration <code>drush config-export</code></li> <li>Commit your changes including the updated configuration and <code>openapi.json</code></li> </ol>"},{"location":"DPL-CMS/bnf-import-export/","title":"BNF: Content import/export","text":"<p>The sharing of content in the BNF system uses GraphQL for the communication between sites.</p>"},{"location":"DPL-CMS/bnf-import-export/#extending-the-content-shared","title":"Extending the content shared","text":"<p>To add data (new fields, paragraphs or node types) is a four step process:</p> <ol> <li>Ensure that the GraphQL compose module exposes the data.</li> <li>Adding the data to the queried data.</li> <li>Running the code generator Sailor.</li> <li>Adding/modifying the mappers to add the data to created nodes.</li> </ol>"},{"location":"DPL-CMS/bnf-import-export/#adding-data-to-graphql","title":"Adding data to GraphQL","text":"<p>The BNF modules is using the GraphQL compose module to expose the content data. It can be administered through Drupal at at <code>/admin/config/graphql_compose</code>.</p> <p>Note that GraphQL compose doesn't handle non-exposed subtypes very well, so all paragraph types must be enabled to avoid fatal errors on queries that fetch a node that include the paragraph. if a paragraph type doesn't need to be synchronized, enable the paragraph, but not any of it's fields.</p>"},{"location":"DPL-CMS/bnf-import-export/#modifying-the-query","title":"Modifying the query","text":"<p>The GraphQL query used for synchronizing nodes is defined in <code>web/modules/custom/bnf/queries/node.graphql</code> file. This query selects all needed data on a node and is used to generate a PHP client used by the 'bnf' module.</p>"},{"location":"DPL-CMS/bnf-import-export/#running-sailor","title":"Running Sailor","text":"<p>The Sailor GraphQL client generator maintains its own copy of the GraphQL schema, so if the schema changes (and occasionally, for good measure), it needs to fetch a fresh copy:</p> <pre><code>task dev:bnfcli -- ./vendor/bin/sailor introspect\n</code></pre> <p>Then generate the client code from the schema and the query:</p> <pre><code>task dev:bnfcli -- ./vendor/bin/sailor\n</code></pre> <p>Sailor will update the files in the <code>Drupal\\bnf\\GraphQL</code> namespace.</p> <p>This is a good time to run the tests with</p> <pre><code>task dev:cli -- ./vendor/bin/phpunit --filter bnf\n</code></pre> <p>To ensure that existing tests still pass. Adding fields to existing entities is likely to make tests fail as the newly generated client will expect the new fields to be specified in the mocked response data the tests uses.</p>"},{"location":"DPL-CMS/bnf-import-export/#updating-the-mappers","title":"Updating the mappers","text":"<p>Each generated class has a corresponding mapper plugin which maps the response object into a proper Drupal equivalent, starting with the node and working inwards recursively.</p> <p>When working with the mapper classes, it's recommended to start out with adding tests that verify that the mapper sets the proper properties on the resultant entities, and use the tests to confirm that the mapper works as intended. These tests will ensure that any changes to the GraphQL schema in the future that breaks the synchronization doesn't go unnoticed.</p> <p>Don't forget to test the new mapping manually, as the mock objects only reflect what you think should be done, not what actually works.</p> <p>What needs to be done to synchronize some data depends on the nature of the data:</p> <p>Simple value fields like the title field is handled by the containing entity, and it's just a matter of making the right mappers <code>map()</code> function take it into account:</p> <pre><code>  public function map(ObjectLike $object): mixed {\n    ...\n    $node-&gt;set('title', $object-&gt;title);\n</code></pre> <p>More advanced fields might be represented by it's own object in the response, but doesn't really warrant it's own mapper. Formatted text fields, for instance. These should be handled in the containing entity too:</p> <pre><code>  public function map(ObjectLike $object): mixed {\n    ...\n    $paragraph-&gt;set('field_body', [\n      'value' =&gt; $object-&gt;body-&gt;value,\n      'format' =&gt; $object-&gt;body-&gt;format,\n    ]);\n</code></pre> <p>More advanced types, and paragraphs in particular, should be handled by creating a mapper for the type and then recursively map:</p> <pre><code>  public function map(ObjectLike $object): mixed {\n    ...\n    if ($object-&gt;paragraphs) {\n      $paragraphs = [];\n\n      foreach ($object-&gt;paragraphs as $paragraph) {\n        $paragraphs[] = $this-&gt;manager-&gt;map($paragraph);\n      }\n\n      $node-&gt;set('field_paragraphs', $paragraphs);\n    }\n</code></pre> <p>Look at the existing code for guidance.</p>"},{"location":"DPL-CMS/bnf/","title":"Bibliotekernes Nationale Formidling","text":"<p>Bibliotekernes Nationale Formidling (henceforth \"BNF\") is a national team handling sharing of content for libraries. The <code>bnf_server</code> and <code>bnf_client</code> modules support their work.</p>"},{"location":"DPL-CMS/bnf/#server-module","title":"Server module","text":"<p>The server module is enabled on the main BNF site, which acts as a hub for content sharing. The BNF team uses this site to create and edit content provided for the libraries.</p>"},{"location":"DPL-CMS/bnf/#client-module","title":"Client module","text":"<p>The client module is enabled on library sites and handles pushing and fetching content to/from the BNF site.</p>"},{"location":"DPL-CMS/caching/","title":"Caching","text":"<p>DPL-CMS relies on two levels of caching. Standard Drupal Core caching, and Varnish as an accelerating HTTP cache.</p>"},{"location":"DPL-CMS/caching/#drupal","title":"Drupal","text":"<p>The Drupal Core cache uses Redis as its storage backend. This takes the load off of the database-server that is typically shared with other sites.</p> <p>Further more, as we rely on Varnish for all caching of anonymous traffic, the core Internal Page Cache module has been disabled.</p>"},{"location":"DPL-CMS/caching/#varnish","title":"Varnish","text":"<p>Varnish uses the standard Drupal VCL from lagoon.</p> <p>The site is configured with the Varnish Purge module and configured with a cache-tags based purger that ensures that changes made to the site, is purged from Varnish instantly.</p> <p>The configuration follows the Lagoon best practices - reference the Lagoon documentation on Varnish for further details.</p>"},{"location":"DPL-CMS/code-guidelines/","title":"Code guidelines","text":"<p>The following guidelines describe best practices for developing code for the DPL CMS project. The guidelines should help achieve:</p> <ul> <li>A stable, secure and high quality foundation for building and maintaining   library websites</li> <li>Consistency across multiple developers participating in the project</li> <li>The best possible conditions for sharing modules between DPL CMS websites</li> <li>The best possible conditions for the individual DPL CMS website to customize   configuration and appearance</li> </ul> <p>Contributions to the core DPL CMS project will be reviewed by members of the Core team. These guidelines should inform contributors about what to expect in such a review. If a review comment cannot be traced back to one of these guidelines it indicates that the guidelines should be updated to ensure transparency.</p>"},{"location":"DPL-CMS/code-guidelines/#coding-standards","title":"Coding standards","text":"<p>The project follows the Drupal Coding Standards and best practices for all parts of the project: PHP, JavaScript and CSS. This makes the project recognizable for developers with experience from other Drupal projects. All developers are expected to make themselves familiar with these standards.</p> <p>The following lists significant areas where the project either intentionally expands or deviates from the official standards or areas which developers should be especially aware of.</p>"},{"location":"DPL-CMS/code-guidelines/#general","title":"General","text":"<ul> <li>The default language for all code and comments is English.</li> </ul>"},{"location":"DPL-CMS/code-guidelines/#php","title":"PHP","text":"<ul> <li>Code must be compatible with all currently available minor and major versions   of PHP from 8.0 and onwards. This is important when trying to ensure smooth   updates going forward. Note that this only applies to custom code.</li> <li>Code must be compatible with Drupal Best Practices as defined by the   Drupal Coder module</li> <li>Code must use types   to define function arguments, return values and class properties.</li> <li>Code must use strict typing.</li> </ul>"},{"location":"DPL-CMS/code-guidelines/#javascript","title":"JavaScript","text":"<ul> <li>All functionality exposed through JavaScript should use the   Drupal JavaScript API   and must be attached to the page using Drupal behaviors.</li> <li>All classes used for selectors in Javascript must be prefixed with <code>js-</code>.   Example: <code>&lt;div class=\"gallery js-gallery\"&gt;</code> - <code>.gallery</code> must only be used in   CSS, <code>js-gallery</code> must only be used in JS.</li> <li>Javascript should not affect classes that are not state-classes. State classes   such as <code>is-active</code>, <code>has-child</code> or similar are classes that can be used as   an interlink between JS and CSS.</li> </ul>"},{"location":"DPL-CMS/code-guidelines/#css","title":"CSS","text":"<ul> <li>Modules and themes should use SCSS (The project uses PostCSS   and PostCSS-SCSS). The Core system   will ensure that these are compiled to CSS files automatically as a part of   the development process.</li> <li>Class names should follow the Block-Element-Modifier architecture   (BEM). This rule does not apply to state classes.</li> <li>Components (blocks) should be isolated from each other. We aim for an   atomic frontend   where components should be able to stand alone. In practice, there will be   times where this is impossible, or where components can technically stand   alone, but will not make sense from a design perspective (e.g. putting a   gallery in a sidebar).</li> <li>Components should be technically isolated by having 1 component per scss file.   **As a general rule, you can have a file called <code>gallery.scss</code> which contains   <code>.gallery, .gallery__container, .gallery__*</code> and so on. Avoid referencing   other components when possible.</li> <li>All components/mixins/similar must be documented with a code comment. When you   create a new <code>component.scss</code>, there must be a comment at the top, describing   the purpose of the component.</li> <li>Avoid using auto-generated Drupal selectors such as <code>.pane-content</code>. Use   the Drupal theme system to write custom HTML and use precise, descriptive   class names. It is better to have several class names on the same element,   rather than reuse the same class name for several components.</li> <li>All \"magic\" numbers must be documented. If you need to make something e.g.   350 px, you preferably need to find the number using calculating from the   context (<code>$layout-width * 0.60</code>) or give it a descriptive variable name   (<code>$side-bar-width // 350px works well with the current $layout-width_</code>)</li> <li>Avoid using the parent selector (<code>.class &amp;</code>). The use of parent selector   results in complex deeply nested code which is very hard to maintain. There   are times where it makes sense, but for the most part it can and should be   avoided.</li> </ul>"},{"location":"DPL-CMS/code-guidelines/#naming","title":"Naming","text":""},{"location":"DPL-CMS/code-guidelines/#modules","title":"Modules","text":"<ul> <li>All modules written specifically for Ding3 must be prefixed with <code>dpl</code>.</li> <li>The <code>dpl</code> prefix is not required for modules which provide functionality deemed   relevant outside the DPL community and are intended for publication on   Drupal.org.</li> </ul>"},{"location":"DPL-CMS/code-guidelines/#files","title":"Files","text":"<p>Files provided by modules must be placed in the following folders and have the extensions defined here.</p> <ul> <li>General</li> <li><code>MODULENAME.*.yml</code></li> <li><code>MODULENAME.module</code></li> <li><code>MODULENAME.install</code></li> <li><code>templates/*.html.twig</code></li> <li>Classes, interfaces and traits</li> <li><code>src/**/*.php</code></li> <li>PHPUnit tests</li> <li><code>tests/**/*.php</code></li> <li>CSS</li> <li>If the module does not not use processing: <code>/css/COMPONENTNAME.css</code></li> <li>If the module uses preprocessing: <code>/scss/COMPONENTNAME.scss</code></li> <li>JavaScript</li> <li><code>js/*.js</code></li> <li>Images</li> <li><code>img/*.(png|jpeg|gif|svg)</code></li> </ul>"},{"location":"DPL-CMS/code-guidelines/#module-elements","title":"Module elements","text":"<p>Programmatic elements such as settings, state values and views modules must comply to a set of common guidelines.</p> <ul> <li>Machine names should be prefixed with the name of the module that is   responsible for managing the elements.</li> <li>Administrative titles, human readable names and descriptions should be   relatable to the module name.</li> </ul> <p>As there is no finite set of programmatic elements for a DPL CMS site these apply to all types unless explicitly specified.</p>"},{"location":"DPL-CMS/code-guidelines/#code-structure","title":"Code Structure","text":"<p>The project follows the code structure suggested by the drupal/recommended-project Composer template.</p> <p>Modules, themes etc. must be placed within the corresponding folder in this repository. If a module developed in relation to this project is of general purpose to the Drupal community it should be placed on Drupal.org and included as an external dependency.</p> <p>A module must provide all required code and resources for it to work on its own or through dependencies. This includes all configuration, theming, CSS, images and JavaScript libraries.</p> <p>All default configuration required for a module to function should be implemented using the Drupal configuration system and stored in the version control with the rest of the project source code.</p>"},{"location":"DPL-CMS/code-guidelines/#updating-modules","title":"Updating modules","text":"<p>If an existing module is expanded with updates to current functionality the default behavior must be the same as previous versions or as close to this as possible. This also includes new modules which replaces current modules.</p> <p>If an update does not provide a way to reuse existing content and/or configuration then the decision on whether to include the update resides with the business.</p>"},{"location":"DPL-CMS/code-guidelines/#altering-existing-modules","title":"Altering existing modules","text":"<p>Modules which alter or extend functionality provided by other modules should use appropriate methods for overriding these e.g. by implementing alter hooks or overriding dependencies.</p>"},{"location":"DPL-CMS/code-guidelines/#translations","title":"Translations","text":"<p>All interface text in modules must be in English. Localization of such texts must be handled using the Drupal translation API.</p> <p>All interface texts must be provided with a context. This supports separation between the same text used in different contexts. Unless explicitly stated otherwise the module machine name should be used as the context.</p>"},{"location":"DPL-CMS/code-guidelines/#third-party-code","title":"Third party code","text":"<p>The project uses package managers to handle code which is developed outside of the Core project repository. Such code must not be committed to the Core project repository.</p> <p>The project uses two package manages for this:</p> <ul> <li>Composer - primarily for managing PHP packages,   Drupal modules and other code libraries which are executed at runtime in the   production environment.</li> <li>Yarn - primarily for managing code needed to establish   the pipeline for managing frontend assets like linting, preprocessing and   optimization of JavaScript, CSS and images.</li> </ul> <p>When specifying third party package versions the project follows these guidelines:</p> <ul> <li>Use the ^ next significant release operator   for packages which follow semantic versioning.</li> <li>The version specified must be the latest known working and secure version. We   do not want accidental downgrades.</li> <li>We want to allow easy updates to all working releases within the same major   version.</li> <li>Packages which are not intended to be executed at runtime in the production   environment should be marked as development dependencies.</li> </ul>"},{"location":"DPL-CMS/code-guidelines/#altering-third-party-code","title":"Altering third party code","text":"<p>The project uses patches rather than forks to modify third party packages. This makes maintenance of modified packages easier and avoids a collection of forked repositories within the project.</p> <ul> <li>Use an appropriate method for the corresponding package manager for managing   the patch.</li> <li>Patches should be external by default. In rare cases it may be needed to   commit them as a part of the project.</li> <li>When providing a patch you must document the origin of the patch e.g. through   an url in a commit comment or preferably in the package manager configuration   for the project.</li> </ul>"},{"location":"DPL-CMS/code-guidelines/#error-handling-and-logging","title":"Error handling and logging","text":"<p>Code may return null or an empty array for empty results but must throw exceptions for signalling errors.</p> <p>When throwing an exception the exception must include a meaningful error message to make debugging easier. When rethrowing an exception then the original exception must be included to expose the full stack trace.</p> <p>When handling an exception code must either log the exception and continue execution or (re)throw the exception - not both. This avoids duplicate log content.</p> <p>Drupal modules must use the Logging API. When logging data the module must use its name as the logging channel and an appropriate logging level.</p> <p>Modules integrating with third party services must implement a Drupal setting for logging requests and responses and provide a way to enable and disable this at runtime using the administration interface. Sensitive information (such as passwords, CPR-numbers or the like) must be stripped or obfuscated in the logged data.</p>"},{"location":"DPL-CMS/code-guidelines/#code-comments","title":"Code comments","text":"<p>Code comments which describe what an implementation does should only be used for complex implementations usually consisting of multiple loops, conditional statements etc.</p> <p>Inline code comments should focus on why an unusual implementation has been implemented the way it is. This may include references to such things as business requirements, odd system behavior or browser inconsistencies.</p>"},{"location":"DPL-CMS/code-guidelines/#commit-messages","title":"Commit messages","text":"<p>Commit messages in the version control system help all developers understand the current state of the code base, how it has evolved and the context of each change. This is especially important for a project which is expected to have a long lifetime.</p> <p>Commit messages must follow these guidelines:</p> <ol> <li>Each line must not be more than 72 characters long</li> <li>The first line of your commit message (the subject) must contain a short    summary of the change. The subject should be kept around 50 characters long.</li> <li>The subject must be followed by a blank line</li> <li>Subsequent lines (the body) should explain what you have changed and why the    change is necessary. This provides context for other developers who have not    been part of the development process. The larger the change the more    description in the body is expected.</li> <li>If the commit is a result of an issue in a public issue tracker,    platform.dandigbib.dk, then the subject must start with the issue number   followed by a colon (:). If the commit is a result of a private issue tracker   then the issue id must be kept in the commit body.</li> </ol> <p>When creating a pull request the pull request description should not contain any information that is not already available in the commit messages.</p> <p>Developers are encouraged to read How to Write a Git Commit Message by Chris Beams.</p>"},{"location":"DPL-CMS/code-guidelines/#tool-support","title":"Tool support","text":"<p>The project aims to automate compliance checks as much as possible using static code analysis tools. This should make it easier for developers to check contributions before submitting them for review and thus make the review process easier.</p> <p>The following tools pay a key part here:</p> <ol> <li>PHP_Codesniffer with the    following rulesets:</li> <li>Drupal Coding Standards       as defined the Drupal Coder module</li> <li>RequireStrictTypesSniff       as defined by PHP_Codesniffer</li> <li>Eslint and Airbnb JavaScript coding standards    as defined by Drupal Core</li> <li>Prettier as defined by Drupal Core</li> <li>Stylelint with the following rulesets:</li> <li>As defined by Drupal Core</li> <li>BEM as defined by the stylelint-bem project</li> <li>Browsersupport as defined by the      stylelint-no-unsupported-browser-features project</li> <li>PHPStan with the following configuration:</li> <li>Analysis level 8 to support detection of missing types</li> <li>Drupal support as defined by the phpstan-drupal project</li> <li>Detection of deprecated code as defined by the phpstan-deprecation-rules project</li> </ol> <p>In general all tools must be able to run locally. This allows developers to get quick feedback on their work.</p> <p>Tools which provide automated fixes are preferred. This reduces the burden of keeping code compliant for developers.</p> <p>Code which is to be exempt from these standards must be marked accordingly in the codebase - usually through inline comments (Eslint, PHP Codesniffer). This must also include a human readable reasoning. This ensures that deviations do not affect future analysis and the Core project should always pass through static analysis.</p> <p>If there are discrepancies between the automated checks and the standards defined here then developers are encouraged to point this out so the automated checks or these standards can be updated accordingly.</p>"},{"location":"DPL-CMS/config-import/","title":"Configuration import","text":"<p>Setting up a new site for testing certain scenarios can be repetitive. To avoid this the project provides a module: DPL Config Import.  This module can be used to import configuration changes into the site and install/uninstall modules in a single step.</p> <p>The configuration changes are described in a YAML file with configuration entry keys and values as well as module ids to install or uninstall.</p>"},{"location":"DPL-CMS/config-import/#how-to-use","title":"How to use","text":"<ol> <li>Download the example file    that comes with the module.</li> <li>Edit it to set the different configuration values.</li> <li>Upload the file at <code>/admin/config/configuration/import</code></li> <li>Clear the cache.</li> </ol>"},{"location":"DPL-CMS/config-import/#how-it-is-parsed","title":"How it is parsed","text":"<p>The yaml file has two root elements <code>configuration</code> and <code>modules</code>.</p> <p>A basic file looks like this:</p> <pre><code>configuration:\n  # Add keys for configuration entries to set.\n  # Values will be merged with existing values.\n  system.site:\n    # Configuration values can be set directly\n    slogan: 'Imported by DPL config import'\n    # Nested configuration is also supported\n    page:\n      # All values in nested configuration must have a key. This is required to\n      # support numeric configuration keys.\n      403: '/user/login'\n\nmodules:\n  # Add module ids to install or uninstall\n  install:\n    - menu_ui\n  uninstall:\n    - redis\n</code></pre>"},{"location":"DPL-CMS/configuration-management/","title":"Configuration Management","text":"<p>We use the Configuration Ignore Auto module to manage configuration.</p> <p>In general all configuration is ignored except for configuration which should explicitly be managed by DPL CMS core.</p>"},{"location":"DPL-CMS/configuration-management/#background","title":"Background","text":"<p>Configuration management for DPL CMS is a complex issue. The complexity stems from the following factors:</p>"},{"location":"DPL-CMS/configuration-management/#site-types","title":"Site types","text":"<p>There are multiple types of DPL CMS sites all using the same code base:</p> <ol> <li>Developer (In Danish: Programm\u00f8r) sites where the library is entirely free    to work with the codebase for DPL CMS as they please for their site</li> <li>Webmaster sites where the library can install and    manage additional modules for their DPL CMS site</li> <li>Editor (In Danish: Redakt\u00f8r) sites where the library can configure their    site based on predefined configuration options provided by DPL CMS</li> <li>Core sites which are default versions of DPL CMS used for development and    testing purposes</li> </ol> <p>All these site types must support the following properties:</p> <ol> <li>It must be possible for system administrators to deploy new versions of    DPL CMS which may include changes to the site configuration</li> <li>It must be possible for libraries to configure their site based on the    options provided by their type site. This configuration must not be    overridden by new versions of DPL CMS.</li> </ol>"},{"location":"DPL-CMS/configuration-management/#configuration-types","title":"Configuration types","text":"<p>This can be split into different types of configuration:</p> <ol> <li>Core configuration: This is the configuration for the base installation of    DPL CMS which is shared across all sites. The configuration will be imported    on deployment to support central development of the system.</li> <li>Local configuration: This is the local configuration for the individual    site. The level of configuration depends on the site type but no matter the    type this configuration must not be overridden on deployment of new versions    of DPL CMS.</li> </ol>"},{"location":"DPL-CMS/configuration-management/#howtos","title":"Howtos","text":""},{"location":"DPL-CMS/configuration-management/#install-a-new-site-from-scratch","title":"Install a new site from scratch","text":"<ol> <li>Run <code>drush site-install --existing-config -y</code></li> </ol>"},{"location":"DPL-CMS/configuration-management/#add-new-core-configuration","title":"Add new core configuration","text":"<ol> <li>Create the relevant configuration through the administration interface</li> <li>Run <code>drush config-export -y</code></li> <li>Append the key for the configuration to    <code>config_ignore.settings.ignored_config_entities</code> with the <code>~</code> prefix</li> <li>Commit the new configuration files and the updated <code>config_ignore.settings</code>    file</li> </ol>"},{"location":"DPL-CMS/configuration-management/#update-existing-core-configuration","title":"Update existing core configuration","text":"<ol> <li>Update the relevant configuration through the administration interface</li> <li>Run <code>drush config-export -y</code></li> <li>Commit the updated configuration files</li> </ol> <p>NB: The keys for these configuration files should already be in <code>config_ignore.settings.ignored_config_entities</code>.</p>"},{"location":"DPL-CMS/configuration-management/#add-new-local-configuration","title":"Add new local configuration","text":"<ol> <li>Update the relevant configuration through the administration interface</li> <li>Run <code>drush config-export -y</code></li> <li>Commit the updated configuration files</li> </ol>"},{"location":"DPL-CMS/configuration-management/#enable-a-new-module","title":"Enable a new module","text":"<ol> <li>Add the module to the project code base or as a Composer dependency</li> <li>Create an update hook in the DPL CMS installation profile which enables the    module<sup>1</sup>. You may want to use <code>dpl_update.install</code>.</li> </ol> <pre><code>function dpl_update_update_9000() {\n   \\Drupal::service('module_installer')-&gt;install(['shortcut']);\n}\n</code></pre> <ol> <li>Run the update hook locally <code>drush updatedb -y</code></li> <li>Export configuration <code>drush config-export -y</code></li> <li>Commit the resulting changes to the site configuration, codebase and/or    Composer files</li> </ol>"},{"location":"DPL-CMS/configuration-management/#uninstall-a-existing-module","title":"Uninstall a existing module","text":"<ol> <li>Create an update hook in the DPL CMS installation profile which uninstalls    the module<sup>1</sup></li> </ol> <pre><code>function dpl_cms_update_9001() {\n   \\Drupal::service('module_installer')-&gt;uninstall(['shortcut']);\n}\n</code></pre> <ol> <li>Run the update hook locally <code>drush updatedb -y</code></li> <li>Commit the resulting changes to the site configuration</li> <li>Export configuration <code>drush config-export -y</code></li> <li>Plan for a future removal of code for the module</li> </ol>"},{"location":"DPL-CMS/configuration-management/#deploy-configuration-changes","title":"Deploy configuration changes","text":"<ol> <li>Run <code>drush deploy</code></li> </ol> <p>NB: It is important that the official Drupal deployment procedure is followed. Database updates must be executed before configuration is imported. Otherwise we risk ending up in a situation where the configuration contains references to modules which are not enabled.</p> <ol> <li> <p>Creating update hooks for modules is only necessary once we have sites running in production which will not be reinstalled. Until then it is OK to enable/uninstall modules as normal and committing changes to <code>core.extensions</code>.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"DPL-CMS/event-field-config/","title":"Event field configuration","text":"<p>We use the Recurring Events and Field Inheritance modules to manage events. This creates two fieldable entity types, <code>eventseries</code> and <code>eventinstance</code>, which we customize to our needs. We try to follow a structured approach when setting up setup of fields between the two entities.</p>"},{"location":"DPL-CMS/event-field-config/#howto","title":"Howto","text":""},{"location":"DPL-CMS/event-field-config/#create-a-new-event-field","title":"Create a new event field","text":"<ol> <li>Add a new field on the <code>eventseries</code> entity on <code>/admin/structure/events/series/types/eventseries_type/default/edit/fields</code></li> <li>Add a new field on the <code>eventinstance</code> entity on <code>/admin/structure/events/instance/types/eventinstance_type/default/edit/fields</code>.</li> <li>Reuse all configuration from the field on <code>eventseries</code> including type,    label, machine name, number of values etc.    Exception: The field on the <code>eventinstance</code> entity must not be    required. Otherwise the Fallback strategy will not work.</li> <li>Add field interitance from <code>eventseries</code> to <code>eventinstance</code>on <code>/admin/structure/field_inheritance</code>    with the label \"Event [field name]\" e.g. \"Event tags\" and the \"Fallback\"    inheritance strategy.</li> <li>Use <code>eventseries</code>, <code>default</code> and the machine name for the field as the    source.</li> <li>Use <code>eventinstance</code>, <code>default</code> and the machine name for the field as the    destination.</li> </ol>"},{"location":"DPL-CMS/event-field-config/#render-a-new-field","title":"Render a new field","text":"<ol> <li>Configure the display of the field for <code>eventseries</code> on <code>/admin/structure/events/series/types/eventseries_type/default/edit/display</code></li> <li>Configure the display of the field for <code>eventinstance</code> on <code>/admin/structure/events/instance/types/eventinstance_type/default/edit/display</code></li> <li>Rearrange the fields such that the base field is disabled and the inherited    field is displayed. This is necessary to display the inherited value from    the series if there is no value on the instance but avoid rendering the    value twice if the instance has a value.</li> <li>Configure the display for the inherited field on <code>eventinstance</code> in the same    way as the source field on <code>eventseries</code>.</li> <li>Implement a template for the field in the theme e.g. <code>field--field-tags.html.twig</code></li> <li>Create a template for the inherited field in the theme. Include the entity    type in the template name to clarify that this is used for event instances    e.g. <code>field--eventinstance--event-tags.html.twig</code>.</li> <li>If the field should work the same across series and instances then include    the series template in the instance template:    <code>{{ include('field--field-tags.html.twig') }}</code></li> </ol>"},{"location":"DPL-CMS/event-integration/","title":"Event integration","text":"<p>Events make up an important part of the overall activities the Danish Public Libraries. One business aspect of these events is ticketing. Municipalities in Denmark use different external vendors for handling this responsibility which includes functionalities such payment, keeping track of availability, validation, seating etc.</p> <p>On goal for libraries is to keep staff workflows as simple as possible and avoid duplicate data entry. To achieve this DPL CMS exposes data and functionality as a part of the public API of the system.</p>"},{"location":"DPL-CMS/event-integration/#data-synchronization","title":"Data synchronization","text":"<p>The public API for DPL CMS is documented through an OpenAPI 2.0 specification.</p> <p>The following flow diagram represents a suggested approach for synchronizing event data between DPL CMS and an external system.</p> <pre><code>sequenceDiagram\n  Actor EventParticipant\n  Participant DplCms\n  Participant ExternalSystem\n  ExternalSystem -&gt;&gt; DplCms: Retrieve all events\n  activate ExternalSystem\n  activate DplCms\n  DplCms -&gt;&gt; ExternalSystem: List of all publicly available events\n  deactivate DplCms\n  ExternalSystem -&gt;&gt; ExternalSystem: (Optional) Filter out any events that have not been marked as relevant (ticket_manager_relevance)\n  ExternalSystem -&gt;&gt; ExternalSystem: Identify new events by UUID and create them locally\n  ExternalSystem -&gt;&gt; DplCms: Update events with external urls\n  ExternalSystem -&gt;&gt; ExternalSystem: Identify existing events by UUID and update them locally\n  ExternalSystem -&gt;&gt; ExternalSystem: Identify local events with UUID which are&lt;br/&gt;not represented in the list and delete them locally\n  deactivate ExternalSystem\n  Note over DplCms,ExternalSystem: Time passes\n  EventParticipant --&gt;&gt; DplCms: View event\n  EventParticipant --&gt;&gt; DplCms: Purchase ticket\n  DplCms --&gt;&gt; EventParticipant: Refer to external url\n  EventParticipant --&gt;&gt; ExternalSystem: Purchase ticket\n  activate ExternalSystem\n  ExternalSystem --&gt;&gt; EventParticipant: Ticket\n  ExternalSystem -&gt;&gt; DplCms: Update event with state e.g. \"sold out\"\n  deactivate ExternalSystem</code></pre>"},{"location":"DPL-CMS/event-integration/#authentication","title":"Authentication","text":"<p>An external system which intends to integrate with events is setup in the same way as library staff. It is represented by a Drupal user and must be assigned an appropriate username, password and role by a local administrator for the library. This information must be communicated to the external system through other secure means.</p> <p>The external system must authenticate through HTTP basic auth using this information when updating events.</p>"},{"location":"DPL-CMS/event-integration/#api-versioning","title":"API versioning","text":"<p>Please read the related ADR for how we handle API versioning.</p>"},{"location":"DPL-CMS/example-content/","title":"Example content","text":"<p>We use the Default Content module to manage example content. Such content is typically used when setting up development and testing environments.</p> <p>All actual example content is stored with the DPL Example Content module.</p> <p>Usage of the module in this project is derived from the official documentation.</p>"},{"location":"DPL-CMS/example-content/#howtos","title":"Howtos","text":""},{"location":"DPL-CMS/example-content/#add-additional-default-content","title":"Add additional default content","text":"<ol> <li>Create the default content</li> <li>Determine the UUIDs for the entities which should be exported as default    content. The easiest way to do this is to enable the Devel module, view    the entity and go to the Devel tab.    </li> <li>Add the UUID (s) (and if necessary entity types) to the    <code>dpl_example_content.info.yml</code> file</li> <li>Export the entities by running <code>drush default-content:export-module dpl_example_content</code></li> <li>Commit the new files under <code>web/modules/custom/dpl_example_content</code></li> </ol>"},{"location":"DPL-CMS/example-content/#update-existing-default-content","title":"Update existing default content","text":"<ol> <li>Update existing content</li> <li>Export the entities by running <code>drush default-content:export-module dpl_example_content</code></li> <li>Remove references to and files from UUIDs which are no longer relevant.</li> <li>Commit updated files under <code>web/modules/custom/dpl_example_content</code></li> </ol>"},{"location":"DPL-CMS/lagoon-environments/","title":"Lagoon environments","text":"<p>We use the Lagoon application delivery platform to host environments for different stages of the DPL CMS project. Our Lagoon installation is managed by the DPL Platform project.</p> <p>One such type of environment is pull request environments. These environments are automatically created when a developer creates a pull request with a change against the project and allows developers and project owners to test the result before the change is accepted.</p>"},{"location":"DPL-CMS/lagoon-environments/#howtos","title":"Howtos","text":""},{"location":"DPL-CMS/lagoon-environments/#create-an-environment-for-a-pull-request","title":"Create an environment for a pull request","text":"<ol> <li>Create a pull request for the change on GitHub. The pull request must be    created from a branch in the same repository as the target branch.</li> <li>Wait for GitHub Actions related to Lagoon deployment to complete. Note: This    deployment process can take a while. Be patient.</li> <li>A link to the deployed environment is available in the section between pull    request activity and Actions</li> <li>The environment is deleted when the pull request is closed</li> </ol>"},{"location":"DPL-CMS/lagoon-environments/#access-the-administration-interface-for-a-pull-request-environment","title":"Access the administration interface for a pull request environment","text":"<p>Accessing the administration interface for a pull request environment may be needed to test certain functionalities. This can be achieved in two ways:</p>"},{"location":"DPL-CMS/lagoon-environments/#through-the-lagoon-administration-ui","title":"Through the Lagoon administration UI","text":"<ol> <li>Access the administration UI (see below)</li> <li>Go to the environment corresponding to the pull request number</li> <li>Go to the Task section for the environment</li> <li>Select the \"Generate login link [drush uli]\" task and click \"Run task\"</li> <li>Refresh the page to see the task in the task list and wait a bit</li> <li>Refresh the page to see the task complete</li> <li>Go to the task page</li> <li>The log output contains a one-time login link which can be used to access    the administration UI</li> </ol>"},{"location":"DPL-CMS/lagoon-environments/#through-the-lagoon-cli","title":"Through the Lagoon CLI","text":"<ol> <li>Run <code>task lagoon:drush:uli</code></li> <li>The log output contains a one-time login link which can be used to access    the administration UI</li> </ol>"},{"location":"DPL-CMS/lagoon-environments/#access-the-lagoon-administration-ui","title":"Access the Lagoon administration UI","text":"<ol> <li>Contact administrators of the DPL Platform Lagoon instance to apply for an    user account.</li> <li>Access the URL for the UI of the instance e.g https://ui.lagoon.dplplat01.dpl.reload.dk/</li> <li>Log in with your user account (see above)</li> <li>Go to the dpl-cms project</li> </ol>"},{"location":"DPL-CMS/lagoon-environments/#setup-the-lagoon-cli","title":"Setup the Lagoon CLI","text":"<ol> <li>Locate information about the Lagoon instance to use in the DPL Platform    documentation</li> <li>Access the URL for the UI of the instance</li> <li>Log in with your user account (see above)</li> <li>Go to the Settings page</li> <li>Add your SSH public key to your account</li> <li>Install the Lagoon CLI</li> <li>Configure the Lagoon CLI to use the instance:</li> </ol> <pre><code>lagoon config add \\\n  --lagoon [instance name e.g. \"dpl-platform\"] \\\n  --hostname [host to connect to with SSH] \\\n  --port [SSH port] \\\n  --graphql [url to GraphQL endpoint] \\\n  --ui [url to UI] \\\n</code></pre> <ol> <li>Verify the installation:</li> </ol> <pre><code>lagoon login --lagoon [instance name]\nlagoon whoami --lagoon [instance name]\n</code></pre> <ol> <li>Use the DPL Platform as your default Lagoon instance:</li> </ol> <pre><code>lagoon config default --lagoon [instance name]\n</code></pre>"},{"location":"DPL-CMS/lagoon-environments/#using-cron-in-pull-request-environments","title":"Using cron in pull request environments","text":"<p>The <code>.lagoon.yml</code> has an environments section where it is possible to control various settings. On root level you specify the environment you want to address (eg.: <code>main</code>). And on the sub level of that you can define the cron settings. The cron settings for the main branch looks (in the moment of this writing) like this:</p> <pre><code>environments:\n  main:\n    cronjobs:\n    - name: drush cron\n      schedule: \"M/15 * * * *\"\n      command: drush cron\n      service: cli\n</code></pre> <p>If you want to have cron running on a pull request environment, you have to make a similar block under the environment name of the PR. Example: In case you would have a PR with the number #135 it would look like this:</p> <pre><code>environments:\n  pr-135:\n    cronjobs:\n    - name: drush cron\n      schedule: \"M/15 * * * *\"\n      command: drush cron\n      service: cli\n</code></pre>"},{"location":"DPL-CMS/lagoon-environments/#workflow-with-cron-in-pull-request-environments","title":"Workflow with cron in pull request environments","text":"<p>This way of making sure cronb is running in the PR environments is a bit tedious but it follows the way Lagoon is handling it. A suggested workflow with it could be:</p> <ul> <li>Create PR with code changes as normally</li> <li>Write the <code>.lagoon.yml</code> configuration block connected to the current PR #</li> <li>When the PR has been approved you delete the configuration block again</li> </ul>"},{"location":"DPL-CMS/local-development/","title":"Local development","text":""},{"location":"DPL-CMS/local-development/#docker-setup","title":"Docker setup","text":""},{"location":"DPL-CMS/local-development/#requirements","title":"Requirements","text":"<p>In order to run local development you need:</p> <ul> <li><code>go-task</code></li> <li>Docker</li> <li>Preferably support for <code>VIRTUAL_HOST</code> environment variables for Docker   containers. Examples: Dory (OSX) or   <code>nginx-proxy</code>.</li> </ul>"},{"location":"DPL-CMS/local-development/#macos-and-docker","title":"MacOS and Docker","text":"<p>If you are using MacOS, you can use the standard \"Docker for Mac\" app, however there has been developers that have experienced it acting slow.</p> <p>Alternatively, you can use Orbstack, which is a direct replacement for Docker for Mac, but is optimized to run much faster.</p>"},{"location":"DPL-CMS/local-development/#docker-for-mac","title":"Docker for Mac","text":"<p>If you do end up using Docker for Mac, it is recommended to use VirtioFS on the mounted volumes  in docker-compose, to speed up the containers.</p> <p></p>"},{"location":"DPL-CMS/local-development/#configuration-of-https","title":"Configuration of https","text":"<p>Install mkcert on your host machine and generate and install a root certificate by running mkcert -install on your host machine (one time only).</p> <p>Mac users can run: <code>brew install mkcert</code> to install mkcert</p> <p>Then run:</p> <p><code>mkcert -install</code></p> <p>Mac users should then do (one time only):</p> <p>$ mkdir -p ~/.local/share &amp;&amp; ln -s \"$(mkcert -CAROOT)\" ~/.local/share</p>"},{"location":"DPL-CMS/local-development/#if-you-are-using-dory","title":"If you are using Dory","text":"<p>Update the ssl_certs_dir as described here:</p> <p>https://github.com/reload/drupal-apache-fpm?tab=readme-ov-file#using-autogenerated-certificates</p>"},{"location":"DPL-CMS/local-development/#howtos","title":"Howtos","text":""},{"location":"DPL-CMS/local-development/#enable-xdebug","title":"Enable XDebug","text":"<p>Prerequisites:</p> <ul> <li>An IDE with support for XDebug e.g. JetBrains PhpStorm</li> <li>Optionally: A browser extension to activate XDebug</li> </ul> <p>For performance reasons XDebug is disabled by default. It can be enabled temporarily through a task:</p> <ol> <li>Run <code>task dev:enable-xdebug</code></li> <li>Validate that XDebug is enabled by inspecting http://dpl-cms.docker/admin/reports/status/php.    It should contain extended information about XDebug</li> <li>Debug the application by setting breakpoints, listen for incoming    connections in your IDE and activate XDebug from you client/browser</li> <li>When you are finished, hit <code>enter</code> in the terminal where you enabled XDebug.    This will disable XDebug</li> </ol>"},{"location":"DPL-CMS/local-development/#download-database-and-files-from-lagoon","title":"Download database and files from Lagoon","text":""},{"location":"DPL-CMS/local-development/#retrieve-the-latest-backup-of-database-and-files-from-lagoon","title":"Retrieve the latest backup of database and files from Lagoon","text":"<p>Prerequisites:</p> <ul> <li>A connected Lagoon CLI</li> <li><code>jq</code> installed locally</li> </ul> <p>Run the following command to retrieve the latest backup of database and files from a Lagoon project:</p> <pre><code>LAGOON_PROJECT=&lt;lagoon-project-name&gt; task lagoon:backup:restore\n</code></pre>"},{"location":"DPL-CMS/local-development/#copy-a-specific-database-snapshot-from-lagoon-environment-to-local-setup","title":"Copy a specific database snapshot from Lagoon environment to local setup","text":"<p>Prerequisites:</p> <ul> <li>Login credentials to the Lagoon UI, or an existing database dump</li> </ul> <p>The following describes how to first fetch a database-dump and then import the dump into a running local environment. Be aware that this only gives you the database, not any files from the site.</p> <ol> <li>To retrieve a database-dump from a running site, consult the    \"How do I download a database dump?\"    guide in the official Lagoon. Skip this step if you already have a    database-dump.</li> <li>Place the dump in the restore/database directory, be aware    that the directory is only allowed to contain a single <code>.sql</code> file.</li> <li>Start a local environment using <code>task dev:reset</code></li> <li>Import the database by running <code>task dev:restore:database</code></li> </ol>"},{"location":"DPL-CMS/local-development/#copy-a-specific-snapshot-of-files-from-lagoon-environment-to-local-setup","title":"Copy a specific snapshot of files from Lagoon environment to local setup","text":"<p>Prerequisites:</p> <ul> <li>Login credentials to the Lagoon UI, or an existing nginx files dump</li> </ul> <p>The following describes how to first fetch a files backup package and then replace the files in a local environment.</p> <p>If you need to get new backup files from the remote site:</p> <ol> <li>Login to the lagoon administration and navigate to the project/environment.</li> <li>Select the backup tab:</li> </ol> <p></p> <ol> <li>Retrieve the files backup you need:</li> </ol> <p> 4. Due to a UI bug you need to RELOAD the window and then it should be possible    to download the nginx package.</p> <p>Replace files locally:</p> <ol> <li>Place the files dump in the files-backup directory, be aware    that the directory is only allowed to contain a single <code>.tar.gz</code> file.</li> <li>Start a local environment using <code>task dev:reset</code></li> <li>Restore the files\u0161 by running <code>task dev:restore:files</code></li> </ol>"},{"location":"DPL-CMS/local-development/#get-a-specific-release-of-dpl-react-without-using-composer-install","title":"Get a specific release of dpl-react - without using composer install","text":"<p>In a development context it is not very handy only to be able to get the latest version of the main branch of dpl-react.</p> <p>So a command has been implemented that downloads the specific version of the assets and overwrites the existing library.</p> <p>You need to specify which branch you need to get the assets from. The latest HEAD of the given branch is automatically build by Github actions so you just need to specify the branch you want.</p> <p>It is used like this:</p> <pre><code>BRANCH=[BRANCH_FROM_DPL_REACT_REPOSITORY] task dev:dpl-react:overwrite\n</code></pre> <p>Example:</p> <pre><code>BRANCH=feature/more-releases task dev:dpl-react:overwrite\n</code></pre>"},{"location":"DPL-CMS/logging/","title":"Logging","text":"<p>We use logging to provide visibility into application behavior and related user activitiesm to supports effective troubleshooting and debugging. When an error or issue arises, logs provide the context and history to help identify what went wrong and where.</p>"},{"location":"DPL-CMS/logging/#architecture","title":"Architecture","text":"<p>We use the logging API provided by Drupal Core and rely on third party modules to expose log data to the underlying platform in a format that is appropriate for consumption by the platform.</p>"},{"location":"DPL-CMS/logging/#logged-events","title":"Logged events","text":"<p>We log the following events:</p> <ul> <li>Significant events occurring during the execution of the content management   system relating to usage and background events. Examples include:</li> <li>Scheduling of unpublication of events</li> <li>Renewal of security tokens</li> <li>Error conditions during the execution of the project codebase. Examples   include:</li> <li>Inability to retrieve data from external systems</li> <li>Invalid data provided by external systems</li> <li>Unexpected state of the local system</li> <li>Events triggered by Drupal Core and other third party modules used in the   system. Examples include:</li> <li>User logins</li> <li>Creation, editing and deletion of content</li> <li>Execution of background processes</li> </ul> <p>The architecture of the system, where self-service actions carried out by patrons is handled by JavaScript components running in the browser, means that searching for materials, management of reservations and updating patron information cannot be logged by the CMS.</p>"},{"location":"DPL-CMS/logging/#logged-data","title":"Logged data","text":"<p>Each logged event contains the following information by default:</p> <ul> <li>A message specified by the developer in the source code. Examples:</li> <li>\"Session closed for [editorial user]\"</li> <li>\"Finished processing scheduled jobs ([time spent] sec, [number of jobs]     total, [number of failures] failed)\"</li> <li>The log severity specified by the developer in the source code.</li> <li>The date and time the event occurred</li> <li>Context added by default by Drupal if available for the actor (end user   or external system) when the event occurred:</li> <li>The associated Drupal user account (anonymized for patrons)</li> <li>Url accessed</li> <li>Referring url</li> <li>IP address</li> </ul> <p>In general sensitive information (such as passwords, CPR-numbers or the like) must be stripped or obfuscated in the logged data. This is specified by our coding guidelines. It is the responsibility of developers to identify such issues during development and peer review.</p> <p>The architecture of the system severely limits the access to sensitive data during the execution of the project and thus reduces the general risk.</p>"},{"location":"DPL-CMS/logging/#log-severities","title":"Log severities","text":"<p>The system uses the eight log severities specified by PHP-FIG PSR-3 and Syslog RFC5424 as provided by the Drupal logging API.</p> <p>Events logged with the severity error or higher are monitored at the platform level and should be used with this in mind. Note that Drupal will log unchecked exceptions as this level by default.</p>"},{"location":"DPL-CMS/page-objects/","title":"Page objects in Cypress tests","text":"<p>When writing Cypress tests you're trying turn something very abstract (a wish to test something) into something very specific (the exact interactions with the browser DOM), so naturally a given test will contain a lot of implementation detail.</p> <p>However, in this process, the original intent of the test (\"test that the user can reserve a material\") gets replaced with the implementation details (\"click the button with id 'reserve-12649846' and check that the div with id 'messages' contains the text 'Reserved Harry Potter and the Goblet of Fire'\").</p> <p>This can make a test difficult to read, and it skips a level of abstraction humans are very familiar with (\"go to this page, click the reserve button\").</p> <p>The \"Page Object Model\" (commonly known as \"POM\") adds in this abstraction and promotes reuse across tests.</p>"},{"location":"DPL-CMS/page-objects/#anatomy-of-a-page-object","title":"Anatomy of a page object","text":"<p>A page object can consist of:</p> <ul> <li>Elements: More specific names for elements on the page. They allow   the page object to expose an element to the test (<code>headline()</code> for   instance), without exposing the specifics   (<code>div[data-type=headline]</code>). Be careful with elements that might   bleed implementation details, like a <code>&lt;select&gt;</code> element.</li> <li>Components: Like page objects for parts of pages. Useful for more   advanced UI components that might be shared across pages.</li> <li>Action and query methods: The interface to this page, basically   names \"what can you do\" and \"what can you see\" on this page.</li> </ul>"},{"location":"DPL-CMS/page-objects/#using-page-objects","title":"Using page objects","text":"<p>Basic usage of a page object is straightforward, a cut down example:</p> <pre><code>import { AdminModulesPage } from '../pages/admin-modules';\nimport { InstallOrUpdatePage } from '../pages/install-or-update';\n\ndescribe('Webmaster', () =&gt; {\n  it('can upload and enable a module', () =&gt; {\n    const installOrUpdatePage = new InstallOrUpdatePage();\n    installOrUpdatePage.visit([]);\n    installOrUpdatePage.uploadModule(\n      'cypress/fixtures/test_module/v1.0.0/test_module.tar.gz',\n    );\n\n    const adminModulesPage = new AdminModulesPage();\n    adminModulesPage.visit([]);\n    adminModulesPage.moduleExists('test_module').should('be.true');\n    adminModulesPage.moduleEnabled('test_module').should('be.false');\n  });\n});\n</code></pre> <p>After instantiating the page object, call <code>visit()</code> to make Cypress go to the page. Page objects always assume the browser is currently on the right page, which is why <code>visit()</code> is needed, but a page object can opt to return a new page object if doing an action that causes navigation in the browser.</p> <p>One of the most common pitfalls of page objects is confusion stemming from having a page object for one page, but the browser being on another page, so look out for that. If there's a good chance that this happens in a test, there's <code>page_object.assertIsOnPage()</code> that can assure the right page is current, but it's generally not needed as the test should fail anyway.</p> <p>The real interaction with the page is mostly done through \"action\" methods, like <code>installOrUpdatePage.uploadModule()</code> in the above example, or \"query\" methods like <code>adminModulesPage.moduleExists()</code>. Or for simpler pages, just using the elements.</p>"},{"location":"DPL-CMS/page-objects/#implementing-a-page-object","title":"Implementing a page object","text":"<p>Our page objects are based on cypress-page-object, which provides a good base class for page objects.</p> <p>An example (edited for brevity):</p> <pre><code>import { PageObject, Elements } from '@hammzj/cypress-page-object';\n\nexport class AdminModulesPage extends PageObject {\n  public elements: Elements;\n\n  constructor() {\n    super({ path: '/admin/modules' });\n    this.addElements = {\n      table: () =&gt; cy.get('table.module-list'),\n      submit: () =&gt; cy.findByRole('button', { name: /Install/i }),\n      moduleCheckbox: (module: string) =&gt;\n        this.elements.table().find(this.moduleCheckboxId(module)),\n    };\n  }\n\n  enableModule(module: string) {\n    this.elements.moduleCheckbox(module).check();\n    this.elements.submit().click();\n\n    return this;\n  }\n\n  /**\n   * Check if module exists.\n   *\n   * Yields true or false.\n   */\n  moduleExists(module: string) {\n    return this.elements\n      .table()\n      .then(($table) =&gt; $table.find(this.moduleCheckboxId(module)).length &gt; 0);\n  }\n\n  /**\n   * Get module checkbox HTML ID.\n   *\n   * Drupal munges the module name a bit in the ID attribute.\n   */\n  moduleCheckboxId(module: string): string {\n    return '#edit-modules-' + module.replace('_', '-') + '-enable';\n  }\n}\n</code></pre>"},{"location":"DPL-CMS/page-objects/#commentary","title":"Commentary","text":"<p>The constructor calls <code>super()</code> to tell the base page object the URL of the page. Placeholders can be used, consult the documentation for details.</p> <p>The constructor also registers the common elements. It is advisable to use the findByRole/findByLabelText/etc. functions added by testing-library to mimic how acutal users are using the page.</p> <p>The <code>enableModule()</code> method is an action. As demonstrated, using the page elements improves readability. Actions aren't required to return anything, but returning <code>this</code> or in the case of something that does navigation, a page object for the new page aids chain-ability.</p> <p>Keep an eye on the needs of the tests that'll end up using your actions. For instance, if enabling modules would send the user to a confirmation page, the most proper thing would be for <code>enableModule()</code> to return a page object for the confirmation page. But if the most usage of the action is to enable a module for testing of things unrelated to the module enabling flow, it would be more handy for <code>enableModule()</code> to just deal with the confirmation.</p> <p>Query methods like <code>moduleExists()</code> ought to return something chain-able, to keep with the Cypress style. In this case we're returning a boolean from <code>.then()</code>, which Cypress then turns into something you can call <code>.should('be.true');</code> on.</p>"},{"location":"DPL-CMS/page-objects/#general-tips","title":"General tips","text":"<p>When implementing page objects, think of how you'd instruct a knowledgeable user in using the page.</p> <p>While the idea of page objects is to be a general interface to a page, resist the temptation to implement actions/queries before you have a test that need them, but, as always, keep an eye on future extension.</p> <p>And don't get too hung up on doing the page object \"properly\". A funky, but working, page object is more valuable than a non-existing ideal. And when you have a passing test, you can use it to ensure that your refactoring of the page object still works as inteded.</p>"},{"location":"DPL-CMS/permissions-and-roles/","title":"Permissions and roles","text":"<p>'mediator':</p> <ul> <li>For users who frequently create dynamic content on the website.</li> <li>Can create and edit events, articles, pages, and campaigns in   /admin/content.</li> </ul> <p>'editor':</p> <ul> <li>For users who, in addition to content creation, are responsible for   static content (e.g., opening hours) and the site's structure.</li> <li>Can coordinate and manage the work of mediators.</li> <li>Has access to the same as mediator plus all content types in   /admin/content, as well as taxonomy and menus in /admin/structure.</li> </ul> <p>'local_administrator':</p> <ul> <li>For users who need to configure the website in terms of settings,   appearance, integrations, general information, library usage, etc.</li> <li>Has access to the same as editor plus the site's configuration   options in /admin/config, user creation in /admin/people, and   changing the site's appearance in /admin/appearance.</li> <li>This is the role assigned to trusted users on regular libraries.</li> </ul> <p>'administrator':</p> <ul> <li>Can modify the site beyond the CMS's standard settings, e.g., by   enabling and disabling modules.</li> <li>Has access to everything in the CMS.</li> <li>This is the role assigned to trusted users on webmaster libraries.</li> </ul>"},{"location":"DPL-CMS/releases/","title":"Releases","text":""},{"location":"DPL-CMS/releases/#building-and-publishing-releases","title":"Building and publishing releases","text":"<p>A release of dpl-cms can be build by pushing a tag that matches the following pattern:</p> <pre><code># Replace &lt;version&gt; with the version.\ngit tag &lt;version&gt;\n\n# Eg.\ngit tag 1.2.3\n</code></pre> <p>The actual release is performed by the <code>Publish source</code> Github action which invokes <code>task source:deploy</code>  which in turn uses the tasks <code>source:build</code> and <code>source:push</code> to build and publish the release.</p> <p>Using the action should be the preferred choice for building and publishing releases, but should you need to - it is possible to run the task manually given you have the necessary permissions for pushing the resulting source-image. Should you only need to produce the image, but not push it the task you can opt for just invoking the <code>source:build</code> task.</p> <p>You can override the name of the built image and/or the destination registry temporarily by providing a number of environment variables (see the Taskfile). To permanently change these configurations, eg. in a fork, change the defaults directly in the <code>Taskfile.yml</code>.</p>"},{"location":"DPL-CMS/translation/","title":"Translation","text":"<p>We manage translations as a part of the codebase using <code>.po</code> translation files. Consequently translations must be part of either official or local translations to take effect on the individual site.</p> <p>DPL CMS is configured to use English as the master language but is configured to use Danish for all users through language negotiation. This allows us to follow a process where English is the default for the codebase but actual usage of the system is in Danish.</p>"},{"location":"DPL-CMS/translation/#translation-system","title":"Translation system","text":"<p>To make the \"translation traffic\" work following components are being used:</p> <ul> <li>GitHub</li> <li>Stores <code>.po</code> files in git with     translatable strings and translations</li> <li>GitHub Actions</li> <li>Scans codebase for new translatable strings and commits them to GitHub</li> <li>Exports translatable configuration strings into a separate <code>*.config.po</code> file</li> <li>Merges the two files: <code>*.po</code> and <code>*.config.po</code> into a <code>*.combined.po</code> file</li> <li>Notifies POEditor that new translatable strings are available</li> <li>When a project is exported from POEditor:<ul> <li>The <code>*.combined.po</code> is split into two files: <code>*.po</code> and <code>*.config.po</code></li> <li>The <code>*.po</code> files are published to GitHub Pages</li> </ul> </li> <li>POEditor</li> <li>Provides an interface for translators</li> <li>Links translations with <code>.po</code> files on GitHub</li> <li>Provides webhooks where external systems can notify of new translations</li> <li>DPL CMS</li> <li>Drupal installation which is configured to use GitHub Pages as an interface     translation server from which <code>.po</code>     files can be consumed.</li> <li>In the development setup and in cronjobs defined in the environments there   are two jobs in charge of importing the regular translations   and the configuration translations.</li> </ul> <p>The following diagram show how these systems interact to support the flow of from introducing a new translateable string in the codebase to DPL CMS consuming an updated translation with said string.</p> <p>case</p> <pre><code>sequenceDiagram\n  Actor Translator\n  Actor Developer\n  Developer -&gt;&gt; Developer: Open pull request with new translatable string\n  Developer -&gt;&gt; GitHubActions: Merge pull request into develop\n  GitHubActions -&gt;&gt; GitHubActions: Scan codebase and write strings to .po file\n  GitHubActions -&gt;&gt; GitHubActions: Fill .po file with existing translations\n%% &lt;!-- markdownlint-disable-next-line MD013 --&gt;\n  Note over GitHubActions,GitHubActions: If config translations&lt;br/&gt;are available&lt;br/&gt;they are used&lt;br/&gt;otherwise empty strings\n%% &lt;!-- markdownlint-disable-next-line MD013 --&gt;\n  GitHubActions -&gt;&gt; GitHubActions: Exports configuration translations into a .config.po file\n%% &lt;!-- markdownlint-disable-next-line MD013 --&gt;\n  GitHubActions -&gt;&gt; GitHubActions: The two .po files are merged together into a .combined.po file\n  GitHubActions -&gt;&gt; GitHub: Commit combined.po file with updated strings\n  GitHubActions -&gt;&gt; POEditor: Call webhook\n  POEditor -&gt;&gt; GitHub: Fetch updated combined.po file\n  POEditor -&gt;&gt; POEditor: Synchronize translations with latest strings and translations\n  Translator -&gt;&gt; POEditor: Translate strings\n  Translator -&gt;&gt; POEditor: Export strings to GitHub\n  POEditor -&gt;&gt; GitHub: Commit combined.po file with updated translations to develop\n  GitHub -&gt;&gt; GitHub: .combined.po is split into two files: .po and .config.po\n  GitHub -&gt;&gt; GitHub: All the po files are published to Github Pages\n  DplCms -&gt;&gt; GitHub: Fetch .po file with latest translations\n  DplCms -&gt;&gt; DplCms: Import updated translations\n  DplCms -&gt;&gt; GitHub: Import config.po file with latest configuration translations</code></pre>"},{"location":"DPL-CMS/translation/#howtos","title":"Howtos","text":""},{"location":"DPL-CMS/translation/#add-new-or-update-existing-translation","title":"Add new or update existing translation","text":"<ol> <li>Log into POEditor.com and go to the <code>dpl-cms</code> project</li> <li>Go to the relevant language</li> <li>Locate the string (term) to be translated</li> <li>Translate the string</li> </ol>"},{"location":"DPL-CMS/translation/#publish-updated-translations","title":"Publish updated translations","text":"<ol> <li>Log into POEditor.com</li> <li>Select the \"Settings\" tab</li> <li>Click the GitHub code hosting service</li> <li>Check the relevant language(s)</li> <li>Select \"Export to GitHub\" and click \"Go\"</li> </ol>"},{"location":"DPL-CMS/translation/#import-updated-translations","title":"Import updated translations","text":"<ol> <li>Run <code>drush locale-check</code></li> <li>Run <code>drush locale-update</code></li> </ol>"},{"location":"DPL-CMS/translation/#import-updated-config-translations","title":"Import updated config translations","text":"<p>Run  <code>drush dpl_po:import-remote-config-po [LANGUAGE_CODE] [CONFIGURATION_PO_FIL_EXTERNAL_URL]</code></p> <p>Example:</p> <pre><code>drush dpl_po:import-remote-config-po da https://danskernesdigitalebibliotek.github.io/dpl-cms/translations/da.config.po\n</code></pre>"},{"location":"DPL-CMS/webmaster-libraries/","title":"Webmaster libraries","text":"<p>Libraries on the \"webmaster\" plan has extended permissions compared to regular libraries. They have access to most of the Drupal administration pages, and can install custom modules.</p> <p>See Permissions and roles for details on Drupal setup.</p> <p>In addition to the Drupal setup, the site should use the <code>webmaster</code> plan when configured in DPL Platform, this creates a <code>moduletest</code> environment for testing and implies a slightly different deployment scheduling.</p>"},{"location":"DPL-CMS/webmaster-modules/","title":"Webmaster modules","text":"<p>Administrators on webmaster libraries has the ability to upload and enable extra modules. These can be both standard Drupal contrib modules, or modules developed by/for the library.</p> <p>Developing bespoke modules doesn't not differ from developing modules in general, as installing and configuring modules works as expected.</p> <p>However, it not uncommon for the developer to wish to provide some configuration \"outside\" the modules own configuration, adding taxonomy vocabularies, adding fields to nodes or even changing basic configuration variables, which is somewhat challenging as DPL CMS controls the configuration.</p> <p>So to avoid every module doing it differently, this guide exists. And if enough modules finds the need, the technique described here could be implemented in a <code>dpl_webmaster</code> module that provide it as a service.</p>"},{"location":"DPL-CMS/webmaster-modules/#webmaster-module-configuration-handling","title":"Webmaster module configuration handling","text":"<p>First off, a warning: when using this technique you take the responsibility of the configuration added/overridden in this way. If you for instance override the node form display configuration for a node type, it's your responsibility to keep it up to date with changes in DPL CMS core.</p> <p>Adding new configuration for modules not in DPL CMS is more safe, but you should look out for dependencies. If a configuration depends on a particular node type or field being available for instance.</p> <p>In general, you should get more familiar with the configuration YAML you're putting into your module, than you're used to from general Drupal configuration management, and know exactly why you need each one.</p>"},{"location":"DPL-CMS/webmaster-modules/#overview","title":"Overview","text":"<p>Configuration handling in webmaster modules consists of three parts:</p> <ol> <li>The configuration itself, in Drupals configuration export YAML    format, in a <code>config/sync</code> directory in the module.</li> <li>An event subscriber that overlays the modules configuration when    Drupal imports the configuration.</li> <li>A <code>hook_install</code> and possibly <code>hook_update_N</code> functions that    trigger configuration import when the module is installed and    updated.</li> </ol> <p>We'll describe the parts in detail in the following walk-through.</p>"},{"location":"DPL-CMS/webmaster-modules/#walk-through","title":"Walk-through","text":""},{"location":"DPL-CMS/webmaster-modules/#initial-configuration","title":"Initial configuration","text":"<ol> <li>Start with a fresh DPL CMS site with the code base from git, that    has up to date configuration (<code>task dev:cli -- drush cim</code> should do    nothing).</li> <li>Make the required configuration changes in the Drupal    administration interface.</li> <li>Run <code>task dev:cli -- drush cex -y</code> to export the configuration.</li> <li>Now <code>git status</code> (or your preferred Git tool) tells you which files    has been changed, these are the files you need.</li> <li>Copy the changed configuration files to <code>config/sync</code> in your    module and revert the changes to the files in the root    <code>config/sync</code> folder.</li> </ol>"},{"location":"DPL-CMS/webmaster-modules/#event-subscriber","title":"Event subscriber","text":"<p>In order to get DPL CMS to actually use the configuration you just saved, we'll need to make it visible to Drupal. This can be done by implementing an event subscriber that overlays the configuration on <code>ConfigEvents::STORAGE_TRANSFORM_IMPORT</code>.</p> <p>An implementation can be found in kdb_brugbyen. You can simply copy that and fix the two references to the module (the namespace and the configuration path).</p>"},{"location":"DPL-CMS/webmaster-modules/#installupdate-hook","title":"Install/update hook","text":"<p>The above will add in the module configuration when the configuration is imported, but installing or updating a module does not trigger a configuration import, so you'll need to do it yourself.</p> <p>Outside of DPL CMS, triggering a configuration import from install/update hooks is not recommended, but inside DPL CMS we're in a controlled environment where library sites should always be in sync with the provided configuration. So triggering an import to overlay module configuration can be considered safe.</p> <p>Importing configuration is both simple and horribly convoluted. The <code>ConfigImporter</code> class does all the heavy lifting, but it's not a service and requires digging out thirteen different services in order to be instantiated. Again, you can just copy the code from kdb_brugbyen.</p>"},{"location":"DPL-CMS/webmaster-modules/#maintenance","title":"Maintenance","text":"<p>Keeping the module configuration up to date might pose some challenges.</p>"},{"location":"DPL-CMS/webmaster-modules/#module-configuration-changes","title":"Module configuration changes","text":"<p>Changing the configuration is often simple, just do the modifications, export and copy the changed files. You'll need to add an update hook that triggers a configuration import in order for the changes to take effect when the module is updated.</p>"},{"location":"DPL-CMS/webmaster-modules/#dpl-cms-configuration-changes","title":"DPL CMS configuration changes","text":"<p>Merging in changes from DPL CMS could be problematic, as applying the configuration changes from the module that's based on the previous version of DPL CMS might throw a <code>ConfigImporterException</code> and make the deployment fail.</p> <p>The hard failure modes of this sounds bad, but the deployment policy of first rolling out new DPL CMS releases to the libraries <code>moduletest</code> environments should catch these issues before they hit production.</p> <p>When this happens, you can either rebuild the configuration in the administration interface and export it fresh, or by careful inspection of diffs apply the needed changes by hand to the YAML files.</p> <p>Even if importing the configuration doesn't show any errors, the fact that the module in effect replaces part of the DPL CMS configuration might cause upstream changes to get lost, so you should occasionally check for this. This can be done by doing a configuration export and inspecting the diff of the configuration changes for changes that's unrelated to the modules changes.</p> <p>Deploying the fixed version requires some extra care. Obviously, you cannot update the module after updating DPL CMS, as the upgrade will fail when trying to apply the old changes from the module to the new DPL CMS configuration. So you'll have to update the module first, but you should not trigger an configuration import when doing so, as that would likely fail just as bad when trying to overlay the modules new configuration on the old DPL CMS configuration (so no <code>hook_update_N</code> for this version of the module).</p> <p>Test it out first on <code>moduletest</code>.</p> <ol> <li>Ask the hosting team to get <code>moduletest</code> reverted to the previous DPL CMS version and database and files synced with production.</li> <li>Update the module.</li> <li>Ask the hosting team to upgrade the <code>moduletest</code> environment.</li> </ol> <p>If it works, you can update the module in production and ask for upgrading production. Make the library aware that, in the timespan between updating the module and updating DPL CMS, they cannot do anything that triggers a configuration import. In practice this should translate to \"don't upload any webmaster modules\", as that should be about the only thing apart from a DPL CMS update that triggers an import.</p>"},{"location":"DPL-CMS/architecture/","title":"Architecture decision records","text":"<p>We document decisions regarding the architecture of the project using ADRs.</p> <p>We follow the format suggested by Michael Nygaard containing the sections: Title, Context, Decision and Consequences.</p> <p>We do not use the Status section. If an ADR is merged into the main branch it is by default accepted.</p> <p>We have added an Alternatives considered section to ensure we document alternative solutions and the pros and cons for these.</p>"},{"location":"DPL-CMS/architecture/adr-001a-configuration-management-OLD/","title":"Architecture Decision Record: Configuration Management","text":""},{"location":"DPL-CMS/architecture/adr-001a-configuration-management-OLD/#notice-this-is-outdated-and-left-here-only-for-historical-purposes","title":"Notice - this is outdated, and left here only for historical purposes","text":"<p>See the new ADR in adr-001b-configuration-management.md</p>"},{"location":"DPL-CMS/architecture/adr-001a-configuration-management-OLD/#context","title":"Context","text":"<p>Configuration management for DPL CMS is a complex issue. The complexity stems from different types of DPL CMS sites.</p> <p>There are two approaches to the problem:</p> <ol> <li>All configuration is local unless explicitly marked as core configuration</li> <li>All configuration is core unless explicitly marked as local configuration</li> </ol> <p>A solution to configuration management must live up to the following test:</p> <ol> <li>Initialize a local environment to represent a site</li> <li>Import the provided configuration through site installation using    <code>drush site-install --existing-config -y</code></li> <li>Log in to see that Core configuration is imported. This can be verified if    the site name is set to DPL CMS.</li> <li>Change a Core configuration value e.g. on http://dpl-cms.docker/admin/config/development/performance</li> <li>Run <code>drush config-import -y</code> and see that the change is rolled back and the    configuration value is back to default. This shows that Core configuration    will remain managed by the configuration system.</li> <li>Change a local configuration value like the site name on http://dpl-cms.docker/admin/config/system/site-information</li> <li>Run <code>drush config-import -y</code> to see that no configuration is imported. This    shows that local configuration which can be managed by Editor libraries will    be left unchanged.</li> <li>Enable and configure the Shortcut module and add a new Shortcut set.</li> <li>Run <code>drush config-import -y</code> to see that the module is not disabled and the    configuration remains unchanged. This shows that local configuration in the    form of new modules added by Webmaster libraries will be left unchanged.</li> </ol>"},{"location":"DPL-CMS/architecture/adr-001a-configuration-management-OLD/#decision","title":"Decision","text":"<p>We use the Configuration Ignore module to manage configuration.</p> <p>The module maintains a list of patterns for configuration which will be ignored during the configuration import process. This allows us to avoid updating local configuration.</p> <p>By adding the wildcard <code>*</code> at the top of this list we choose an approach where all configuration is considered local by default.</p> <p>Core configuration which should not be ignored can then be added to subsequent lines with the <code>~</code> which prefix. On a site these configuration entries will be updated to match what is in the core configuration.</p> <p>Config Ignore also has the option of ignoring specific values within settings. This is relevant for settings such as <code>system.site</code> where we consider the site name local configuration but 404 page paths core configuration.</p>"},{"location":"DPL-CMS/architecture/adr-001a-configuration-management-OLD/#alternatives-considered","title":"Alternatives considered","text":""},{"location":"DPL-CMS/architecture/adr-001a-configuration-management-OLD/#deconfig-partial-imports","title":"Deconfig + Partial Imports","text":"<p>The Deconfig module allows developers to mark configuration entries as exempt from import/export. This would allow us to exempt configuration which can be managed by the library.</p> <p>This does not handle configuration coming from new modules uploaded on webmaster sites. Since we cannot know which configuration entities such modules will provide and Deconfig has no concept of wildcards we cannot exempt the configuration from these modules. Their configuration will be removed again at deployment.</p> <p>We could use partial imports through <code>drush config-import --partial</code> to not remove configuration which is not present in the configuration filesystem.</p> <p>We prefer Config Ignore as it provides a single solution to handle the entire problem space.</p>"},{"location":"DPL-CMS/architecture/adr-001a-configuration-management-OLD/#config-ignore-auto","title":"Config Ignore Auto","text":"<p>The Config Ignore Auto module extends the Config Ignore module. Config Ignore Auto registers configuration changes and adds them to an ignore list. This way they are not overridden on future deployments.</p> <p>The module is based on the assumption that if an user has access to a configuration form they should also be allowed to modify that configuration for their site.</p> <p>This turns the approach from Config Ignore on its head. All configuration is now considered core until it is changed on the individual site.</p> <p>We prefer Config Ignore as it only has local configuration which may vary between sites. With Config Ignore Auto we would have local configuration and the configuration of Config Ignore Auto.</p> <p>Config Ignore Auto also have special handling of the <code>core.extensions</code> configuration which manages the set of installed modules. Since webmaster sites can have additional modules installed we would need workarounds to handle these.</p>"},{"location":"DPL-CMS/architecture/adr-001a-configuration-management-OLD/#config-split","title":"Config Split","text":"<p>The Config Split module allows developers to split configurations into multiple groups called settings.</p> <p>This would allow us to map the different types of configuration to different settings.</p> <p>We have not been able to configure this module in a meaningful way which also passed the provided test.</p>"},{"location":"DPL-CMS/architecture/adr-001a-configuration-management-OLD/#consequences","title":"Consequences","text":"<ul> <li>Core developers will have to explicitly select new configuration to not ignore   during the development process. One can not simply run <code>drush config-export</code>   and have the appropriate configuration not ignored.</li> <li>Because <code>core.extension</code> is ignored Core developers will have to explicitly   enable and uninstall modules through code as a part of the development   process.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-001b-configuration-management/","title":"Architecture Decision Record: Configuration Management","text":""},{"location":"DPL-CMS/architecture/adr-001b-configuration-management/#context","title":"Context","text":"<p>Configuration management for DPL CMS is a complex issue. The complexity stems from different types of DPL CMS sites.</p> <p>There are two approaches to the problem:</p> <ol> <li>All configuration is local unless explicitly marked as core configuration</li> <li>All configuration is core unless explicitly marked as local configuration</li> </ol> <p>A solution to configuration management must live up to the following test:</p> <ol> <li>Initialize a local environment to represent a site</li> <li>Import the provided configuration through site installation using    <code>drush site-install --existing-config -y</code></li> <li>Log in to see that Core configuration is imported. This can be verified if    the site name is set to DPL CMS.</li> <li>Change a Core configuration value e.g. on http://dpl-cms.docker/admin/config/development/performance</li> <li>Run <code>drush config-import -y</code> and see that the change is rolled back and the    configuration value is back to default. This shows that Core configuration    will remain managed by the configuration system.</li> <li>Change a local configuration value like the site name on http://dpl-cms.docker/admin/config/system/site-information</li> <li>Run <code>drush config-import -y</code> to see that no configuration is imported. This    shows that local configuration which can be managed by Editor libraries will    be left unchanged.</li> <li>Enable and configure the Shortcut module and add a new Shortcut set.</li> <li>Run <code>drush config-import -y</code> to see that the module is not disabled and the    configuration remains unchanged. This shows that local configuration in the    form of new modules added by Webmaster libraries will be left unchanged.</li> </ol>"},{"location":"DPL-CMS/architecture/adr-001b-configuration-management/#decision","title":"Decision","text":"<p>We use the Configuration Ignore module and the Config Ignore Auto module to manage configuration.</p> <p>The base module maintains a list of patterns for configuration which will be ignored  during the configuration import process. This allows us to avoid updating local configuration.</p> <p>Here, we can add some of the settings files that we already know needs to be ignored and admin-respected. But in reality, we don't need to do this manually, because of the second module:</p> <p>Config Ignore Auto is only enabled on non-development sites. It works by treating any settings that are updated (site settings, module settings etc.) as to be ignored. These settings will NOT be overriden on next deploy by <code>drush config-import</code>.</p> <p>The consequences of using this setup is</p> <p>1) We need to ignore <code>core.extension.yml</code>, for administrators to manage modules   This means that we need to enable/disable new modules using code.   See <code>dpl_base.install</code> for how to do this, through Drupal update hooks. 2) If a faulty permission has been added, or if a decision has been made to    remove an existing permission, there might be config that we dont want to    ignore, that is ignored on some libraries.</p> <pre><code>This means we'll first have to detect which libraries have overriden config\n\n```bash\n  drush config:get config_ignore_auto.settings ignored_config_entities\n    --format json\n```\n\nand then either decide to override it, or migrate the existing.\n</code></pre> <p>3) A last, and final consequence, is that we need to treat permissions more    strictly that we do now.</p> <pre><code>An example is `adminster site settings` also both allows stuff we want to\nignore (site name), but also things we don't want to ignore (404 node ID).\n</code></pre>"},{"location":"DPL-CMS/architecture/adr-001b-configuration-management/#alternatives-considered","title":"Alternatives considered","text":""},{"location":"DPL-CMS/architecture/adr-001b-configuration-management/#deconfig-partial-imports","title":"Deconfig + Partial Imports","text":"<p>The Deconfig module allows developers to mark configuration entries as exempt from import/export. This would allow us to exempt configuration which can be managed by the library.</p> <p>This does not handle configuration coming from new modules uploaded on webmaster sites. Since we cannot know which configuration entities such modules will provide and Deconfig has no concept of wildcards we cannot exempt the configuration from these modules. Their configuration will be removed again at deployment.</p> <p>We could use partial imports through <code>drush config-import --partial</code> to not remove configuration which is not present in the configuration filesystem.</p> <p>We prefer Config Ignore as it provides a single solution to handle the entire problem space.</p>"},{"location":"DPL-CMS/architecture/adr-001b-configuration-management/#config-split","title":"Config Split","text":"<p>The Config Split module allows developers to split configurations into multiple groups called settings.</p> <p>This would allow us to map the different types of configuration to different settings.</p> <p>We have not been able to configure this module in a meaningful way which also passed the provided test.</p>"},{"location":"DPL-CMS/architecture/adr-001b-configuration-management/#consequences","title":"Consequences","text":"<ul> <li>Core developers will have to explicitly select new configuration to not ignore   during the development process. One can not simply run <code>drush config-export</code>   and have the appropriate configuration not ignored.</li> <li>Because <code>core.extension</code> is ignored Core developers will have to explicitly   enable and uninstall modules through code as a part of the development   process.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-002-user-handling/","title":"Architecture Decision Record: User Handling","text":""},{"location":"DPL-CMS/architecture/adr-002-user-handling/#context","title":"Context","text":"<p>There are different types of users that are interacticting with the CMS system:</p> <ul> <li>Patrons that is authenticated by logging into Adgangsplatformen.</li> <li>Editors and administrators (and similar roles) that are are handling content   and configuration of the site.</li> </ul> <p>We need to be able to handle that both type of users can be authenticated and authorized in the scope of permissions that are tied to the user type.</p> <p>We had some discussions wether the Adgangsplatform users should be tied to a Drupal user or not. As we saw it we had two options when a user logs in:</p> <ol> <li>Keep session/access token client side in the browser and not creating a    Drupal user.</li> <li>Create a Drupal user and map the user with the external user.</li> </ol>"},{"location":"DPL-CMS/architecture/adr-002-user-handling/#decision","title":"Decision","text":"<p>We ended up with desicion no. 2 mentioned above. So we create a Drupal user upon login if it is not existing already.</p> <p>We use the OpeOpenID Connect / OAuth client module to manage patron authentication and authorization. And we have developed a plugin for the module called: Adgangsplatformen which connects the external oauth service with dpl-cms.</p> <p>Editors and administrators a.k.a normal Drupal users and does not require additional handling.</p>"},{"location":"DPL-CMS/architecture/adr-002-user-handling/#consequences","title":"Consequences","text":"<ul> <li>By having a Drupal user tied to the external user we can use that context and   make the server side rendering show different content according to the   authenticated user.</li> <li>Adgangsplatform settings have to be configured in the plugin in order to work.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-002-user-handling/#future-considerations","title":"Future considerations","text":"<p>Instead of creating a new user for every single user logging in via Adgangsplatformen you could consider having just one Drupal user for all the external users. That would get rid of the UUID -&gt; Drupal user id mapping that has been implemented as it is now. And it would prevent creation of a lot of users. The decision depends on if it is necessary to distinguish between the different users on a server side level.</p>"},{"location":"DPL-CMS/architecture/adr-003-ddb-react-integration/","title":"Architecture Decision Record: DPL React integration","text":""},{"location":"DPL-CMS/architecture/adr-003-ddb-react-integration/#context","title":"Context","text":"<p>The DPL React components needs to be integrated and available for rendering in Drupal. The components are depending on a library token and an access token being set in javascript.</p>"},{"location":"DPL-CMS/architecture/adr-003-ddb-react-integration/#decision","title":"Decision","text":"<p>We decided to download the components with composer and integrate them as Drupal libraries.</p> <p>As described in adr-002-user-handling we are setting an access token in the user session when a user has been through a succesful login at Adgangsplatformen.</p> <p>We decided that the library token is fetched by a cron job on a regular basis and saved in a <code>KeyValueExpirable</code> store which automatically expires the token when it is outdated.</p> <p>The library token and the access token are set in javascript on the endpoint: <code>/dpl-react/user.js</code>. By loading the script asynchronically when mounting the components i javascript we are able to complete the rendering.</p>"},{"location":"DPL-CMS/architecture/adr-004-ddb-react-caching/","title":"Architecture Decision Record: Caching of DPL React and other js resources","text":""},{"location":"DPL-CMS/architecture/adr-004-ddb-react-caching/#context","title":"Context","text":"<p>The general caching strategy is defined in another document and this focused on describing the caching strategy of DPL react and other js resources.</p> <p>We need to have a caching strategy that makes sure that:</p> <ul> <li>The js files defined as Drupal libraries (which DPL react is) and pages that   make use of them are being cached.</li> <li>The same cache is being flushed upon deploy because that is the moment where   new versions of DPL React can be introduced.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-004-ddb-react-caching/#decision","title":"Decision","text":"<p>We have created a purger in the Drupal Varnish/Purge setup that is able to purge everything. The purger is being used in the deploy routine by the command: <code>drush cache:rebuild-external -y</code></p>"},{"location":"DPL-CMS/architecture/adr-004-ddb-react-caching/#consequences","title":"Consequences","text":"<ul> <li>Everything will be invalidated on every deploy. Note: Although we are sending   a <code>PURGE</code> request we found out, by studing the vcl of Lagoon, that the <code>PURGE</code>   request actually is being translated into a <code>BAN</code> on <code>req.url</code>.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-005-api-mocking/","title":"Architecture Decision Record: API mocking","text":""},{"location":"DPL-CMS/architecture/adr-005-api-mocking/#context","title":"Context","text":"<p>DPL CMS integrates with a range of other business systems through APIs. These APIs are called both clientside (from Browsers) and serverside (from Drupal/PHP).</p> <p>Historically these systems have provided setups accessible from automated testing environments. Two factors make this approach problematic going forward:</p> <ol> <li>In the future not all systems are guaranteed to provide such environments    with useful data.</li> <li>Test systems have not been as stable as is necessary for automated testing.    Systems may be down or data updated which cause problems.</li> </ol> <p>To address these problems and help achieve a goal of a high degree of test coverage the project needs a way to decouple from these external APIs during testing.</p>"},{"location":"DPL-CMS/architecture/adr-005-api-mocking/#decision","title":"Decision","text":"<p>We use WireMock to mock API calls. Wiremock provides the following feature relevant to the project:</p> <ul> <li>Wiremock is free open source software which can be deployed in development and   tests environment using Docker</li> <li>Wiremock can run in HTTP(S) proxy mode. This allows us to run a single   instance and mock requests to all external APIs</li> <li>We can use the <code>wiremock-php</code>   client library to instrument WireMock from PHP code. We modernized the   <code>behat-wiremock-extension</code>   to instrument with Behat tests which we use for integration testing.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-005-api-mocking/#instrumentation-vs-recordreplay","title":"Instrumentation vs. record/replay","text":"<p>Software for mocking API requests generally provide two approaches:</p> <ul> <li>Instrumentation where an API can be used to define which responses will be   returned for what requests programmatically.</li> <li>Record/replay where requests passing through are persisted (typically to the   filesystem) and can be modified and restored at a later point in time.</li> </ul> <p>Generally record/replay makes it easy to setup a lot of mock data quickly. However, it can be hard to maintain these records as it is not obvious what part of the data is important for the test and the relationship between the individual tests and the corresponding data is hard to determine.</p> <p>Consequently, this project prefers instrumentation.</p>"},{"location":"DPL-CMS/architecture/adr-005-api-mocking/#alternatives-considered","title":"Alternatives considered","text":"<p>There are many other tools which provide features similar to Wiremock. These include:</p> <ul> <li>Hoverfly: FOSS, Docker image and proxy   support. PHP clients are less mature and no Behat   integration.</li> <li>Mountebank: FOSS and Docker image. No proxy support,   PHP client   is less mature and no Behat integration.</li> <li>MockServer: FOSS, Docker image and proxy   support. No PHP client and no Behat integration.</li> <li>Mockoon: FOSS and Docker image. Does not provide   instrumentation.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-005-api-mocking/#consequences","title":"Consequences","text":"<ul> <li>Developers may have to engage in maintenance of the <code>wiremock-php</code> and   <code>behat-wiremock-extension</code> library</li> </ul>"},{"location":"DPL-CMS/architecture/adr-005-api-mocking/#status","title":"Status","text":"<p>Instrumentation of Wiremock with PHP is made obsolete with the migration from Behat to Cypress.</p>"},{"location":"DPL-CMS/architecture/adr-006-api-specification/","title":"Architecture Decision Record: API specification","text":""},{"location":"DPL-CMS/architecture/adr-006-api-specification/#context","title":"Context","text":"<p>DPL CMS provides HTTP end points which are consumed by the React components. We want to document these in an established structured format.</p> <p>Documenting endpoints in a established structured format allows us to use tools to generate client code for these end points. This makes consumption easier and is a practice which is already used with other services in the React components.</p> <p>Currently these end points expose business logic tied to configuration in the CMS. There might be a future where we also need to expose editorial content through APIs.</p>"},{"location":"DPL-CMS/architecture/adr-006-api-specification/#decision","title":"Decision","text":"<p>We use the RESTful Web Services Drupal module to expose an API from DPL CMS and document the API using the OpenAPI 2.0/Swagger 2.0 specification as supported by the OpenAPI and OpenAPI REST Drupal modules.</p> <p>This is a technically manageable, standards compliant and performant solution which supports our initial use cases and can be expanded to support future needs.</p>"},{"location":"DPL-CMS/architecture/adr-006-api-specification/#alternatives-considered","title":"Alternatives considered","text":"<p>There are two other approaches to working with APIs and specifications for Drupal:</p> <ul> <li>JSON:API:   Drupals JSON:API module provides many features over the REST module   when it comes to exposing editorial content (or Drupal entities in general).   However it does not work well with other types of functionality which is what   we need for our initial use cases.</li> <li>GraphQL:   GraphQL is an approach which does not work well with Drupals HTTP based   caching layer. This is important for endpoints which are called many times   for each client.   Also from version 4.x and beyond the GraphQL Drupal module   provides no easy way for us to expose editorial content at a later point in time.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-006-api-specification/#consequences","title":"Consequences","text":"<ul> <li>This is an automatically generated API and specification. To avoid other   changes leading to unintended changes this we keep the latest version of the   specification in VCS and setup automations to ensure that the   generated specification matches the inteded one.   When developers update the API they have to use the provided tasks   to update the stored API accordingly.</li> <li>OpenAPI and OpenAPI REST are Drupal modules which have not seen updates for a   while. We have to apply patches to get them to work for us. Also they do not   support the latest version of the OpenAPI specification, 3.0. We risk   increased maintenance because of this.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-007-breadcrumb-and-url-patterns/","title":"Architecture Decision Record: Breadcrumb structure &amp; URL patterns","text":""},{"location":"DPL-CMS/architecture/adr-007-breadcrumb-and-url-patterns/#context","title":"Context","text":"<p>The tagging system we have (tags &amp; categories) considers content as 'islands': Two peices of content may be tagged with the same, but they do not know about each other.</p> <p>DDF however needs a way to structure content hierarchies. After a lot of discussion, we reached the conclusion that this can be materialized  through the breadcrumb - the breadcrumb is basically the frontend version of the content structure tree that editors will create and manage.</p> <p>Because of this, the breadcrumb is \"static\" and not \"dynamic\" - e.g., on some sites, the breadcrumb is built dynamically, based on the route that the user takes through the site, but in this case, the whole structure is built by the editors.</p> <p>However, some content is considered \"flat islands\" - e.g. articles and events should not know anything about each other, but still be categorized.</p> <p>Either way, the breadcrumb also defines the URL alias.</p>"},{"location":"DPL-CMS/architecture/adr-007-breadcrumb-and-url-patterns/#decision","title":"Decision","text":"<p>There are two types of breadcrumbs:</p> <ul> <li>Category-based</li> <li>Articles and events can be tagged with categories. These categories may</li> <li>have a hiarchy, and this tree will be displayed as part of the article</li> <li>breadcrumb.</li> <li>Content Structure</li> <li>A custom taxonomy, managed by webmasters, where they choose \"core-content     references\". This builds the tree.</li> <li>When creating non-core pages, there is a field that the editor can choose     where this page \"lives\" in the structure tree.</li> <li>Based on this, the breadcrumb will be built.</li> </ul> <p>All of this is managed by <code>dpl_breadcrumb</code> module.</p>"},{"location":"DPL-CMS/architecture/adr-007-breadcrumb-and-url-patterns/#alternatives-considered","title":"Alternatives considered","text":"<p>We tried using menus instead of taxonomy, based on experience from another project, but it caused too much confusion and a general poor admin experience. More info about that in the ticket comments:</p>"},{"location":"DPL-CMS/architecture/adr-007-breadcrumb-and-url-patterns/#consequences","title":"Consequences","text":"<p>A functional breadcrumb, that is very hard to replace/migrate if we choose a different direction.</p>"},{"location":"DPL-CMS/architecture/adr-007-cypress-functional-testing/","title":"Architecture Decision Record: Cypress for functional testing","text":""},{"location":"DPL-CMS/architecture/adr-007-cypress-functional-testing/#context","title":"Context","text":"<p>DPL CMS employs functional testing to ensure the functional integrity of the project.</p> <p>This is currently implemented using Behat which allows developers to instrument a browser navigating through different use cases using Gherkin, a business readable, domain specific language. Behat is used within the project based on experience using it from the previous generation of DPL CMS.</p> <p>Several factors have caused us to reevaluate that decision:</p> <ul> <li>The Drupal community has introduced Nightwatch for browser testing</li> <li>Usage of Behat within the Drupal community has stagnated</li> <li>Developers within this project have questioned the developer experience and   future maintenance of Behat</li> <li>Developers have gained experience using Cypress for   browser based testing of the React components</li> </ul>"},{"location":"DPL-CMS/architecture/adr-007-cypress-functional-testing/#decision","title":"Decision","text":"<p>We choose to replace Behat with Cypress for functional testing.</p>"},{"location":"DPL-CMS/architecture/adr-007-cypress-functional-testing/#alternatives-considered","title":"Alternatives considered","text":"<p>There are other prominent tools which can be used for browser based functional testing:</p> <ul> <li>Playwright:   Playwright is a promising tool for browser based testing. It supports many   desktop and mobile browsers. It does not have the same widespread usage as   Cypress.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-007-cypress-functional-testing/#consequences","title":"Consequences","text":"<ul> <li>Although Cypress supports intercepting requests to external systems   this only works for clientside requests. To maintain a consistent approach to   mocking both serverside and clientside requests to external systems we   integrate Cypress with Wiremock using a similar approach to what we have done   with Behat.</li> <li>There is a community developed module which integrates Drupal with Cypress.   We choose not to use this as it provided limited value to our use case and   we prefer to avoid increased complexity.</li> <li>We will not only be able to test on mobile browsers as this is not supported   by Cypress. We prefer consistency across projects and expected improved   developer efficiency over what we expect to be improved complexity of   introducing a tool supporting this natively or expanding Cypress setup to   support mobile testing.</li> <li>We opt not to use Gherkin to describe our test cases. The business has   decided that this has not provided sufficient value for the existing project   that the additional complexity is not needed. Cypress community plugins   support writing tests in Gherkin.   These could be used in the future.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-008-external-system-integration/","title":"Architecture Decision Record: Integration with external systems","text":""},{"location":"DPL-CMS/architecture/adr-008-external-system-integration/#context","title":"Context","text":"<p>DPL CMS is only intended to integrate with one external system: Adgangsplatformen. This integration is necessary to obtain patron and library tokens needed for authentication with other business systems. All these integrations should occur in the browser through React components.</p> <p>The purpose of this is to avoid having data passing through the CMS as an intermediary. This way the CMS avoids storing or transmitting sensitive data. It may also improve performance.</p> <p>In some situations it may be beneficiary to let the CMS access external systems to provide a better experience for business users e.g. by displaying options with understandable names instead of technical ids or validating data before it reaches end users.</p>"},{"location":"DPL-CMS/architecture/adr-008-external-system-integration/#decision","title":"Decision","text":"<p>We choose to allow CMS to access external systems server-side using PHP. This must be done on behalf of the library - never the patron.</p>"},{"location":"DPL-CMS/architecture/adr-008-external-system-integration/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li>Implementing React components to provide administrative controls in the CMS.   This would increase the complexity of implementing such controls and cause   implementors to not consider improvements to the business user experience.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-008-external-system-integration/#consequences","title":"Consequences","text":"<ul> <li>We allow PHP client code generation for external services. These should not   only include APIs to be used with library tokens. This signals what APIs are   OK to be accessed server-side.</li> <li>The CMS must only access services using the library token provided by the   <code>dpl_library_token.handler</code> service.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-009-translation-system/","title":"Architecture Decision Record: Translation system","text":""},{"location":"DPL-CMS/architecture/adr-009-translation-system/#context","title":"Context","text":"<p>The current translation system for UI strings in DPL CMS is based solely on code deployment of <code>.po</code> files.</p> <p>However DPL CMS is expected to be deployed in about 100 instances just to cover the Danish Public Library institutions. Making small changes to the UI texts in the codebase would require a new deployment for each of the instances.</p> <p>Requiring code changes to update translations also makes it difficult for non-technical participants to manage the process themselves. They have to find a suitable tool to edit <code>.po</code> files and then pass the updated files to a developer.</p> <p>This process could be optimized if:</p> <ol> <li>Translations were provided by a central source</li> <li>Translations could be managed directly by non-technical users</li> <li>Distribution of translations is decoupled from deployment</li> </ol>"},{"location":"DPL-CMS/architecture/adr-009-translation-system/#decision","title":"Decision","text":"<p>We keep using GitHub as a central source for translation files.</p> <p>We configure Drupal to consume translations from GitHub. The Drupal translation system already supports runtime updates and consuming translations from a remote source.</p> <p>We use POEditor to perform translations. POEditor is a translation management tool that supports <code>.po</code> files and integrates with GitHub. To detect new UI strings a GitHub Actions workflow scans the codebase for new strings and notifies POEditor. Here they can be translated by non-technical users. POEditor supports committing translations back to GitHub where they can be consumed by DPL CMS instances.</p>"},{"location":"DPL-CMS/architecture/adr-009-translation-system/#consequences","title":"Consequences","text":"<p>This approach has a number of benefits apart from addressing the original issue:</p> <ul> <li>POEditor is a specialized tool to manage translations. It supports features   such as translation memory, glossaries and machine translation.</li> <li>POEditor is web-based. Translators avoid having to find and install a suitable   tool to edit <code>.po</code> files.</li> <li>POEditor is software-as-a-service. We do not need to maintain the translation   interface ourselves.</li> <li>POEditor is free for open source projects. This means that we can use it   without having to pay for a license.</li> <li>Code scanning means that new UI strings are automatically detected and   available for translation. We do not have to manually synchronize translation   files or ensure that UI strings are rendered by the system before they can be   translated. This can be complex when working with special cases, error   messages etc.</li> <li>Translations are stored in version control. Managing state is complex and this   means that we have easy visibility into changes.</li> <li>Translations are stored on GitHub. We can move away from POEditor at any time   and still have access to all translations.</li> <li>We reuse existing systems instead of building our own.</li> </ul> <p>A consequence of this approach is that developers have to write code that supports scanning. This is partly supported by the Drupal Code Standards. To support contexts developers also have to include these as a part of the <code>t()</code> function call e.g.</p> <pre><code>// Good\n$this-&gt;t('A string to be translated', [], ['context' =&gt; 'The context']);\n$this-&gt;t('Another string', [], ['context' =&gt; 'The context']);\n// Bad\n$c = ['context' =&gt; 'The context']\n$this-&gt;t('A string to be translated', [], $c);\n$this-&gt;t('Another string', [], $c);\n</code></pre> <p>We could consider writing a custom sniff or PHPStan rule to enforce this</p>"},{"location":"DPL-CMS/architecture/adr-009-translation-system/#potion","title":"Potion","text":"<p>For covering the functionality of scanning the code we had two potential projects that could solve the case:</p> <ul> <li>Potion</li> <li>Potx</li> </ul> <p>Both projects can scan the codebase and generate a <code>.po</code> or <code>.pot</code> file with the translation strings and context.</p> <p>At first it made most sense to go for Potx since it is used by localize.drupal.org and it has a long history. But Potx is extracting strings to a <code>.pot</code> file without having the possibility of filling in the existing translations. So we ended up using Potion which can fill in the existing strings.</p> <p>A flip side using Potion is that it is not being maintained anymore. But it seems quite stable and a lot of work has been put into it. We could consider to back it ourselves.</p>"},{"location":"DPL-CMS/architecture/adr-009-translation-system/#alternatives-considered","title":"Alternatives considered","text":"<p>We considered the following alternatives:</p> <ol> <li>Establishing our own localization server. This proved to be very complex.    Available solutions are either technically outdated    or still under heavy development.    None of them have integration with GitHub where our project is located.</li> <li>Using a separate instance of DPL CMS in Lagoon as a central translation hub.    Such an instance would require maintenance and we would have to implement a    method for exposing translations to other instances.</li> </ol>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/","title":"Architecture Decision Record: Recurring events","text":""},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#context","title":"Context","text":"<p>Events make up an important part of the overall activities the Danish Public Libraries. They aim to promote information, education and cultural activity. Events held at the library have many different formats like book readings, theater for children, tutoring and exhibitions of art. Some of these events are singular and some are recurring.</p> <p>We define a recurring event as an event that occurs more than once over the course of a period of time where the primary different between each occurrence of the event is the event start and end time.</p> <p>A simple solution to this would be to use a multi-value date range field but there are a number of factors that makes this more challenging:</p>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#functional-requirements","title":"Functional requirements","text":"<ul> <li>Schedules: Editors would like to create occurrences based on a schedule   e.g. every Tuesday from 15:00 to 17:00 between January 1st and March 31th.   This is simpler than having to set each date and time manually.</li> <li>Reuse: Editors would like to avoid having to retype information for each   instance if it does not wary.</li> <li>Exceptions: Editors need to be able to create exceptions. An event might   not occur during a holiday.</li> <li>Variatons: Occurrences may have variations between them. If attendance   requires a ticket, then each occurrence should have a unique url to buy   these. An occurrence can also be marked as sold out or cancelled. This is   preferable to deleting the instance for the sake of communication to end   users.</li> <li>Relationships: End users would like to a see the relationship between   occurrences. If the date of one occurrence does not fit their personal   schedules it is nice to see the alternatives.</li> <li>Instances in lists: End users should be able to see individual   occurrences in lists. If an event occurs every Tuesday and the end user   scrolls down a list of events then the event should be presented on every   Tuesday so the end user can get a clear picture of what is going on that day.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#other-qualities","title":"Other qualities","text":"<ul> <li>Editorial user experience: Creating schedules can be complex. Editors   should be able to do this without being confronted with fields that are hard   to understand or seem unnecessary.</li> <li>Maintenance: If we base a solution on third party code we need to   consider future maintenance.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#decision","title":"Decision","text":"<p>We have decided to base our solution on the Recurring Events Drupal module.</p> <p>The purpose of the module overlaps with our need in regards to handling recurring events. The module is based on a construct where a recurring event consists of an event series entity which has one or more related event instances. Each instance corresponds to an specific date when the event occurs.</p> <p>The module solves our requirements the following way:</p>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#schedule","title":"Schedule","text":"<p>The module supports creating shedules for events with daily, weekly, monthly, and annual repetitions. Each frequency can be customized, for example, which days of the week the weekly event should repeat on (e.g., every Tuesday and Thursday), which days of the month events should repeat on (e.g., the first Wednesday, the third Friday).</p>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#reuse","title":"Reuse","text":"<p>Event series and instances are fieldable entities. The module relies on the Field Inheritance Drupal module which allows data to be set on event series and reuse it on individual entities.</p>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#exceptions","title":"Exceptions","text":"<p>Recurring events support exceptions in two ways:</p> <ol> <li>Editors can delete individual instances after they have been created,</li> <li>Editors can create periods in schedules where no instances should be created.    Such periods can also be created globally to make them apply to all series.    This can be handy for handling national holidays.</li> </ol>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#variations","title":"Variations","text":"<p>The Field Inheritance module supports different modes of reuse. By using the fallback method we can allow editors override values from event series on individual instances.</p>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#relationships","title":"Relationships","text":"<p>Recurring events creates a relationship between an event series and individual instances. Through this relationsship we can determine what other instances might be for an individual instance.</p>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#instances-in-lists","title":"Instances in lists","text":"<p>It is possible to create lists of individual instances of events using Views.</p>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#editorial-user-experience","title":"Editorial user experience","text":"<p>Recurring events uses a lot of vertical screen real estate on form elements needed to define schedules (recurrence type, date/time, schedule, excluded dates).</p> <p>The module supports defining event duration (30 minutes, 1 hour, 2 hours). This is simpler than having to potentially repeat date and time.</p>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#maintenance","title":"Maintenance","text":"<p>Recurring events lists six maintainers on Drupal.org and is supported by three companies. Among them is Lullabot, a well known company within the community.</p> <p>The module has over 1.000 sites reported using the module. The latest version, 2.0.0-rc16, was recently released on December 1th 2023.</p> <p>The dependency, Field Inheritance, currently requires two patches to Drupal Core.</p>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#consequences","title":"Consequences","text":"<p>By introducing new entity types for event series and instance has some consequences:</p> <ul> <li>All work currently done in relation to event nodes have to be migrated to   event series and/or instances.</li> <li>We cannot use modules which only work with nodes. Experience shows that   such modules have been gradually replaced by modules which work with all   entities. Examples include modules like Entity Clone   and Entity Queue.</li> <li>We cannot use the Drupal Core Views module to create lists of content which   combine nodes like articles and events. To address this need we can use   Search API which supports   creating indices and from these, views, across entity types. We are planning   to use this module anyway.</li> </ul> <p>For future work related to events we have to consider when it is appropriate to use event series and when to use event instances.</p> <p>To create a consistent data structure for events we have to use recurring events - even for singular events.</p> <p>We may have to do work to improve the editorial experience. This should preferably be upstreamed to the Recurring Events module.</p> <p>Going forward we will have to participate in keeping Field Inheritance patches to Drupal Core updated to match future versions until they are merged.</p>"},{"location":"DPL-CMS/architecture/adr-010-recurring-events/#alternatives-considered","title":"Alternatives considered","text":"<p>In the process two alternative Drupal modules were considered:</p> <ul> <li>Smart date: This was heavily   considered due to good editorial experience and maintenance status. The module   also shows promise in regard to handling opening hours for libraries. For   recurring events the module was eventually discarded due to lacking support   for variations out of the box.</li> <li>Entity repeat: This was   ruled out due to lack of relationship between event instances, poor editorial   experience and worrying outlook regading maintenance (very small user base,   no official Drupal 10 version)</li> </ul>"},{"location":"DPL-CMS/architecture/adr-011-configuration-translation-system/","title":"Architecture Decision Record: Configuration translation system","text":""},{"location":"DPL-CMS/architecture/adr-011-configuration-translation-system/#context","title":"Context","text":"<p>The translation system described in adr-009-translation-system handles solely the translations added through Drupals traditional translation handling.</p> <p>But there was a wish for being able to translate configuration related strings as well. This includes titles, labels, descriptions, and other elements in the admin area.</p> <p>We needed to find a way to handle translation of that kind as well.</p>"},{"location":"DPL-CMS/architecture/adr-011-configuration-translation-system/#decision","title":"Decision","text":"<p>We went for a solution where we activated the Configuration Translation Drupal core module and added the Configuration Translation PO contrib module.</p> <p>And we added a range of custom drush commands to handle the various configuration translation tasks.</p>"},{"location":"DPL-CMS/architecture/adr-011-configuration-translation-system/#consequences","title":"Consequences","text":"<p>By sticking to the handling of PO files in configuration handling that we are already using in our general translation handling, we can keep the current Github workflows with some alterations.</p> <p>Unfortunately handling translations of configuration on local sites is still as difficult as before. Translation of configuration texts cannot be found in the standard UI strings translation list.</p>"},{"location":"DPL-CMS/architecture/adr-011-configuration-translation-system/#alterations-to-former-translation-workflow","title":"Alterations to former translation workflow","text":"<p>With the config translation PO files added we tried to uncover if POEditor was able to handle two PO files simultaneously in both import and export context. It could not.</p> <p>But we still needed, in Drupal, to be able to import two different files: One for general translations and one for configuration translations.</p> <p>We came up with the idea that we could merge the two files going when importing into POEditor and split it again when exporting from POEditor.</p> <p>We tried it out and it worked so that was the solution we ended up with.</p>"},{"location":"DPL-CMS/architecture/adr-011-configuration-translation-system/#alternatives-considered","title":"Alternatives considered","text":""},{"location":"DPL-CMS/architecture/adr-011-configuration-translation-system/#using-the-config_translate-module","title":"Using the config_translate module","text":"<p>We could activate only the config_translate module and add a danish translations in config/sync. But then:</p> <ol> <li>We would not be able to use POEditor</li> <li>We would need to translate the string on behalf of the administrators</li> </ol>"},{"location":"DPL-CMS/architecture/adr-011-configuration-translation-system/#a-hack","title":"A hack","text":"<p>We could keep the machine names of the config in English but write the titles, labels, descriptions in Danish.</p> <p>But that would have the following bad consequences:</p> <ol> <li>The administrators would have to find all the texts in various, not obvious, places in the admin area.</li> <li>It would differ from the general translation routine which is confusing</li> <li>We would not be able to handle multiple languages for the configuration translations</li> </ol>"},{"location":"DPL-CMS/architecture/adr-011-configuration-translation-system/#extending-the-potion-module","title":"Extending the Potion module","text":"<p>Change the Potion module to be able to scan configuration translations as well.</p> <p>We did not have a clear view of the concept of localizing configuration translations in the same manner as the Potion module scans the codebase. It could either be cumbersome to get the two worlds to meet in the same Potion functionalities or simply incompatible.</p>"},{"location":"DPL-CMS/architecture/adr-012-api-versioning/","title":"Architecture Decision Record: API versioning","text":""},{"location":"DPL-CMS/architecture/adr-012-api-versioning/#context","title":"Context","text":"<p>DPL CMS exposes data and functionality through HTTP endpoints which are documented by an OpenAPI specification as described in a previous ADR.</p> <p>Over time this API may need to be updated as the amount of data and functionality increases and changes. Handling changes is an important aspect of an API as such changes may affect third parties consuming this API.</p>"},{"location":"DPL-CMS/architecture/adr-012-api-versioning/#decision","title":"Decision","text":"<p>We use URI versioning of the API exposed by DPL CMS.</p> <p>This is a simple approach to versioning which works well with the RESTful Web Services Drupal module that we use to develop HTTP endpoints with. Through the specification of the paths provided by the endpoints we can define which version of an API the endpoint corresponds to.</p>"},{"location":"DPL-CMS/architecture/adr-012-api-versioning/#breaking-changes","title":"Breaking changes","text":"<p>When a breaking change is made the version of the API is increased by one e.g. from <code>/api/v1/events</code> to <code>/api/v2/events</code>.</p> <p>We consider the following changes breaking:</p> <ol> <li>Adding required request parameters to HTTP endpoints</li> <li>Removing functionality of an endpoint (e.g. an HTTP method or request    parameter)</li> <li>Removing an exiting data field in response data</li> <li>Updating the semantics of an existing data field in response data</li> </ol> <p>The following changes are not considered breaking:</p> <ol> <li>Adding optional request parameters</li> <li>Adding additional data fields to existing structures in response data</li> </ol> <p>The existing version will continue to exist.</p>"},{"location":"DPL-CMS/architecture/adr-012-api-versioning/#alternatives-considered","title":"Alternatives considered","text":""},{"location":"DPL-CMS/architecture/adr-012-api-versioning/#header-based-versioning","title":"Header based versioning","text":"<p>Header based versioning is used by other systems exposing REST APIs in the infrastructure of the Danish Public Libraries. However we cannot see this approach working well with the RESTful Web Services Drupal module. It does not deal with multiple versions of an endpoint which different specifications.</p>"},{"location":"DPL-CMS/architecture/adr-012-api-versioning/#graphql","title":"GraphQL","text":"<p>Versionless GraphQL APIs are a common practice Drupal can support GraphQL through a third party module but using this would require us to reverse our approach to API development and specification.</p>"},{"location":"DPL-CMS/architecture/adr-012-api-versioning/#consequences","title":"Consequences","text":"<p>Based on this approach we can provide updated versions of our API by leveraging our existing toolchain.</p>"},{"location":"DPL-CMS/architecture/adr-013-javascript-logging/","title":"Architecture Decision Record: JavaScript logging","text":""},{"location":"DPL-CMS/architecture/adr-013-javascript-logging/#context","title":"Context","text":"<p>In the DPL CMS, we integrate React applications within a Drupal system as outlined in a previous ADR. Effective JavaScript error logging is crucial for timely detection, diagnosis, and resolution of issues. It's essential that our logging setup not only captures client-side errors efficiently but also integrates seamlessly with Drupal's Watchdog logging system. This allows for logs to be forwarded to Grafana for real-time monitoring and analysis, enhancing our system's reliability and performance monitoring capabilities.</p>"},{"location":"DPL-CMS/architecture/adr-013-javascript-logging/#decision","title":"Decision","text":"<p>After evaluating several options, we decided to integrate JSNLog via the JSNLog Drupal module for logging JavaScript errors. This integration allows us to capture and log client-side errors directly from our React components into our server-side logging infrastructure.</p>"},{"location":"DPL-CMS/architecture/adr-013-javascript-logging/#alternatives-considered","title":"Alternatives considered","text":"<ul> <li> <p>JSLog Drupal module:</p> </li> <li> <p>The module does not have a stable release at the time of writing, which   poses risks regarding reliability and ongoing support.</p> </li> <li> <p>During testing, it generated excessively large numbers of log entries,   which could overwhelm our logging infrastructure and complicate error analysis.</p> </li> <li> <p>Custom built solution:</p> </li> <li> <p>Significant development time and resources required to build, test, and   maintain the module.</p> </li> <li> <p>Lacks the community support and proven stability found in established   third-party solutions, potentially introducing risks in terms of long-term   reliability and scalability.</p> </li> <li> <p>Third-party services:</p> </li> <li>We deliberately dismissed options such as Sentry, Raygun, and similar   third-party services due to our reluctance to introduce additional external   dependencies and complexities.</li> <li>There are also possibilities for logging in Loki with a third-party library.   We avoided this because of unknown scope and complexities.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-013-javascript-logging/#consequences","title":"Consequences","text":"<ol> <li>Enhanced Error Detection and Diagnostics:</li> <li>Pros: Improved visibility into client-side errors helps in faster    detection and resolution of issues that impact user experience.</li> <li>Cons: The detailed error logging could potentially lead to larger    volumes of data to manage and analyze, which may require additional resources.</li> <li>Seamless Integration with Existing Systems:</li> <li>Pros: By utilizing a Drupal module that connects JSNLog with Watchdog,    errors logged on the client side are automatically integrated into the    existing Drupal logging framework. This ensures that all system logs are    centralized, simplifying management and analysis.</li> <li>Cons: Dependency on the Drupal module for JSNLog could introduce    complexities, especially if the module is not regularly updated or falls out   of sync with new versions of JSNLog or Drupal.</li> </ol>"},{"location":"DPL-CMS/architecture/adr-014-delingstjenesten/","title":"Architecture Decision Record: Delingstjenesten","text":""},{"location":"DPL-CMS/architecture/adr-014-delingstjenesten/#context","title":"Context","text":"<p>Delingstjenesten wants to implement a content sharing site for the libraries to promote content reuse and help smaller libraries with limited content production resources.</p> <p>The major use-cases is:</p> <ul> <li>Give editors on libraries the option to easily import produced   content (articles, news items) from Delingstjenesten into their   site.</li> <li>Give editors the option to share specific content on their site with   the community at large, through Delingstjenesten.</li> <li>A subscription feature that allows editors to get a steady flow of   automatically published selected content.</li> </ul> <p>BNR (Bibliotekernes Nationale Redaktion, the content editors of the Delingstjenesten site) wishes to enrich and QA submitted content before making it generally available.</p>"},{"location":"DPL-CMS/architecture/adr-014-delingstjenesten/#decision","title":"Decision","text":"<p>Delingstjenesten is built on top of DPL CMS, the advantage of this is twofold: Technically it avoids having to recreate and maintain copies of the content structures of DPL CMS, secondly it allows BNR to use DPL CMS features to set up the site as a showroom for the content.</p>"},{"location":"DPL-CMS/architecture/adr-014-delingstjenesten/#architecture","title":"Architecture","text":"<p>The system works with as little stored data as possible to avoid configuration and data that can be out of sync. Delingstjenesten does not have a list of client sites, but talks with anyone that provide the right credentials and a reachable URI.</p> <p>The client sites keeps track of their subscriptions themselves and periodically asks Delingstjenesten for updates. A subscription is represented on the library site by an entity that contains the UUID of the term on Delingstjenesten that's subscribed to, and which local terms to assign to content imported through this subscription.</p> <p>Communication between the library sites and Delingstjenesten is done using GraphQL, using a combination of queries and mutations.</p>"},{"location":"DPL-CMS/architecture/adr-014-delingstjenesten/#alternatives-considered","title":"Alternatives considered","text":"<p>Alternatives to GraphQL was briefly considered, but as the GO project relies heavily on GraphQL, and there is some talk about moving the existing REST services to GraphQL, it was decided to stick to one technology for everybody's sanity's sake.</p>"},{"location":"DPL-CMS/architecture/adr-014-delingstjenesten/#future-considerations","title":"Future considerations","text":"<p>The Sailor GraphQL PHP client generator that's currently used to generate a strongly typed client doesn't recognize when the same types is used across the API. This means that the same type is represented by different classes across operations, paragraphs and nodes, which makes the mapping unnecessarily convoluted and have resulted in some un-GraphQL-like queries to work around.</p> <p>Finding/building a client generator without this limitation would simply the mapping of data significantly and allow for better usage of GraphQL.</p>"},{"location":"DPL-CMS/architecture/adr-015-go-cms-intersite-links/","title":"Architecture Decision Record: Links between Go and CMS","text":""},{"location":"DPL-CMS/architecture/adr-015-go-cms-intersite-links/#context","title":"Context","text":"<p>The Go site (for kids) and the regular CMS appear to users as two sites with their own URLs, but in reality all content is in the same Drupal instance, with Go having its own React frontend.</p> <p>The regular CMS doesn't render Go node types very well, and the Go front-end can't render CMS node types at all, but per default all links \"between sites\" is rendered as local links.</p> <p>So there's a need to ensure that content is shown on the correct \"site\".</p> <p>This is complicated by the fact that figuring out which \"site\" the current request is servicing isn't obvious.</p>"},{"location":"DPL-CMS/architecture/adr-015-go-cms-intersite-links/#decision","title":"Decision","text":"<p>By leveraging Drupals <code>PathProcessor</code> system, we can ensure that links in link fields and WYSIWYG field (via the <code>linkit</code> module) gets rendered as external links to the correct site.</p> <p>The <code>PathProcessor</code> intercepts all <code>/node/&lt;nid&gt;</code> links (before alias processing), and checks the node content type to determine if it should be left internal or externalized. If it needs to be external, it simply sets the <code>base_url</code> property to the relevant site.</p> <p>This is basically how the domain language negotiation plugin in Drupal core implements links between language versions, so the method should be fairly safe.</p> <p>What \"site\" we're on is determined by checking the <code>use absolute cms urls</code> permission, which is only given to the <code>go_graphql_client</code> role which in turn is only given to the <code>go_graphql</code> user that's used for the Go GraphQL consumer.</p> <p>This is because \"being on the Go site\" is defined as GraphQL requests from the React front-end. When this is not the case it's assumed to be a CMS request.</p> <p>The rewriting is not done on administrative pages to ensure that the original path is used when editing the content.</p>"},{"location":"DPL-CMS/architecture/adr-015-go-cms-intersite-links/#consequences","title":"Consequences","text":"<p>Drupal user 1 isn't able to use the Go front-end, as the link logic will be reversed. This is because user 1 is explicitly excluded from the above check, as user 1 always gets all permissions. This is purely a theoretical limitation, user 1 isn't able to log into the Go site as the Go site uses Unilogin for authentication.</p> <p>Currently it only affects the base part of the URL and assumes that the path itself it the same across the sites. Which is a reasonable assumption as it is the same site, but it means that the sites share a path namespace.</p>"},{"location":"DPL-CMS/architecture/adr-015-go-cms-intersite-links/#alternatives-considered","title":"Alternatives considered","text":"<p>Different approaches was considered but rejected, some only covering part of the problem space.</p>"},{"location":"DPL-CMS/architecture/adr-015-go-cms-intersite-links/#redirects-and-extra-graphql-fields","title":"Redirects and extra GraphQL fields","text":"<p>Instead of rewriting paths, simply redirect all requests for Go content types to the Go counterpart on the CMS site, and adding an extra field to the <code>Link</code> type in GraphQL to inform the Go front-end that a link is to the CMS.</p> <ol> <li>Extra requests.</li> <li>Go front-end would have to be adapted to deal with three different    link types, internal, external and internal-but-external.</li> <li>WYSIWYG fields would need special care.</li> </ol>"},{"location":"DPL-CMS/architecture/adr-015-go-cms-intersite-links/#use-a-go-prefix","title":"Use a /go prefix","text":"<p>Keep all Go pages under <code>/go</code>.</p> <ol> <li>Doesn't look good.</li> <li>Front-end still needs to deal with two different types of internal    links.</li> </ol>"},{"location":"DPL-CMS/architecture/adr-016-page-objects/","title":"Architecture Decision Record: Page objects in Cypress E2E tests","text":""},{"location":"DPL-CMS/architecture/adr-016-page-objects/#context","title":"Context","text":"<p>Cypress tests of anything but the simplest pages can quickly become large, difficult to follow and repetitive across tests. It's difficult to separate the important steps from the implementation details.</p>"},{"location":"DPL-CMS/architecture/adr-016-page-objects/#decision","title":"Decision","text":"<p>We've decided to go with the Page Object Model. It's an established model that's proven its worth.</p> <p>It provides an abstraction for working with pages in tests that moves the implementation details of pages from the test to the page object, and allows for writing your test in a language that's closer to the users mental model.</p> <p>A guide for usage and implementation has been added.</p>"},{"location":"DPL-CMS/architecture/adr-016-page-objects/#consequences","title":"Consequences","text":"<ul> <li>Abstracting the page details away from the test and into the page   object makes the focus of the test more clear.</li> <li>Page objects are reusable across tests, this reduces duplication and   the amount of test code that needs to be updated when refactoring.</li> <li>Abstractions are not free, it does add a bit more work for the   simple case of pressing a button or asserting the contents of a   <code>&lt;div&gt;</code>.</li> </ul>"},{"location":"DPL-CMS/architecture/adr-016-page-objects/#alternatives-considered","title":"Alternatives considered","text":"<p>We started out by splitting out abstractions of user actions into functions. Cypress developers themselves suggests essentially the same thing, but using actions, which makes the functions available on the <code>cy</code> object.</p> <p>The issue with this approach is that it lacks structure and abstraction. Every action is part of a global namespace, but only a subset of actions is applicable on a given page.</p> <p>Secondly it provides no pattern for getting information from a page. A given test needs to know the exact implementation of a page it interacts with, regardless of whether that page is actually the focus or the test or only used in the process of testing another component.</p> <p>One might attempt to fix these issues with naming conventions for actions and data fetching helpers, but that's basically just trying to implement POM without the O.</p>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/","title":"Alternative Lagoon File Transfer Runbook","text":""},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#overview","title":"Overview","text":"<p>This runbook provides alternative instructions for transferring files from Lagoon environments to local development when the Lagoon UI backup feature fails. This is particularly useful for webmaster libraries that depend on module files being available locally. The regular way of transferring files is described in this document.</p>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#when-to-use-this-runbook","title":"When to Use This Runbook","text":"<ul> <li>Need to download files from a Lagoon environment to local development</li> <li>Setting up a webmaster library locally that requires module files</li> <li>Manual file synchronization between environments</li> </ul>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Lagoon CLI installed and configured</li> <li>SSH access to the target Lagoon project</li> </ul>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#step-by-step-procedure","title":"Step-by-Step Procedure","text":""},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#step-1-connect-to-lagoon-environment","title":"Step 1: Connect to Lagoon Environment","text":"<ol> <li>Open terminal</li> <li>Connect to the target project's SSH environment:</li> </ol> <pre><code>lagoon ssh -p [project] -e main\n</code></pre> <p>Replace <code>[project]</code> with the actual project name</p>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#step-2-create-archive-of-files","title":"Step 2: Create Archive of Files","text":"<p>Choose one of the following options based on your needs:</p>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#option-a-all-site-files","title":"Option A: All Site Files","text":"<pre><code>tar -cvzf /tmp/[project]_files.tar.gz /app/web/sites/default/files/\n</code></pre>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#option-b-drupal-modules-only","title":"Option B: Drupal Modules Only","text":"<pre><code>tar -cvzf /tmp/[project]_files_modules.tar.gz /app/web/sites/default/files/modules_local\n</code></pre> <p>Replace <code>[project]</code> with your project name for consistent naming.</p>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#step-3-exit-ssh-session","title":"Step 3: Exit SSH Session","text":"<pre><code>exit\n</code></pre> <p>Or use keyboard shortcut: <code>Ctrl + D</code></p>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#step-4-get-ssh-connection-string","title":"Step 4: Get SSH Connection String","text":"<ol> <li>Retrieve the SSH connection details:</li> </ol> <pre><code>lagoon ssh -p [project] -e main --conn-string\n</code></pre> <ol> <li>Note the connection string format: <code>[project]-[environment]@[ip]</code></li> </ol>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#step-5-transfer-files-using-scp","title":"Step 5: Transfer Files Using SCP","text":"<ol> <li>Use SCP to copy the archive to your local machine:</li> </ol> <pre><code>scp [project]-[environment]@[ip]:/tmp/[project]_files.tar.gz /tmp/\n</code></pre>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#step-6-extract-files-locally","title":"Step 6: Extract Files Locally","text":"<ol> <li>Navigate to the temporary directory:</li> </ol> <pre><code>cd /tmp\n</code></pre> <ol> <li>Extract the archive:</li> </ol> <pre><code>tar -xzf [project]_files.tar.gz\n</code></pre>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#step-7-copy-files-to-project","title":"Step 7: Copy Files to Project","text":"<ol> <li>Navigate to your local project directory</li> <li>Copy the necessary files to your project's <code>web/sites/default/files/</code> directory:</li> </ol> <pre><code>cp -r /tmp/app/web/sites/default/files/* /path/to/your/project/web/sites/default/files/\n</code></pre>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#example-walkthrough","title":"Example Walkthrough","text":"<p>For a project called \"skanderborg\":</p> <pre><code># Step 1: Connect to SSH\nlagoon ssh -p skanderborg -e main\n</code></pre> <pre><code># Step 2: Create archive (modules only)\ntar -cvzf /tmp/skanderborg_files_modules.tar.gz /app/web/sites/default/files/modules_local\n\n```bash\n# Step 3: Exit SSH\nexit\n\n```bash\n# Step 4: Get connection string\nlagoon ssh -p skanderborg -e main --conn-string\n</code></pre> <pre><code># Step 5: Transfer files\nscp skanderborg-main@ssh.lagoon.amazeeio.cloud:/tmp/skanderborg_files_modules.tar.gz/tmp/\n</code></pre> <pre><code># Step 6: Extract locally\ncd /tmp\ntar -xzf skanderborg_files_modules.tar.gz\n</code></pre> <pre><code># Step 7: Copy to project\ncp -r /tmp/app/web/sites/default/files/modules_local/* ./web/sites/default/files/modules_local/\n</code></pre>"},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#troubleshooting","title":"Troubleshooting","text":""},{"location":"DPL-CMS/guides/alternative-lagoon-file-transfer/#issue-ssh-connection-failed","title":"Issue: SSH Connection Failed","text":"<p>The <code>lagoon ssh</code>command fails occasionally. Usually it is in the ssh handshake phase. Just run the command again until it succeeds.</p>"},{"location":"DPL-CMS/guides/bnf-debugging/","title":"Debugging BNF content synchronization","text":"<p>Issues concerning content synchronization between BNF and individual libraries can be challenging, so here's a few helpful pointers.</p>"},{"location":"DPL-CMS/guides/bnf-debugging/#first-off","title":"First off","text":"<p>Ensure that the issue is in fact a synchronization issue. A complaint that \"the subscription doesn't work\" can turn out to be completely unrelated to content synchronization. So ask the reporter exactly what content they're missing and what they're expecting.</p>"},{"location":"DPL-CMS/guides/bnf-debugging/#cron-issues","title":"Cron issues","text":"<p>The subscriptions are run through cron on the client site. So if the problem is \"nothing's happening\", it's worth checking that cron runs and doesn't throw any errors.</p>"},{"location":"DPL-CMS/guides/bnf-debugging/#queue-issues","title":"Queue issues","text":"<p>The BNF client module has a <code>job_schedule</code> that queues subscription and node updates which is then processed by the queue system.</p>"},{"location":"DPL-CMS/guides/bnf-debugging/#subscription-issues","title":"Subscription issues","text":"<p>The subscription queue job queries for new content and queues node updates for new content. As it just queues, nothing much can go wrong here apart from GraphQL related failuers. The \"Last updated content\" property visible in the back-end corresponds to changed time of the latest queued node (modulo cron run delay), so this is an indicator as to whether the client has \"seen\" the newest nodes.</p>"},{"location":"DPL-CMS/guides/bnf-debugging/#node-syncing-issues","title":"Node syncing issues","text":"<p>The <code>NodeUpdate</code> queue worker handles the job of synchronizing new and existing nodes, and is the most likely for failures to happen, due to the complexity. It ought to log both success and failures with the relevant node UUID, so look for that. Any exceptions thrown in the process should be logged with a stack trace to ease debugging.</p>"},{"location":"DPL-CMS/guides/bnf-debugging/#but-what-about","title":"\"But what about?\"","text":"<p>Even without any issues in the above, there might be a different expectation to what should be synchronized than is actually happening. Nodes are not copied verbatim, some fields are specially handled and some paragraphs are not supported at all. In this case there's not much else to do than dig into the mapper code to determine what is actually done.</p>"},{"location":"DPL-Design-System/","title":"DPL Design System","text":"<p>DPL Design System is a library of UI components that should be used as a common base system for \"Danmarks Biblioteker\" / \"Det Digitale Folkebibliotek\". The design is implemented with Storybook / React and is output with HTML markup and css-classes through an addon in Storybook.</p> <p>The codebase follows the naming that designers have used in Figma closely to ensure consistency.</p>"},{"location":"DPL-Design-System/#requirements","title":"Requirements","text":"<p>This project comes with go-task and docker compose, hence the requirements are limited to having docker install and tasks.</p>"},{"location":"DPL-Design-System/#manual-requirements","title":"Manual requirements","text":"<p>This project can be used outside docker with the following requirements:</p> <ul> <li><code>node 16</code></li> <li><code>yarn</code></li> </ul> <p>Check in the terminal which versions you have installed with <code>node -v</code>.</p>"},{"location":"DPL-Design-System/#installation","title":"Installation","text":"<p>Use the tasks defined in <code>Taskfile</code> to run the project:</p> <pre><code>task dev:install\n</code></pre>"},{"location":"DPL-Design-System/#installation-outside-docker","title":"Installation outside docker","text":"<p>Use the node package manager to install project dependencies:</p> <pre><code>yarn install\n</code></pre>"},{"location":"DPL-Design-System/#development","title":"Development","text":"<p>To start the docker compose setup in development simple use the <code>start</code> task:</p> <pre><code>task dev:start\n</code></pre> <p>To see the output from the compile process and start of storybook:</p> <pre><code>task dev:logs\n</code></pre> <p>Use <code>task</code> and tabulator key in the terminal to see the other predefined tasks:</p> <pre><code>task dev:[TAB]\n</code></pre>"},{"location":"DPL-Design-System/#development-without-docker","title":"Development without docker","text":"<p>To start developing run:</p> <pre><code>yarn dev\n</code></pre> <p>Components and CSS will be automatically recompiled when making changes in the source code.</p>"},{"location":"DPL-Design-System/#usage","title":"Usage","text":"<p>The project is available in two ways and should be consumed accordingly:</p> <ol> <li>As package in the local npm registry for this repository</li> <li>As a <code>dist.zip</code> file attached to a release for this repository</li> </ol> <p>Both releases contain the built assets of the project: JavaScript files, CSS styles and icons.</p> <p>You can find the HTML output for a given story under the HTML tab inside storybook.</p>"},{"location":"DPL-Design-System/#npm-package","title":"NPM package","text":"<p>The GitHub NPM package registry requires authentication if you are to access packages there.</p> <p>Consequently, if you want to use the design system as an NPM package or if you use a project that depends on the design system as an NPM package you must authenticate:</p> <ol> <li>Create a GitHub token with the required scopes: <code>repo</code> and <code>read:packages</code></li> <li>Run <code>npm login --registry=https://npm.pkg.github.com</code></li> <li>Enter the following information:</li> </ol> <pre><code>&gt; Username: [Your GitHub username]\n&gt; Password: [Your GitHub token]\n&gt; Email: [An email address used with your GitHub account]\n</code></pre> <p>Note that you will need to reauthenticate when your personal access token expires.</p>"},{"location":"DPL-Design-System/#deployment-and-releases","title":"Deployment and releases","text":"<p>The project is automatically built and deployed on pushes to every branch and every tag and the result is available as releases which support both types of usage. This applies for the original repository on GitHub and all GitHub forks.</p> <p>You can follow the status of deployments in the Actions list for the repository on GitHub. The action logs also contain additional details regarding the contents and publication of each release.  If using a fork then deployment actions can be seen on the corresponding list.</p> <p>In general consuming projects should prefer tagged releases as they are stable proper releases.</p> <p>During development where the design system is being updated in parallel with the implementation of a consuming project it may be advantageous to use a release tagging a branch.</p>"},{"location":"DPL-Design-System/#tagged-releases","title":"Tagged releases","text":"<p>Run the following to publish a tag and create a release:</p> <pre><code>git tag -a v*.*.* &amp;&amp; git push origin v*.*.*\n</code></pre>"},{"location":"DPL-Design-System/#usage-npm-package","title":"Usage: npm package","text":"<p>In the consuming project update usage to the new release:</p> <pre><code>npm install @danskernesdigitalebibliotek/dpl-design-system@*.*.*\n</code></pre>"},{"location":"DPL-Design-System/#usage-release-file","title":"Usage: Release file","text":"<p>Find the release for the tag on the releases page on GitHub and download the <code>dist.zip</code> file from there and use it as needed in the consuming project.</p>"},{"location":"DPL-Design-System/#branch-releases","title":"Branch releases","text":"<p>The project automatically creates a release for each branch.</p> <p>Example: Pushing a commit to a new branch <code>feature/reservation-modal</code> will create the following parts:</p> <ol> <li>A git tag for the commit <code>release-feature/reservation-modal</code>. A tag is needed    to create a GitHub release.</li> <li>A GitHub release for the called feature/reservation-modal. The build is    attached here.</li> <li>A package in the local npm repository tagged <code>feature-reservation-modal</code>.    Special characters like <code>/</code> are not supported by npm tags and are converted    to <code>-</code>.</li> </ol> <p>Updating the branch will update all parts accordingly.</p>"},{"location":"DPL-Design-System/#usage-npm-package_1","title":"Usage: npm package","text":"<p>In the consuming project update usage to the new release:</p> <pre><code>npm install @danskernesdigitalebibliotek/dpl-design-system@feature-reservation-modal\n</code></pre> <p>If your release belongs to a fork you can use aliasing to point to the release of the package in the npm repository for the fork:</p> <pre><code>npm config set @my-fork:registry=https://npm.pkg.github.com\nnpm install @danskernesdigitalebibliotek/dpl-design-system@npm:@my-fork/dpl-design-system@feature-reservation-modal\n</code></pre> <p>This will update your <code>package.json</code> and lock files accordingly. Note that branch releases use temporary versions in the format <code>0.0.0-[GIT-SHA]</code> and you may in practice see these referenced in both files.</p> <p>If you push new code to the branch you have to update the version used in the consuming project:</p> <pre><code>npm update @danskernesdigitalebibliotek/dpl-design-system\n</code></pre> <p>Aliasing, repository configuration and updating installed packages are also supported by Yarn.</p>"},{"location":"DPL-Design-System/#usage-release-file_1","title":"Usage: Release file","text":"<p>Find the release for the branch on the releases page on GitHub and download the <code>dist.zip</code> file from there and use it as needed in the consuming project.</p> <p>If your branch belongs to a fork then you can find the release on the releases page for the fork.</p> <p>Repeat the process if you push new code to the branch.</p>"},{"location":"DPL-Design-System/#storybook","title":"Storybook","text":"<p>Spin up storybook by running this command in the terminal:</p> <pre><code>yarn storybook\n</code></pre> <p>When storybook is ready it automatically opens up in a browser with the interface ready to use.</p>"},{"location":"DPL-Design-System/#chromatic","title":"Chromatic","text":"<p>We are using Chromatic for visual test. You can access the dashboard under the <code>danskernesdigitalebibliotek</code> (organisation) <code>dpl-design-system</code> (project).</p> <p>https://www.chromatic.com/builds?appId=616ffdab9acbf5003ad5fd2b</p> <p>You can deploy a version locally to Chromatic by running:</p> <pre><code>yarn chromatic\n</code></pre> <p>Make sure to set the <code>CHROMATIC_PROJECT_TOKEN</code> environment variable is available in your shell context. You can access the token from:</p> <p>https://www.chromatic.com/manage?appId=616ffdab9acbf5003ad5fd2b&amp;view=configure</p>"},{"location":"DPL-Design-System/#what-is-storybook","title":"What is Storybook","text":"<p>Storybook is an open source tool for building UI components and pages in isolation from your app's business logic, data, and context. Storybook helps you document components for reuse and automatically visually test your components to prevent bugs. It promotes the component-driven process and agile development.</p> <p>It is possible to extend Storybook with an ecosystem of addons that help you do things like fine-tune responsive layouts or verify accessibility.</p>"},{"location":"DPL-Design-System/#how-to-use","title":"How to use","text":"<p>The Storybook interface is simple and intuitive to use. Browse the project's stories now by navigating to them in the sidebar.</p> <p>The stories are placed in a flat structure, where developers should not spend time thinking of structure, since we want to keep all parts of the system under a heading called Library. This Library is then dividid in folders where common parts are kept together.</p> <p>To expose to the user how we think these parts stitch together for example for the new website, we have a heading called Blocks, to resemble what cms blocks a user can expect to find when building pages in the choosen CMS.</p> <p>This could replicate in to mobile applications, newsletters etc. all pulling parts from the Library.</p> <p>Each story has a corresponding <code>.stories</code> file. View their code in the <code>src/stories</code> directory to learn how they work. The <code>stories</code> file is used to add the component to the Storybook interface via the title. Start the title with \"Library\" or \"Blocks\" and use / to divide into folders fx. <code>Library / Buttons / Button</code></p>"},{"location":"DPL-Design-System/#addons","title":"Addons","text":"<p>Storybook ships with some essential pre-installed addons to power the core Storybook experience.</p> <ul> <li>Controls</li> <li>Actions</li> <li>Docs</li> <li>Viewport</li> <li>Backgrounds</li> <li>Toolbars</li> <li>Measure</li> <li>Outline</li> </ul> <p>There are many other helpful addons to customise the usage and experience. Additional addons used for this project:</p> <ul> <li> <p>HTML / storybook-addon-html:   This addon is used to display compiled HTML markup for each story and make it   easier for developers to grab the code. Because we are developing with React,   it is necessary to be able to show the HTML markup with the css-classes to   make it easier for other developers that will implement it in the future.   If a story has controls the HTML markup changes according to the controls that   are set.</p> </li> <li> <p>Designs / storybook-addon-designs:   This addon is used to embed Figma in the addon panel for a better   design-development workflow.</p> </li> <li> <p>A11Y:   This addon is used to check the accessibility of the components.</p> </li> </ul> <p>All the addons can be found in <code>storybook/main.js</code> directory.</p>"},{"location":"DPL-Design-System/#important-to-notice","title":"Important to notice","text":""},{"location":"DPL-Design-System/#internal-classes","title":"Internal classes","text":"<p>To display some components (fx Colors, Spacing) in a more presentable way, we are using some \"internal\" css-classes which can be found in the <code>styles/internal.scss</code> file. All css-classes with \"internal\" in the front should therefore be ignored in the HTML markup.</p>"},{"location":"DPL-Design-System/Icon-guidelines/","title":"Icon Usage Guidelines","text":"<p>This folder contains SVG icons that are used in the design system. When using icons from this folder, please follow the guidelines below:</p> <ul> <li> <p>Icons located in public/ folder is the current source of truth. All icons will be placed in this folder. Ensure that icons placed gere are using the <code>class</code> attribute, and not <code>className</code> for SVG element.</p> </li> <li> <p>For icons used in React components that require class modifications or similar, first create the icon in public/icons, and then manually copy the SVG from the <code>public/icons</code> folder and make any necessary changes, i.e replacing the <code>class</code> with <code>className</code> or other modifications.</p> </li> </ul> <p>Currently, there is no automated solution for this process that ensures consistency for this. This may be changed in the future.</p>"},{"location":"DPL-Design-System/layout-documentation/","title":"Project Layout documentation &amp; Examples","text":""},{"location":"DPL-Design-System/layout-documentation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Variables</li> <li>Mixins</li> <li>Example Usage</li> </ul>"},{"location":"DPL-Design-System/layout-documentation/#introduction","title":"Introduction","text":"<p>This documentation provides detailed information on the CSS/SCSS standards for the 'formidling' project. It covers the usage of mixins for layout and spacing, as well as guidelines for responsive design.</p>"},{"location":"DPL-Design-System/layout-documentation/#note-on-applicability","title":"Note on Applicability","text":"<p>This document and its standards apply specifically to elements of the 'formidling' project starting from December 1, 2023, and onwards. It has been created to aligns with the recent developments in the project and is not retroactively applied to earlier components or structures.</p> <p>Layout related SCSS is defined in the <code>variables.layout.scss</code> file.</p> <p>If anything in <code>variables.layout.scss</code> is modified or extended, make sure you include your changes in this documentation.</p>"},{"location":"DPL-Design-System/layout-documentation/#variables","title":"Variables","text":"<p>The file defines several variables for managing layout and spacing:</p> <ul> <li><code>$layout__max-width--*</code>: These variables store the maximum width values aligned with breakpoints.</li> <li><code>block__max-width--*</code>: These variables store the maximum width values between block (paragraph) elements.</li> <li><code>$layout__edge-spacing</code>: This variable stores the edge spacing (padding) value for containers.</li> </ul> <p>All components that require a max-width should use the <code>block__max-width--*</code> variables along with layout-container.</p>"},{"location":"DPL-Design-System/layout-documentation/#mixins","title":"Mixins","text":"<p>The file defines two mixins:</p> <ul> <li> <p><code>layout-container($max-width, $padding)</code>: This mixin sets the maximum width,  and padding of an element, as well as centering them.</p> </li> <li> <p><code>block-spacing($modifier)</code>: This manages the vertical margins of elements, allowing for consistent spacing throughout the project. Use <code>$modifer</code> to create negative/sibling styles.</p> </li> </ul> <p>Vertical padding is not a part of layout.scss. Use regular <code>$spacing</code> variables for that.</p> <p><code>block-spacing($modifier)</code> is an approach use for the CMS, where all paragraphs are rendered with a wrapper that will automatically add necessary spacing between the components. Any component that is not a paragraph, should therefore follow this approach by including the mixin <code>@mixin block-spacing</code></p> <p>The <code>$modfier</code> is currently a either</p> <ul> <li><code>sibling</code> used for add a -1px to remove borders between sibling elements.</li> <li><code>negative</code> used for elements that require -margin instead.</li> </ul>"},{"location":"DPL-Design-System/layout-documentation/#example-usage","title":"Example Usage","text":"<p>Here are some examples of how to use these mixins, and utility classes.</p>"},{"location":"DPL-Design-System/layout-documentation/#including-mixins-in-components-using-bem","title":"Including mixins in components using BEM","text":"<pre><code>// Using mixins in your BEM-named parent container.\n.your-BEM-component-name {\n  @include layout-container;\n\n  @include media-query__small() {\n    // Applying new edge spacing (Padding) using $spacings / other.\n    @include layout-container($padding: $s-xl);\n  }\n\n @include media-query__large() {\n    // Removing max-width &amp; applying specific padding from $spacings / other\n     @include layout-container($max-width: 0, $padding: $s-md);\n  }\n}\n</code></pre>"},{"location":"DPL-Design-System/layout-documentation/#adding-spacing-to-non-paragraph-elements-in-the-cms","title":"Adding spacing to non-paragraph elements in the cms","text":"<pre><code>// Using block-spacing for non-paragraph component or container.\n.your-BEM-component-name {\n  @include block-spacing;\n}\n</code></pre>"},{"location":"DPL-Design-System/scss/","title":"SCSS strategy","text":"<p>In December 2023, we have aimed to streamline the way we write SCSS. Some of these rules have not been applied on previous code, but moving forward, this is what we aim to do.</p>"},{"location":"DPL-Design-System/scss/#bem-naming-convention","title":"BEM naming convention","text":""},{"location":"DPL-Design-System/scss/#examples-of-dos-and-donts","title":"Examples of do's and dont's","text":"<p>Assuming we have a Counter block:</p> <ul> <li>Styling must be placed in a correspondingly named file <code>counter.scss</code></li> <li><code>.counter__title</code> \u2705</li> <li><code>&amp;__title</code> \u274c (<code>&amp;__</code> should be avoided, to avoid massive indention.)</li> <li><code>.counter-title</code> \u274c (Must start with <code>.FILE-NAME__</code>)</li> <li><code>.counter__title__text</code> \u274c (Only one level)</li> </ul>"},{"location":"DPL-Design-System/scss/#variants-and-modifiers","title":"Variants and modifiers","text":"<p>Sometimes you'll want to add variants to CSS-only classes. This can be done using modifier classes - e.g. <code>.counter--large</code>, <code>.counter__title--large</code>. These classes must not be set alone. E.g. <code>.counter__title--large</code> must not exist on an element without also having <code>.counter__title</code>.</p>"},{"location":"DPL-Design-System/scss/#mixins-placeholder-and-variables","title":"Mixins, placeholder and variables","text":"<p>Shared tooling is saved in src/styles/scss/tools, NOT in individual stories.</p>"},{"location":"DPL-Design-System/scss/#typography","title":"Typography","text":"<p>Typography is defined in src/styles/scss/tools/variables.typography.scss. These variables, all starting with <code>$typo__</code>, can be used, using a mixin, <code>@include typography($typo__h2);</code> in stories. Generally speaking, font styling should be avoided directly in stories, rather adding new variants in the <code>variables.typography.scss</code> file. This way, we can better keep track of what is available, and avoid duplicate styling in the future.</p>"},{"location":"DPL-Design-System/scss/#legacy-classes","title":"Legacy classes","text":"<p>In the future, we want to apply these rules to old code too. Until then, the old classes are supported using the files in src/styles/scss/legacy.</p> <p>These classes should not be used in new components</p>"},{"location":"DPL-Design-System/architecture/adr-001-skeleton-screens/","title":"Architecture Decision Record: Skeleton Screens","text":""},{"location":"DPL-Design-System/architecture/adr-001-skeleton-screens/#context","title":"Context","text":"<p>In the work of trying to improve the performance of the search results we needed a way to fill the viewport with a simulated interface in order to:</p> <ul> <li>Show some content immediately to the user</li> <li>Prevent layout shifting between loading state and ready state</li> </ul>"},{"location":"DPL-Design-System/architecture/adr-001-skeleton-screens/#decision","title":"Decision","text":"<p>We decided to implement skeleton screens when loading data. The skeleton screens are rendered in pure css. The css classes are coming from the library: skeleton-screen-css</p>"},{"location":"DPL-Design-System/architecture/adr-001-skeleton-screens/#alternatives-considered","title":"Alternatives considered","text":"<p>The library is very small and based on simple css rules, so we could have considered replicating it in our own design system or make something similar. But by using the open source library we are ensured, to a certain extent, that the code is being maintained, corrected and evolves as time goes by.</p> <p>We could also have chosen to use images or GIF's to render the screens. But by using the simple toolbox of skeleton-screen-css we should be able to make screens for all the different use cases in the different apps.</p>"},{"location":"DPL-Design-System/architecture/adr-001-skeleton-screens/#consequences","title":"Consequences","text":"<p>It is now possible, with a limited amount of work, to construct skeleton screens in the loading state of the various user interfaces.</p> <p>Because we use library where skeletons are implemented purely in CSS we also provide a solution which can be consumed in any technology already using the design system without any additional dependencies, client side or server side.</p>"},{"location":"DPL-Design-System/architecture/adr-001-skeleton-screens/#bem-rules-when-using-skeleton-screen-classes-in-dpl-design-system","title":"BEM rules when using Skeleton Screen Classes in dpl-design-system","text":"<p>Because we want to use existing styling setup in conjunction with the Skeleton Screen Classes we sometimes need to ignore the existing BEM rules that we normally comply to. See eg. the search result styling.</p>"},{"location":"DPL-Design-System/architecture/adr-002-form-styling/","title":"Architecture Decision Record: Form Styling","text":""},{"location":"DPL-Design-System/architecture/adr-002-form-styling/#context","title":"Context","text":"<p>There are various types of forms within the project, and it is always a dilemma as to whether to write specific styling per form, or to create a common set of base classes.</p>"},{"location":"DPL-Design-System/architecture/adr-002-form-styling/#decision","title":"Decision","text":"<p>We have decided to create a set of <code>default classes</code> to be used when building different kinds of forms, as to not create a large amount of location that contain form styling. Considering the forms within the project all look very similar/consist of elements that look the same, it will be an advantage to have a centralized place to expand/apply future changes to.</p> <p>As we follow the BEM class structure, the block is called <code>dpl-form</code>, which can be expanded with elements, and modifiers.</p>"},{"location":"DPL-Design-System/architecture/adr-002-form-styling/#alternatives-considered","title":"Alternatives considered","text":"<p>We considered writing new classes every time we introduced a new form, however, this seemed like the inferior option. If a specific form element was to change styling in the future, we would have to adjust all of the specific instances, instead of having a singular definition. And in case a specific instance needs to adopt a different styling, it can be achieved by creating a specific class fot that very purpose.</p>"},{"location":"DPL-Design-System/architecture/adr-002-form-styling/#consequences","title":"Consequences","text":"<p>As per this decision, we expect introduction of new form elements to be styled expanding the current <code>dpl-form</code> class.</p> <p>This currently has an exception in form of form inputs - these have been styled a long time ago and use the class <code>dpl-input</code>.</p>"},{"location":"DPL-Design-System/architecture/adr-002-form-styling/#implementation-in-the-dpl-design-system","title":"Implementation in the dpl-design-system","text":"<p>Here is the link to our form css file.</p>"},{"location":"DPL-Platform/","title":"DPL Platform Documentation","text":"<p>This directory contains the documentation of the DPL Platforms architecture and overall concepts.</p> <p>Documentation of how to use the various sub-components of the project can be found in READMEs in the respective components directory.</p>"},{"location":"DPL-Platform/#table-of-contents","title":"Table of contents","text":"<ul> <li>Architecture Contains the documentation of the platforms architecture.</li> <li>Backup How backups of sites are handled.</li> <li>Current Platform Environments Describes the current   operational environments.</li> </ul>"},{"location":"DPL-Platform/backup/","title":"Backup","text":""},{"location":"DPL-Platform/backup/#site-backup-configuration","title":"Site backup configuration","text":"<p>We configure all production backups with a backup schedule that ensure that the site is backed up at least once a day.</p> <p>Backups executed by the k8up operator follows a backup schedule and then uses Restic to perform the backup itself. The backups are stored in a Azure Blob Container, see the Environment infrastructure for a depiction of its place in the architecture.</p> <p>The backup schedule and retention is configured via the individual sites <code>.lagoon.yml</code>. The file is re-rendered from a template every time the a site is deployed. The templates for the different site types can be found as a part of dpladm.</p> <p>Refer to the lagoon documentation on backups for more general information.</p> <p>Refer to any runbooks relevant to backups for operational instructions on eg. retrieving a backup.</p>"},{"location":"DPL-Platform/code-guidelines/","title":"Code guidelines","text":"<p>The following guidelines describe best practices for developing code for the DPL Platform project. The guidelines should help achieve:</p> <ul> <li>A stable, secure and high quality foundation for building and maintaining   the platform and its infrastructure.</li> <li>Consistency across multiple developers participating in the project</li> </ul> <p>Contributions to the core DPL Platform project will be reviewed by members of the Core team. These guidelines should inform contributors about what to expect in such a review. If a review comment cannot be traced back to one of these guidelines it indicates that the guidelines should be updated to ensure transparency.</p>"},{"location":"DPL-Platform/code-guidelines/#coding-standards","title":"Coding standards","text":"<p>The project follows the Drupal Coding Standards and best practices for all parts of the project: PHP, JavaScript and CSS. This makes the project recognizable for developers with experience from other Drupal projects. All developers are expected to make themselves familiar with these standards.</p> <p>The following lists significant areas where the project either intentionally expands or deviates from the official standards or areas which developers should be especially aware of.</p>"},{"location":"DPL-Platform/code-guidelines/#general","title":"General","text":"<ul> <li>The default language for all code and comments is English.</li> </ul>"},{"location":"DPL-Platform/code-guidelines/#shell-scripts","title":"Shell scripts","text":"<ul> <li>Shell-scripts must pass a shellcheck validation</li> </ul>"},{"location":"DPL-Platform/code-guidelines/#terraform","title":"Terraform","text":"<ul> <li>Any Terraform HCL must be formatted to match the format required by   <code>terraform fmt</code></li> <li>Terraform configuration should be organized into submodules instantiated by   root modules.</li> </ul>"},{"location":"DPL-Platform/code-guidelines/#markdown","title":"Markdown","text":"<ul> <li>Markdown must pass validation by markdownlint</li> </ul>"},{"location":"DPL-Platform/code-guidelines/#code-comments","title":"Code comments","text":"<p>Code comments which describe what an implementation does should only be used for complex implementations usually consisting of multiple loops, conditional statements etc.</p> <p>Inline code comments should focus on why an unusual implementation has been implemented the way it is. This may include references to such things as business requirements, odd system behavior or browser inconsistencies.</p>"},{"location":"DPL-Platform/code-guidelines/#commit-messages","title":"Commit messages","text":"<p>Commit messages in the version control system help all developers understand the current state of the code base, how it has evolved and the context of each change. This is especially important for a project which is expected to have a long lifetime.</p> <p>Commit messages must follow these guidelines:</p> <ol> <li>Each line must not be more than 72 characters long</li> <li>The first line of your commit message (the subject) must contain a short    summary of the change. The subject should be kept around 50 characters long.</li> <li>The subject must be followed by a blank line</li> <li>Subsequent lines (the body) should explain what you have changed and why the    change is necessary. This provides context for other developers who have not    been part of the development process. The larger the change the more    description in the body is expected.</li> <li>If the commit is a result of an issue in a public issue tracker,    platform.dandigbib.dk, then the subject must start with the issue number   followed by a colon (:). If the commit is a result of a private issue tracker   then the issue id must be kept in the commit body.</li> </ol> <p>When creating a pull request the pull request description should not contain any information that is not already available in the commit messages.</p> <p>Developers are encouraged to read How to Write a Git Commit Message by Chris Beams.</p>"},{"location":"DPL-Platform/code-guidelines/#tool-support","title":"Tool support","text":"<p>The project aims to automate compliance checks as much as possible using static code analysis tools. This should make it easier for developers to check contributions before submitting them for review and thus make the review process easier.</p> <p>The following tools pay a key part here:</p> <ol> <li>terraform fmt for standard    Terraform formatting.</li> <li>markdownlint-cli2 for    linting markdown files. The tool is configured via /.markdownlint-cli2.yaml</li> <li>ShellCheck with its default configuration.</li> </ol> <p>In general all tools must be able to run locally. This allows developers to get quick feedback on their work.</p> <p>Tools which provide automated fixes are preferred. This reduces the burden of keeping code compliant for developers.</p> <p>Code which is to be exempt from these standards must be marked accordingly in the codebase - usually through inline comments (markdownlint, ShellCheck). This must also include a human readable reasoning. This ensures that deviations do not affect future analysis and the Core project should always pass through static analysis.</p> <p>If there are discrepancies between the automated checks and the standards defined here then developers are encouraged to point this out so the automated checks or these standards can be updated accordingly.</p>"},{"location":"DPL-Platform/dns/","title":"DNS setup for library sites","text":"<p>When going live with a library site, DNS needs to be updated. Update all domain names configured on the site to the IP/name documented in Current Platform environments.</p> <p>A records should point to <code>20.86.109.250</code></p> <p>CNAME records should point to <code>cluster-1.folkebibliotekernescms.dk</code></p> <p>If any of the domains have CAA records, they should be updated according to ZeroSSL's guide.</p> <p>This is documented for the library staff in the end user documentation.</p>"},{"location":"DPL-Platform/lagoon-projects/","title":"Lagoon projects","text":"<p>The Lagoon instance at <code>dplplat01.dpl.reload.dk</code> contains a range of sites of a slightly different nature.</p>"},{"location":"DPL-Platform/lagoon-projects/#development-sites","title":"Development sites","text":"<p>Development sites set up directly in Lagoon. These usually automatically build select branches and pull requests and are primarily for testing in development.</p> <p>Currently this includes <code>dpl-cms</code> project (main development) and the <code>dpl-bnf</code> project (\"Bibliotekernes Nationale Formidling\" content sharing system).</p>"},{"location":"DPL-Platform/lagoon-projects/#productionstaging-sites","title":"Production/staging sites","text":"<p>Sites managed by +<code>sites.yml</code>. This process creates a GIT repository in the <code>danishpubliclibraries</code> GitHub organization. These repositories is then updated when running the <code>task sites:sync</code>, which causes Lagoon to deploy the new version.</p>"},{"location":"DPL-Platform/maintenance-plan/","title":"Maintenance plan","text":"<p>This document outlines every piece of the platform that need upgrading and updating.</p>"},{"location":"DPL-Platform/maintenance-plan/#lagoon","title":"Lagoon","text":"<p>Update interval: 1-4 months Release URL: https://github.com/uselagoon/lagoon/releases Upgrade Docs URL: Expect downtime: Highly likely Time to do: Unknown at the time of writing (most likely a day the first time) Runbook: Upgrading Lagoon Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#aks","title":"AKS","text":"<p>Update interval: monthly Release URL: https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions?tabs=azure-cli Upgrade Docs URL:   https://learn.microsoft.com/en-us/azure/aks/upgrade-aks-cluster?tabs=azure-portal Expect downtime: yes, there'll be a very short outage alike to releases Time to do: 2-6 hours Runbook: Update AKS Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#support-workloads","title":"Support Workloads","text":"<p>We have a number of support workloads. Which at the time of writing is all Helm Charts. Information on Helm upgrades can be found on the Helm website. There's a general runbook for upgrading the support workloads here: Upgrade Support Workloads.</p>"},{"location":"DPL-Platform/maintenance-plan/#cert-manager","title":"Cert-manager","text":"<p>Update interval: every 6 months Release URL: https://github.com/cert-manager/cert-manager/releases Upgrade Docs URL: https://cert-manager.io/docs/installation/upgrade/ Expect downtime: Unknown, but likely none Time to do: Unknown, but likely &lt; 1 hour Runbook: Upgrade Cert-manager. Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#grafana","title":"Grafana","text":"<p>Update interval: Quaterly Release URL: https://github.com/grafana/grafana/releases Upgrade Docs URL: Expect downtime: No, maybe a little for Grafana, but nothing that affects the   libraries. Time to do: Unknown Runbook: Upgrade Grafana Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#harbor","title":"Harbor","text":"<p>Update interval: Half yearly Release URL: https://github.com/goharbor/harbor-helm/releases Upgrade Docs URL: https://github.com/goharbor/harbor-helm/blob/main/docs/Upgrade.md Expect downtime: Harbor will have downtime, it will affect sites that need   redeployment as well as developers who firing up and environment. Time to do: Unknown Runbook: Upgrading Harbor Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#nginx-ingress-controller","title":"Nginx Ingress Controller","text":"<p>Update interval: Half yearly Release URL: https://github.com/kubernetes/ingress-nginx/releases Upgrade Docs URL: https://kubernetes.github.io/ingress-nginx/deploy/upgrade/#with-helm Expect downtime: Highly likely Time to do: Unkonwn Runbook: Upgrading Nginx-ingresss Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#k8up","title":"K8up","text":"<p>DO NOT UPGRADE. From time to time we should checkin with this page K8Up in lagoon , where they'll hopefully give and update when it is possible to update K8Up to a later version than version 1.x.x Check every quarter.</p>"},{"location":"DPL-Platform/maintenance-plan/#loki","title":"Loki","text":"<p>Update interval: Half yearly Release URL: https://artifacthub.io/packages/helm/grafana/loki Upgrade Docs URL: https://grafana.com/docs/loki/latest/setup/upgrade/ Expect downtime: Might be some, but nothing that will hit the libraries Time to do: Unkown Runbook: Upgrade Loki Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#minio","title":"Minio","text":"<p>As we're replacing Minio, we will have to do a section on what ever tool   we're replacing it with.</p>"},{"location":"DPL-Platform/maintenance-plan/#prometheus","title":"Prometheus","text":"<p>Update interval: Half yearly Release URL: https://github.com/prometheus-community/helm-charts/releases?q=kube-prometheus-stack&amp;expanded=true Upgrade Docs URL: https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack#upgrading-chart Expect downtime: Probably, but nothing the libraries will be affected by Time to do: Unkown Runbook: Upgrading Prometheus Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#promtail","title":"Promtail","text":"<p>Update interval: Half yearly Release URL: https://github.com/grafana/helm-charts/releases?q=promtail&amp;expanded=true Upgrade Docs URL: https://github.com/grafana/helm-charts/blob/main/charts/promtail/README.md#upgrading Expect downtime: Probably, but nothing that concerns the libraries. Time to do: Unknown Runbook: Upgrading Promtail Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#dpl-shell-parts","title":"DPL Shell parts","text":"<p>The DPL Shell integrates our day to day tools. The ones, that are not upgraded automatically when a new version of DPL Shell is created, are listed below. Most of these, if not all, are watched by dependabot on GitHub, so we are notified about updates pretty quickly.</p>"},{"location":"DPL-Platform/maintenance-plan/#terraform","title":"Terraform","text":"<p>Update interval: Yearly Release URL: https://hub.docker.com/r/hashicorp/terraform/tags?page=1&amp;ordering=last_updated Expect downtime: None Time to do: 1 hour Runbook: Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#azure-cli","title":"Azure CLI","text":"<p>Update interval: Half yearly Release URL: https://mcr.microsoft.com/v2/azure-cli/tags/list Expect downtime: None Time to do: 1 hour Runbook: Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#kubelogin","title":"kubelogin","text":"<p>Update interval: Quarterly Release URL: https://github.com/Azure/kubelogin/releases Expect downtime: None Time to do: 1 hour Runbook: Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#task","title":"Task","text":"<p>Update interval: Half yearly Release URL: https://github.com/go-task/task/releases Expect downtime: None Time to do: 1 hour Runbook: Notes:</p>"},{"location":"DPL-Platform/maintenance-plan/#lagoon-cli","title":"Lagoon CLI","text":"<p>Update interval: Quarterly Release URL: https://github.com/uselagoon/lagoon-cli/releases Expect downtime: None Time to do: 1 hour Runbook: Notes: Dependabot has not as of yet notified us of any available updates,   so we have to check manually.</p>"},{"location":"DPL-Platform/maintenance-plan/#helm","title":"Helm","text":"<p>Update interval: Quarterly Release URL: https://hub.docker.com/r/alpine/helm/tags?page=1&amp;ordering=last_updated Expect downtime: None Time to do: 1 hour Runbook: Notes:</p>"},{"location":"DPL-Platform/platform-environments/","title":"Current Platform environments","text":""},{"location":"DPL-Platform/platform-environments/#dplplat01","title":"dplplat01","text":""},{"location":"DPL-Platform/platform-environments/#roots","title":"Roots","text":"<ul> <li>Environment repository Github Organisation: github.com/danishpubliclibraries</li> <li>Azure Resource Group: <code>rg-env-dplplat01</code></li> </ul>"},{"location":"DPL-Platform/platform-environments/#urls","title":"URLs","text":"<ul> <li>Base domain: <code>dplplat01.dpl.reload.dk</code></li> <li>Grafana: https://grafana.lagoon.dplplat01.dpl.reload.dk/</li> <li>Lagoon UI: https://ui.lagoon.dplplat01.dpl.reload.dk/</li> </ul>"},{"location":"DPL-Platform/platform-environments/#dns-setup-for-library-sites","title":"DNS setup for library sites","text":"<p>IP: 20.86.109.250 CNAME: cluster-1.folkebibliotekernescms.dk</p> <p>(See DNS for setup)</p>"},{"location":"DPL-Platform/platform-environments/#lagoon-cli-configuration","title":"Lagoon CLI configuration","text":"<ul> <li>Lagoon name: dplplat01</li> <li>Lagoon UI: https://ui.lagoon.dplplat01.dpl.reload.dk/</li> <li>GraphQL endpoint: https://api.lagoon.dplplat01.dpl.reload.dk/graphql</li> <li>SSH host: 20.238.147.183</li> <li>SSH port: 22</li> </ul>"},{"location":"DPL-Platform/platform-environments/#obtaining-lagoon-cli-configuration","title":"Obtaining Lagoon CLI configuration","text":"<p>See Connecting the Lagoon CLI</p>"},{"location":"DPL-Platform/architecture/","title":"DPL Platform architecture documentation","text":"<ul> <li>Architecture Decision Records (ADR) describes the reasoning behind key   decisions made during the design and implementation of the platforms   architecture. These documents stands apart from the remaining documentation in   that they keep a historical record, while the rest of the documentation is a   snapshot of the current system.</li> <li>Platform Environment Architecture   gives an overview of the parts that makes up a single DPL Platform environment.</li> <li>Performance strategy Describes the approach the   platform takes to meet performance requirements.</li> </ul>"},{"location":"DPL-Platform/architecture/alertmanager-setup/","title":"Alertmanager Setup","text":"<p>We use the alertmanager which automatically ties to the metrics of Prometheus but in order to make it work the configuration and rules need to be setup.</p>"},{"location":"DPL-Platform/architecture/alertmanager-setup/#configuration","title":"Configuration","text":"<p>The configuration is stored in a secret:</p> <pre><code>kubectl get secret \\\n  -n prometheus alertmanager-promstack-kube-prometheus-alertmanager -o yaml\n</code></pre> <p>In order to update the configuration you need to get the secret resource definition yaml output and retrieve the <code>data.alertmanager.yaml</code> property.</p> <p>You need to base64 decode the value, update configuration with SMTP settings, receivers and so forth.</p>"},{"location":"DPL-Platform/architecture/alertmanager-setup/#rules","title":"Rules","text":"<p>It is possible to set up various rules(thresholds), both on cluster level and for separate containers and namespaces.</p> <p>Here is a site with examples of rules to get an idea of the possibilities.</p>"},{"location":"DPL-Platform/architecture/alertmanager-setup/#test","title":"Test","text":"<p>We have tested the setup by making a configuration looking like this:</p> <p>Get the configuration form the secret as described above.</p> <p>Change it with smtp settings in order to be able to debug the alerts:</p> <pre><code>global:\n  resolve_timeout: 5m\n  smtp_smarthost: smtp.gmail.com:587\n  smtp_from: xxx@xxx.xx\n  smtp_auth_username: xxx@xxx.xx\n  smtp_auth_password: xxxx\nreceivers:\n- name: default\n- name: email-notification\n  email_configs:\n    - to: xxx@xxx.xx\nroute:\n  group_by:\n  - namespace\n  group_interval: 5m\n  group_wait: 30s\n  receiver: default\n  repeat_interval: 12h\n  routes:\n  - match:\n      alertname: testing\n    receiver: email-notification\n  - match:\n      severity: critical\n    receiver: email-notification\n</code></pre> <p>Base64 encode the configuration and update the secret with the new configuration hash.</p> <p>Find the cluster ip of the alertmanager service running (the service name can possibly vary):</p> <pre><code>kubectl get svc -n prometheus promstack-kube-prometheus-alertmanager\n</code></pre> <p>And then run a curl command in the cluster (you need to find the IP o):</p> <pre><code># 1.\nkubectl run -i --rm --tty debug --image=curlimages/curl --restart=Never -- sh\n\n# 2\ncurl -XPOST http://[ALERTMANAGER_SERVICE_CLUSTER_IP]:9093/api/v1/alerts \\\n -d '[{\"status\": \"firing\",\"labels\": {\"alertname\": \"testing\",\"service\": \"curl\",\\\n \"severity\": \"critical\",\"instance\": \"0\"},\"annotations\": {\"summary\": \\\n \"This is a summary\",\"description\": \"This is a description.\"},\"generatorURL\": \\\n \"http://prometheus.int.example.net/&lt;generating_expression&gt;\",\\\n \"startsAt\": \"2020-07-22T01:05:38+00:00\"}]'\n</code></pre>"},{"location":"DPL-Platform/architecture/performance-strategy/","title":"Performance strategy","text":"<p>The DPL-CMS Drupal sites utilizes a multi-tier caching strategy. HTTP responses are cached by Varnish and Drupal caches its various internal data-structures in a Redis key/value store.</p>"},{"location":"DPL-Platform/architecture/performance-strategy/#the-request-path","title":"The request-path","text":"<ol> <li>All inbound requests are passed in to an Ingress Nginx controller which    forwards the traffic for the individual sites to their individual Varnish    instances.</li> <li>Varnish serves up any static or anonymous responses it has cached from its    object-store.</li> <li>If the request is cache miss the request is passed further on Nginx which    serves any requests for static assets.</li> <li>If the request is for a dynamic page the request is forwarded to the Drupal-    installation hosted by PHP-FPM.</li> <li>Drupal bootstraps, and produces the requested response.  <ul> <li>During this process it will either populate or reuse it cache which is   stored in Redis.  </li> <li>Depending on the request Drupal will execute a number of queries against   MariaDB and a search index.</li> </ul> </li> </ol>"},{"location":"DPL-Platform/architecture/performance-strategy/#caching-of-http-responses","title":"Caching of http responses","text":"<p>Varnish will cache any http responses that fulfills the following requirements</p> <ul> <li>Is not associated with a php-session (ie, the user is logged in)</li> <li>Is a 200</li> </ul> <p>Refer the Lagoon drupal.vcl, docs.lagoon.sh documentation on the Varnish service and the varnish-drupal image for the specifics on the service.</p> <p>Refer to the caching documentation in dpl-cms for specifics on how DPL-CMS is integrated with Varnish.</p>"},{"location":"DPL-Platform/architecture/performance-strategy/#redis-as-caching-backend","title":"Redis as caching backend","text":"<p>DPL-CMS is configured to use Redis as the backend for its core cache as an alternative to the default use of the sql-database as backend. This ensures that  a busy site does not overload the shared mariadb-server.</p>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/","title":"A DPL Platform environment","text":"<p>A DPL Platform environment consists of a range of infrastructure components on top of which we run a managed Kubernetes instance into with we install a number of software product. One of these is Lagoon which gives us a platform for hosting library sites.</p> <p>An environment is created in two separate stages. First all required infrastructure resources are provisioned, then a semi-automated deployment process carried out which configures all the various software-components that makes up an environment. Consult the relevant runbooks and the DPL Platform Infrastructure documents for the guides on how to perform the actual installation.</p> <p>This document describes all the parts that makes up a platform environment raging from the infrastructure to the sites.</p> <ul> <li>Azure Infrastructure describes the raw cloud infrastructure</li> <li>Software Components describes the base software products   we install to support the platform including Lagoon</li> <li>Sites describes how we define the individual sites on a platform and   the approach the platform takes to deployment.</li> </ul>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#azure-infrastructure","title":"Azure Infrastructure","text":"<p>All resources of a Platform environment is contained in a single Azure Resource Group. The resources are provisioned via a Terraform setup that keeps its resources in a separate resource group.</p> <p>The overview of current platform environments along with the various urls and a summary of its primary configurations can be found the Current Platform environments document.</p> <p>A platform environment uses the following Azure infrastructure resources.</p> <p></p> <ul> <li>A virtual Network - with a subnet, configured with access to a number of services.</li> <li>Separate storage accounts for</li> <li>Monitoring data (logs)</li> <li>Lagoon files (eg. results of running user-triggered administrative actions)</li> <li>Backups</li> <li>Drupal site files</li> <li>A MariaDB used to host the sites databases.</li> <li>A Key Vault that holds administrative credentials to resources that Lagoon   needs administrative access to.</li> <li>An Azure Kubernetes Service cluster that hosts the platform itself.</li> <li>Two Public IPs: one for ingress one for egress.</li> </ul> <p>The Azure Kubernetes Service in return creates its own resource group that contains a number of resources that are automatically managed by the AKS service. AKS also has a managed control-plane component that is mostly invisible to us. It has a separate managed identity which we need to grant access to any additional infrastructure-resources outside the \"MC\" resource-group that we need AKS to manage.</p>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#software-components","title":"Software Components","text":"<p>The Platform consists of a number of software components deployed into the AKS cluster. The components are generally installed via Helm, and their configuration controlled via values-files.</p> <p>Essential configurations such as the urls for the site can be found in the wiki</p> <p>The following sections will describe the overall role of the component and how it integrates with other components. For more details on how the component is configured, consult the corresponding values-file for the component found in the individual environments  configuration folder.</p> <p></p>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#lagoon","title":"Lagoon","text":"<p>Lagoon is an Open Soured Platform As A Service created by Amazee. The platform builds on top of a Kubernetes cluster, and provides features such as automated builds and the hosting of a large number of sites.</p>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#ingress-nginx","title":"Ingress Nginx","text":"<p>Kubernetes does not come with an Ingress Controller out of the box. An ingress- controllers job is to accept traffic approaching the cluster, and route it via services to pods that has requested ingress traffic.</p> <p>We use the widely used Ingress Nginx Ingress controller.</p>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#cert-manager","title":"Cert Manager","text":"<p>Cert Manager allows an administrator specify a request for a TLS certificate, eg. as a part of an Ingress, and have the request automatically fulfilled.</p> <p>The platform uses a cert-manager configured to handle certificate requests via Let's Encrypt.</p>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#prometheus-and-alertmanager","title":"Prometheus and Alertmanager","text":"<p>Prometheus is a time series database used by the platform to store and index runtime metrics from both the platform itself and the sites running on the platform.</p> <p>Prometheus is configured to scrape and ingest the following sources</p> <ul> <li>Node Exporter (Kubernetes   runtime metrics)</li> <li>Ingress Nginx</li> </ul> <p>Prometheus is installed via an Operator which amongst other things allows us to configure Prometheus and Alertmanager via  <code>ServiceMonitor</code> and <code>AlertmanagerConfig</code>.</p> <p>Alertmanager handles the delivery of alerts produced by Prometheus.</p>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#grafana","title":"Grafana","text":"<p>Grafana provides the graphical user-interface to Prometheus and Loki. It is configured with a number of data sources via its values-file, which connects it to Prometheus and Loki.</p>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#loki-and-promtail","title":"Loki and Promtail","text":"<p>Loki stores and indexes logs produced by the pods  running in AKS. Promtail streams the logs to Loki, and Loki in turn makes the logs available to the administrator via Grafana.</p>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#sites","title":"Sites","text":"<p>Each individual library has a Github repository that describes which sites should exist on the platform for the library. The creation of the repository and its contents is automated, and controlled by an entry in a <code>sites.yaml</code>- file shared by all sites on the platform.</p> <p>Consult the following runbooks to see the procedures for:</p> <ul> <li>Adding a site to the platform</li> <li>Removing a site</li> </ul>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#sitesyaml","title":"sites.yaml","text":"<p><code>sites.yaml</code> is found in <code>infrastructure/environments/&lt;environment&gt;/sites.yaml</code>. The file contains a single map, where the configuration of the individual sites are contained under the property <code>sites.&lt;unique site key&gt;</code>, eg.</p> <p><code>yaml sites:   # Site objects are indexed by a unique key that must be a valid lagoon, and   # github project name. That is, alphanumeric and dashes.   core-test1:     name: \"Core test 1\"     description: \"Core test site no. 1\"     # releaseImageRepository and releaseImageName describes where to pull the     # container image a release from.     releaseImageRepository: ghcr.io/danskernesdigitalebibliotek     releaseImageName: dpl-cms-source     # Sites can optionally specify primary and secondary domains.     primary-domain: core-test.example.com     # Fully configured sites will have a deployment key generated by Lagoon.     deploy_key: \"ssh-ed25519 &lt;key here&gt;\"   bib-ros:     name: \"Roskilde Bibliotek\"     description: \"Webmaster environment for Roskilde Bibliotek\"     primary-domain: \"www.roskildebib.dk\"     # The secondary domain will redirect to the primary.     secondary-domains: [\"roskildebib.dk\", \"www2.roskildebib.dk\"]     # A series of sites that shares the same image source may choose to reuse     # properties via anchors     &lt;&lt; : *default-release-image-source</code></p>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#environment-site-git-repositories","title":"Environment Site Git Repositories","text":"<p>Each platform-site is controlled via a GitHub repository. The repositories are provisioned via Terraform. The following depicts the authorization and control- flow in use: </p> <p>The configuration of each repository is reconciled each time a site is created,</p>"},{"location":"DPL-Platform/architecture/platform-environment-architecture/#deployment","title":"Deployment","text":"<p>Releases of DPL CMS are deployed to sites via the dpladm tool. It consults the <code>sites.yaml</code> file for the environment and performs any needed deployment.</p>"},{"location":"DPL-Platform/architecture/adr/","title":"Architecture Decision Records","text":"<p>We loosely follow the guidelines for ADRs described by Michael Nygard.</p> <p>A record should attempt to capture the situation that led to the need for a discrete choice to be made, and then proceed to describe the core of the decision, its status and the consequences of the decision.</p> <p>To summaries a ADR could contain the following sections (quoted from the above article):</p> <ul> <li> <p>Title: These documents have names that are short noun phrases. For example,  \"ADR 1: Deployment on Ruby on Rails 3.0.10\" or \"ADR 9: LDAP for Multitenant Integration\"</p> </li> <li> <p>Context: This section describes the forces at play, including technological   , political, social, and project local. These forces are probably in tension,   and should be called out as such. The language in this section is value-neutral.   It is simply describing facts.</p> </li> <li> <p>Decision: This section describes our response to these forces. It is stated   in full sentences, with active voice. \"We will \u2026\"</p> </li> <li> <p>Status: A decision may be \"proposed\" if the project stakeholders haven't   agreed with it yet, or \"accepted\" once it is agreed. If a later ADR changes   or reverses a decision, it may be marked as \"deprecated\" or \"superseded\" with   a reference to its replacement.</p> </li> <li> <p>Consequences: This section describes the resulting context, after applying  the decision. All consequences should be listed here, not just the \"positive\"  ones. A particular decision may have positive, negative, and neutral consequences,  but all of them affect the team and project in the future.</p> </li> </ul>"},{"location":"DPL-Platform/architecture/adr/adr-001-lagoon/","title":"Architecture Decision Record: Lagoon","text":""},{"location":"DPL-Platform/architecture/adr/adr-001-lagoon/#context","title":"Context","text":"<p>The Danish Libraries needed a platform for hosting a large number of Drupal installations. As it was unclear exactly how to build such a platform and how best to fulfill a number of requirements, a Proof Of Concept project was initiated to determine whether to use an existing solution or build a platform from scratch.</p> <p>After an evaluation, Lagoon was chosen.</p>"},{"location":"DPL-Platform/architecture/adr/adr-001-lagoon/#decision","title":"Decision","text":"<p>The main factors behind the decision to use Lagoon where:</p> <ul> <li>Much lower cost of maintenance than a self-built platform.</li> <li>The platform is continually updated, and the updates are available for free.</li> <li>A well-established platform with a lot of proven functionality right out of   the box.</li> <li>The option of professional support by Amazee</li> </ul> <p>When using and integrating with Lagoon we should strive to</p> <ul> <li>Make as little modifications to Lagoon as possible</li> <li>Whenever possible, use the defaults, recommendations and best practices   documented on eg. docs.lagoon.sh</li> </ul> <p>We do this to keep true to the initial thought behind choosing Lagoon as a platform that gives us a lot of functionality for a (comparatively) small investment.</p>"},{"location":"DPL-Platform/architecture/adr/adr-001-lagoon/#alternatives-considered","title":"Alternatives considered","text":"<p>The main alternative that was evaluated was to build a platform from scratch. While this may have lead to a more customized solution that more closely matched any requirements the libraries may have, it also required a very large investment would require a large ongoing investment to keep the platform maintained and updated.</p> <p>We could also choose to fork Lagoon, and start making heavy modifications to the platform to end up with a solution customized for our needs. The downsides of this approach has already been outlined.</p>"},{"location":"DPL-Platform/architecture/adr/adr-002-rightsizing/","title":"Architecture Decision Record: Rightsizing","text":""},{"location":"DPL-Platform/architecture/adr/adr-002-rightsizing/#context","title":"Context","text":""},{"location":"DPL-Platform/architecture/adr/adr-002-rightsizing/#expected-traffic","title":"Expected traffic","text":"<p> The platform is required to be able to handle an estimated 275.000 page-views per day spread out over 100 websites. A visit to a website that causes the browser to request a single html-document followed by a number of assets is only counted as a single page-view.</p> <p>On a given day about half of the page-views to be made by an authenticated user. We further more expect the busiest site receive about 12% of the traffic.</p> <p>Given these numbers, we can make some estimates of the expected average load. To stay fairly conservative we will still assume that about 50% of the traffic is anonymous and can thus be cached by Varnish, but we will assume that all all sites gets traffic as if they where the most busy site on the platform (12%).</p> <p>12% of 275.000 requests gives us a an average of 33.000 requests. To keep to the conservative side, we concentrate the load to a period of 8 hours. We then end up with roughly 1 page-view pr. second.</p>"},{"location":"DPL-Platform/architecture/adr/adr-002-rightsizing/#expected-workload-characteristics","title":"Expected workload characteristics","text":"<p>The platform is hosted on a Kubernetes cluster on which Lagoon is installed. As of late 2021, Lagoons approach to handling rightsizing of PHP- and in particular Drupal-applications is based on a number factors:</p> <ol> <li>Web workloads are extremely spiky. While a site looks to have to have a    sustained load of 5 rps when looking from afar, it will in fact have anything    from (eg) 0 to 20 simultaneous users on a given second.</li> <li>Resource-requirements are every ephemeral. Even though a request as a peak    memory-usage of 128MB, it only requires that amount of memory for a very    short period of time.</li> <li>Kubernetes nodes has a limit of how many pods will fit on a given node.    This will constraint the scheduler from scheduling too many pods to a node    even if the workloads has declared a very low resource request.</li> <li>With metrics-server enabled, Kubernetes will keep track of the actual    available resources on a given node. So, a node with eg. 4GB ram, hosting    workloads with a requested resource allocation of 1GB, but actually taking    up 3.8GB of ram, will not get scheduled another pod as long as there are    other nodes in the cluster that has more resources available.</li> </ol> <p>The consequence of the above is that we can pack a lot more workload onto a single node than what would be expected if you only look at the theoretical maximum resource requirements.</p>"},{"location":"DPL-Platform/architecture/adr/adr-002-rightsizing/#lagoon-resource-request-defaults","title":"Lagoon resource request defaults","text":"<p>Lagoon sets its resource-requests based on a helm values default in the <code>kubectl-build-deploy-dind</code> image. The default is typically 10Mi pr. container which can be seen in the nginx-php chart which runs a php and nginx container. Lagoon configures php-fpm to allow up to 50 children and allows php to use up to 400Mi memory.</p> <p>Combining these numbers we can see that a site that is scheduled as if it only uses 20 Megabytes of memory, can in fact take up to 20 Gigabytes. The main thing that keeps this from happening in practice is a a combination of the above assumptions. No node will have more than a limited number of pods, and on a given second, no site will have nearly as many inbound requests as it could have.</p>"},{"location":"DPL-Platform/architecture/adr/adr-002-rightsizing/#decision","title":"Decision","text":"<p>Lagoon is a very large and complex solution. Any modification to Lagoon will need to be well tested, and maintained going forward. With this in mind, we should always strive to use Lagoon as it is, unless the alternative is too costly or problematic.</p> <p>Based on real-live operations feedback from Amazee (creators of Lagoon) and the context outline above we will</p> <ul> <li>Leave the Lagoon defaults as they are, meaning most pods will request 10Mi of   memory.</li> <li>Let the scheduler be informed by runtime metrics instead of up front pod   resource requests.</li> <li>Rely on the node maximum pods to provide some horizontal spread of pods.</li> </ul>"},{"location":"DPL-Platform/architecture/adr/adr-002-rightsizing/#alternatives-considered","title":"Alternatives considered","text":"<p>As Lagoon does not give us any manual control over rightsizing out of the box, all alternatives involves modifying Lagoon.</p>"},{"location":"DPL-Platform/architecture/adr/adr-002-rightsizing/#altering-lagoon-defaults","title":"Altering Lagoon Defaults","text":"<p>We've inspected the process Lagoon uses to deploy workloads, and determined that it would be possible to alter the defaults used without too many modifications.</p> <p>The <code>build-deploy-docker-compose.sh</code> script that renders the manifests that describes a sites workloads via Helm includes a service-specific values-file. This file can be used to modify the defaults for the Helm chart. By creating custom container-image for the build-process based on the upstream Lagoon build image, we can deliver our own version of this image.</p> <p>As an example, the following Dockerfile will add a custom values file for the redis service.</p> <pre><code>FROM docker.io/uselagoon/kubectl-build-deploy-dind:latest\nCOPY redis-values.yaml /kubectl-build-deploy/\n</code></pre> <p>Given the following <code>redis-values.yaml</code></p> <pre><code>resources:\n  requests:\n    cpu: 10m\n    memory: 100Mi\n</code></pre> <p>The Redis deployment would request 100Mi instead of the previous default of 10Mi.</p>"},{"location":"DPL-Platform/architecture/adr/adr-002-rightsizing/#introduce-t-shirt-sizes","title":"Introduce \"t-shirt\" sizes","text":"<p>Building upon the modification described in the previous chapter, we could go even further and modify the build-script itself. By inspecting project variables we could have the build-script pass in eg. a configurable value for replicaCount for a pod. This would allow us to introduce a small/medium/large concept for sites. This could be taken even further to eg. introduce whole new services into Lagoon.</p>"},{"location":"DPL-Platform/architecture/adr/adr-002-rightsizing/#consequences","title":"Consequences","text":"<p>This could lead to problems for sites that requires a lot of resources, but given the expected average load, we do not expect this to be a problem even if a site receives an order of magnitude more traffic than the average.</p> <p>The approach to rightsizing may also be a bad fit if we see a high concentration of \"non-spiky\" workloads. We know for instance that Redis and in particular Varnish is likely to use a close to constant amount of memory. Should a lot of Redis and Varnish pods end up on the same node, evictions are very likely to occur.</p> <p>The best way to handle these potential situations is to be knowledgeable about how to operate Kubernetes and Lagoon, and to monitor the workloads as they are in use.</p>"},{"location":"DPL-Platform/architecture/adr/adr-003-system-alerts/","title":"ADR-003 System alerts","text":""},{"location":"DPL-Platform/architecture/adr/adr-003-system-alerts/#context","title":"Context","text":"<p>There has been a wish for a functionality that alerts administrators if certain system values have gone beyond defined thresholds rules.</p>"},{"location":"DPL-Platform/architecture/adr/adr-003-system-alerts/#decision","title":"Decision","text":"<p>We have decided to use alertmanager that is a part of the Prometheus package that is already used for monitoring the cluster.</p>"},{"location":"DPL-Platform/architecture/adr/adr-003-system-alerts/#consequences","title":"Consequences","text":"<ul> <li>We have tried to install alertmanager and testing it.   It works and given the various possibilities of defining   alert rules we consider   the demands to be fulfilled.</li> <li>We will be able to get alerts regarding thresholds on both container and   cluster level which is what we need.</li> <li>Alertmanager fits in the general focus of being cloud agnostic. It is   CNCF approved   and does not have any external infrastructure dependencies.</li> </ul>"},{"location":"DPL-Platform/architecture/adr/adr-004-declarative-site-management/","title":"ADR 004: Declarative Site management","text":""},{"location":"DPL-Platform/architecture/adr/adr-004-declarative-site-management/#context","title":"Context","text":"<p>Lagoon requires a site to be deployed from at Git repository containing a .lagoon.yml and docker-compose.yml A potential logical consequence of this is that we require a Git repository pr site we want to deploy, and that we require that repository to maintain those two files.</p> <p>Administering the creation and maintenance of 100+ Git repositories can not be done manually with risk of inconsistency and errors. The industry best practice for administering large-scale infrastructure is to follow a declarative Infrastructure As Code(IoC) pattern. By keeping the approach declarative it is much easier for automation to reason about the intended state of the system.</p> <p>Further more, in a standard Lagoon setup, Lagoon is connected to the \"live\" application repository that contains the source-code you wish to deploy. In this approach Lagoon will just deploy whatever the HEAD of a given branch points. In our case, we perform the build of a sites source release separate from deploying which means the sites repository needs to be updated with a release-version whenever we wish it to be updated. This is not a problem for a small number of sites - you can just update the repository directly - but for a large set of sites that you may wish to administer in bulk - keeping track of which version is used where becomes a challenge. This is yet another good case for declarative configuration: Instead of modifying individual repositories by hand to deploy, we would much rather just declare that a set of sites should be on a specific release and then let automation take over.</p> <p>While there are no authoritative discussion of imperative vs declarative IoC, the following quote from an OVH Tech Blog summarizes the current consensus in the industry pretty well:</p> <p>In summary declarative infrastructure tools like Terraform and CloudFormation offer a much lower overhead to create powerful infrastructure definitions that can grow to a massive scale with minimal overheads. The complexities of hierarchy, timing, and resource updates are handled by the underlying implementation so you can focus on defining what you want rather than how to do it.</p> <p>The additional power and control offered by imperative style languages can be a big draw but they also move a lot of the responsibility and effort onto the developer, be careful when choosing to take this approach.</p>"},{"location":"DPL-Platform/architecture/adr/adr-004-declarative-site-management/#decision","title":"Decision","text":"<p>We administer the deployment to and the Lagoon configuration of a library site in a repository pr. library. The repositories are provisioned via Terraform that reads in a central <code>sites.yaml</code> file. The same file is used as input for the automated deployment process which renderers the various files contained in the repository including a reference to which release of DPL-CMS Lagoon should use.</p> <p>It is still possible to create and maintain sites on Lagoon independent of this approach. We can for instance create a separate project for the dpl-cms repository to support core development.</p>"},{"location":"DPL-Platform/architecture/adr/adr-004-declarative-site-management/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"DPL-Platform/architecture/adr/adr-004-declarative-site-management/#alternatives-considered","title":"Alternatives considered","text":"<p>We could have run each site as a branch of off a single large repository. This was rejected as a possibility as it would have made the administration of access to a given libraries deployed revision hard to control. By using individual repositories we have the option of grating an outside developer access to a full repository without affecting any other.</p>"},{"location":"DPL-Platform/architecture/adr/adr-005-declarative-site-management-2/","title":"ADR 005: Declarative Site management (2)","text":""},{"location":"DPL-Platform/architecture/adr/adr-005-declarative-site-management-2/#context","title":"Context","text":"<p>We have previously established (in ADR 004) that we want to take a declarative approach to site management. The previous ADR focuses on using a single declarative file, <code>sites.yaml</code>, for driving Github repositories that can be used by Lagoon to run sites.</p> <p>The considerations from that ADR apply equally here. We have identified more opportunities to use a declarative approach, which will simultaneously significantly simplify the work required for platform maintainers when managing sites.</p> <p>Specifically, runbooks with several steps to effectuate a new deployment are a likely source of errors. The same can be said of having to manually run commands to make changes in the platform.</p> <p>Every time we run a command that is not documented in the source code in the platform <code>main</code> branch, it becomes less clear what the state of the platform is.</p> <p>Conversely, every time a change is made in the <code>main</code> branch that has not yet been executed, it becomes less clear what the state of the platform is.</p>"},{"location":"DPL-Platform/architecture/adr/adr-005-declarative-site-management-2/#decision","title":"Decision","text":"<p>We continuously strive towards making the <code>main</code> branch in the dpl-platform repo the single source of truth for what the state of the platform should be. The repository becomes the declaration of the entire state.</p> <p>This leads to at least two concrete steps:</p> <ul> <li> <p>We will automate synchronizing state in the platform-repo with the actual   platform state. This means using <code>sites.yaml</code> to declare the expected state   in Lagoon (e.g. which projects and environments are created), not just the   Github repos. This leaves the deployment process less error prone.</p> </li> <li> <p>We will automate running the synchronization step every time code is checked   into the <code>main</code> branch. This means state divergence between the platform repo   declaration and reality is minimized.</p> </li> </ul> <p>It will still be possible for <code>dpl-cms</code> to maintain its own area of state in the platform, but anything declared in <code>dpl-platform</code> will be much more likely to be the actual state in the platform.</p>"},{"location":"DPL-Platform/architecture/adr/adr-005-declarative-site-management-2/#status","title":"Status","text":"<p>Proposed</p>"},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/","title":"ADR 006: In-cluster Database","text":""},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/#context","title":"Context","text":"<p>Microsoft is sunsetting Azure Database for MariaDB by September 2025. They recommend moving to Azure Database for MySql instead.</p> <p>The project has experienced project wide down time due to database crashes on multiple occasions:</p> <ul> <li>17.01.2025: Database went down due to a restart initiated by the Azure Product Group team taking unsanctioned action</li> <li>15.01.2025: Database went down due to a restart initiated by the Azure Product Group team taking unsanctioned action</li> <li>9.01.2025: Database went down as a result of to many login requests</li> <li>28.11.2024: Database went down due to too many active connection attempts</li> <li>7.11.2024: Database went down due to too many connections</li> <li>Summer 2024: Database was restarted by Azure for unscheduled and unnotified maintenance</li> </ul> <p>Microsoft's database server logs are prohibitively expensive. Microsoft misconfigured the database provision-time. The current Microsoft support tier is useless and useful support is prohibitively expensive. Microsoft support has made unsanctioned changes on the databse, such as maintenance work during business hours and restarts in attempts to fix support tickets.</p> <p>In light of this, we have to consider options.</p>"},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/#considered-options","title":"Considered options","text":"<ul> <li>In-cluster MariaDB</li> <li>Azure Database for MySql</li> </ul>"},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/#pros-cons","title":"Pros &amp; Cons","text":""},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/#azure-database-for-mysql","title":"Azure Database for MySQL","text":""},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/#cons","title":"Cons","text":"<ul> <li>Negative experiences from management of Azure Database for MariaDB</li> <li>The project has experienced project wide down time due to database crashes on multiple occasions:</li> <li>Server logs are disproportionately priced. They are too expensive for this   project. This prevents us from debugging many problems ourselves.</li> <li>The wait time for support can be long and incorrect.</li> <li>Server might be (was in our case) misconfigured</li> <li>Potential data-mismatch, that must be handled migration time</li> </ul>"},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/#pros","title":"Pros","text":"<ul> <li>One-click setup</li> <li>Minimal configuration needed from us</li> <li>Azure support</li> <li>Azure can be blamed for downtime, they are directly or indirectly responsible for.</li> <li>MySql, which is not set to be sunset, might have a noticeably better service as that is their chosen flavor to keep offering.</li> <li>Support for hight-availability with automatic failover to a replica</li> </ul>"},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/#in-cluster-db","title":"In-Cluster DB","text":""},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/#cons_1","title":"Cons","text":"<ul> <li>We have no one but ourselves to blame if something goes awry</li> <li>Logging and monitoring has to be set up manually</li> <li>Point-in-time-recovery is not supported at the moment</li> <li>All configurations and updates have to be done by us</li> </ul>"},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/#pros_1","title":"Pros","text":"<ul> <li>Ability to investigate logs</li> <li>Troubleshooting can be done immeadiately</li> <li>Logging and monitoring can be set up to alert us to noteworthy changes.</li> <li>The server resources are cheaper and will allow us to have more powerful   databases at a lower cost.</li> <li>Ability to split out databases such that one database having issues doesn't   cause all sites to crash.</li> <li>Support for high-availability   if we decide to implement this.</li> <li>Performance can be optimized and finetuned to our usecase</li> <li>Databases will be located closer, logically as well as physically, to workloads relying on them = faster response time, which should be noticable for end-users.</li> <li>We can right-size the database, thereby getting the maximum performance for the buck.</li> <li>Direct access to the server itself.</li> </ul>"},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/#decision","title":"Decision","text":"<p>We have made the decision to implement the In-cluster database due the aforemention pros and cons lists. Negative experiences with stability and management for Azure Database for MariaDB have reduced our trust in continual use of managed databases by Azure. By moving to an in-cluster database get control over a critical part of our infrastructure</p> <p>We will start by implementing a PoC, where we can test for a good setup of the in-cluster database before migrating all the databases to use the in-cluster databases. The PoC will also be used for testing migration against.</p> <p>Every step taken towards moving into in-cluster database shall done transparantly.</p> <p>The In-cluster database must:</p> <ul> <li>Have resource monitoring setup</li> <li>Be able to have backups taken</li> <li>Be able to be restored</li> <li>Have log monitoring setup</li> </ul>"},{"location":"DPL-Platform/architecture/adr/adr-006-in-cluster-db/#status","title":"Status","text":"<p>Approved</p>"},{"location":"DPL-Platform/infrastructure/","title":"DPL Platform Infrastructure","text":"<p>This directory contains the Infrastructure as Code and scripts that are used for maintaining the infrastructure-component that each platform environment consists of. A \"platform environment\" is an umbrella term for the Azure infrastructure, the Kubernetes cluster, the Lagoon installation and the set of GitHub environments that makes up a single DPL Platform installation.</p>"},{"location":"DPL-Platform/infrastructure/#directory-layout","title":"Directory layout","text":"<ul> <li>dpladm/: a tool used for deploying individual sites. The tools can   be run manually, but the recommended way is via the common infrastructure Taskfile.</li> <li>environments/: contains a directory for each platform environment.</li> <li>terraform: terraform setup and tooling that is shared between   environments.</li> <li>task/: Configuration and scripts used by our Taskfile-based automation   The scripts included in this directory can be run by hand in an emergency   but te recommended way to invoke these via task.</li> <li>Taskfile.yml: the common infrastructure task   configuration. Invoke <code>task</code> to get a list of targets. Must be run from within   an instance of DPL shell unless otherwise   noted.</li> </ul>"},{"location":"DPL-Platform/infrastructure/#platform-environment-configurations","title":"Platform Environment configurations","text":"<p>The <code>environments</code> directory contains a subdirectory for each platform environment. You generally interact with the files and directories within the directory to configure the environment. When a modification has been made, it is put in to effect by running the appropiate <code>task</code> :</p> <ul> <li><code>configuration</code>: contains the various configurations the   applications that are installed on top of the infrastructure requires. These   are used by the <code>support:provision:*</code> tasks.</li> <li><code>env_repos</code> contains the Terraform root-module for provisioning GitHub site-   environment repositories. The module is run via the <code>env_repos:provision</code> task.</li> <li><code>infrastructure</code>: contains the Terraform root-module used to provision the basic   Azure infrastructure components that the platform requires.The module is run   via the <code>infra:provision</code> task.</li> <li><code>lagoon</code>: contains Kubernetes manifests and Helm values-files used for installing   the Lagoon Core and Remote that is at the heart of a DPL Platform installation.   THe module is run via the <code>lagoon:provision:*</code> tasks.</li> </ul>"},{"location":"DPL-Platform/infrastructure/#basic-usage-of-dplsh-and-an-environment-configuration","title":"Basic usage of dplsh and an environment configuration","text":"<p>The remaining guides in this document assumes that you work from an instance of the DPL shell. See the DPLSH Runbook for a basic introduction to how to use dplsh.</p>"},{"location":"DPL-Platform/infrastructure/#installing-a-platform-environment-from-scratch","title":"Installing a platform environment from scratch","text":"<p>The following describes how to set up a whole new platform environment to host  platform sites.</p> <p>The easiest way to set up a new environment is to create a new <code>environments/&lt;name&gt;</code> directory and copy the contents of an existing environment replacing any references to the previous environment with a new value corresponding to the new environment. Take note of the various URLs, and make sure to update the Current Platform environments documentation.</p> <p>If this is the very first environment, remember to first initialize the Terraform- setup, see the terraform README.md.</p>"},{"location":"DPL-Platform/infrastructure/#provisioning-infrastructure","title":"Provisioning infrastructure","text":"<p>When you have prepared the environment directory, launch <code>dplsh</code> and go through the following steps to provision the infrastructure:</p> <pre><code># We export the variable to simplify the example, you can also specify it inline.\nexport DPLPLAT_ENV=dplplat01\n\n# Provision the Azure resources\ntask infra:provision\n\n# Create DNS record\nCreate an A record in the administration area of your DNS provider.\nTake the terraform output: \"ingress_ip\" of the former command and create an entry\nlike: \"*.[DOMAN_NAME].[TLD]\": \"[ingress_ip]\"\n\n# Provision the support software that the Platform relies on\ntask support:provision\n</code></pre>"},{"location":"DPL-Platform/infrastructure/#installing-and-configuring-lagoon","title":"Installing and configuring Lagoon","text":"<p>The previous step has established the raw infrastructure and the Kubernetes support projects that Lagoon needs to function. You can proceed to follow the official Lagoon installation procedure.</p> <p>The execution of the individual steps of the guide has been somewhat automated, the following describes how to use the automation, make sure to follow along in the official documentation to understand the steps and some of the additional actions you have to take.</p> <pre><code># The following must be carried out from within dplsh, launched as described\n# in the previous step including the definition of DPLPLAT_ENV.\n\n# 1. Provision a lagoon core into the cluster.\ntask lagoon:provision:core\n\n# 2. Skip the steps in the documentation that speaks about setting up email, as\n# we currently do not support sending emails.\n\n# 3. Setup ssh-keys for the lagoonadmin user\n# Access the Lagoon UI (consult the platform-environments.md for the url) and\n# log in with lagoonadmin + the admin password that can be extracted from a\n# Kubernetes secret:\nkubectl \\\n  -o jsonpath=\"{.data.KEYCLOAK_LAGOON_ADMIN_PASSWORD}\" \\\n  -n lagoon-core \\\n  get secret lagoon-core-keycloak \\\n| base64 --decode\n\n# Then go to settings and add the ssh-keys that should be able to access the\n# lagoon admin user. Consider keeping this list short, and instead add\n# additional users with fewer privileges laster.\n\n# 4. If your ssh-key is passphrase-projected we'll need to setup an ssh-agent\n# instance:\n$ eval $(ssh-agent); ssh-add\n\n# 5. Configure the CLI to verify that access (the cli itself has already been\n#    installed in dplsh)\ntask lagoon:cli:config\n\n# You can now add additional users, this step is currently skipped.\n\n# (6. Install Harbor.)\n# This step has already been performed as a part of the installation of\n# support software.\n\n# 7. Install a Lagoon Remote into the cluster\ntask lagoon:provision:remote\n\n# 8. Register the cluster administered by the Remote with Lagoon Core\n# Notice that you must provide a bearer token via the USER_TOKEN environment-\n# variable. The token can be found in $HOME/.lagoon.yml after a successful\n# \"lagoon login\"\nUSER_TOKEN=&lt;token&gt; task lagoon:add:cluster:\n</code></pre> <p>The Lagoon core has now been installed, and the remote registered with it.</p>"},{"location":"DPL-Platform/infrastructure/#setting-up-a-github-organization-and-repositories-for-a-new-platform-environment","title":"Setting up a GitHub organization and repositories for a new platform environment","text":"<p>Prerequisites:</p> <ul> <li>An properly authenticated azure CLI (<code>az</code>). See the section on initial   Terraform setup for more details on the requirements</li> </ul> <p>First create a new administrative github user and create a new organization with the user. The administrative user should only be used for administering the organization via terraform and its credentials kept as safe as possible! The accounts password can be used as a last resort for gaining access to the account and will not be stored in Key Vault. Thus, make sure to store the password somewhere safe, eg. in a password-manager or as a physical printout.</p> <p>This requires the infrastructure to have been created as we're going to store credentials into the azure Key Vault.</p> <pre><code># cd into the infrastructure folder and launch a shell\n(host)$ cd infrastructure\n(host)$ dplsh\n\n# Remaining commands are run from within dplsh\n\n# export the platform environment name.\n# export DPLPLAT_ENV=&lt;name&gt;, eg\n$ export DPLPLAT_ENV=dplplat01\n\n# 1. Create a ssh keypair for the user, eg by running\n# ssh-keygen -t ed25519 -C \"&lt;comment&gt;\" -f dplplatinfra01_id_ed25519\n# eg.\n$ ssh-keygen -t ed25519 -C \"dplplatinfra@0120211014073225\" -f dplplatinfra01_id_ed25519\n\n# 2. Then access github and add the public-part of the key to the account\n# 3. Add the key to keyvault under the key name \"github-infra-admin-ssh-key\"\n# eg.\n$ SECRET_KEY=github-infra-admin-ssh-key SECRET_VALUE=$(cat dplplatinfra01_id_ed25519)\\\n  task infra:keyvault:secret:set\n\n# 4. Access GitHub again, and generate a Personal Access Token for the account.\n#    The token should\n#     - be named after the platform environment (eg. dplplat01-terraform-timestamp)\n#     - Have a fairly long expiration - do remember to renew it\n#     - Have the following permissions: admin:org, delete_repo, read:packages, repo\n# 5. Add the access token to Key Vault under the name \"github-infra-admin-pat\"\n# eg.\n$ SECRET_KEY=github-infra-admin-pat SECRET_VALUE=githubtokengoeshere task infra:keyvault:secret:set\n\n# Our tooling can now administer the GitHub organization\n</code></pre>"},{"location":"DPL-Platform/infrastructure/#renewing-the-administrative-github-personal-access-token","title":"Renewing the administrative GitHub Personal Access Token","text":"<p>The Personal Access Token we use for impersonating the administrative GitHub user needs to be recreated periodically:</p> <pre><code># cd into the infrastructure folder and launch a shell\n(host)$ cd infrastructure\n(host)$ dplsh\n\n# Remaining commands are run from within dplsh\n\n# export the platform environment name.\n# export DPLPLAT_ENV=&lt;name&gt;, eg\n$ export DPLPLAT_ENV=dplplat01\n\n# 1. Access GitHub, and generate a Personal Access Token for the account.\n#    The token should\n#     - be named after the platform environment (eg. dplplat01-terraform)\n#     - Have a fairly long expiration - do remember to renew it\n#     - Have the following permissions: admin:org, delete_repo, read:packages, repo\n# 2. Add the access token to Key Vault under the name \"github-infra-admin-pat\"\n# eg.\n$ SECRET_KEY=github-infra-admin-pat SECRET_VALUE=githubtokengoeshere \\\n  task infra:keyvault:secret:set\n\n# 3. Delete the previous token\n</code></pre>"},{"location":"DPL-Platform/infrastructure/terraform/","title":"Terraform","text":"<p>This directory contains the configuration and tooling we use to support our use of terraform.</p>"},{"location":"DPL-Platform/infrastructure/terraform/#the-terraform-setup","title":"The Terraform setup","text":"<p>The setup keeps a single terraform-state pr. environment. Each state is kept as separate blobs in a Azure Storage Account.</p> <p></p> <p>Access to the storage account is granted via a Storage Account Key which is kept in a Azure Key Vault in the same resource-group. The key vault, storage account and the resource-group that contains these resources are the only resources that are not provisioned via Terraform.</p>"},{"location":"DPL-Platform/infrastructure/terraform/#initial-setup-of-terraform","title":"Initial setup of Terraform","text":"<p>The following procedure must be carried out before the first environment can be created.</p> <p>Prerequisites:</p> <ul> <li>A Azure subscription</li> <li>An authenticated azure CLI that is allowed to use create resources and grant   access to these resources under the subscription including Key Vaults.   The easiest way to achieve this is to grant the user the <code>Owner</code> and   <code>Key Vault Administrator</code> roles to on subscription.</li> </ul> <p>Use the <code>scripts/bootstrap-tf.sh</code> for bootstrapping. After the script has been run successfully it outputs instructions for how to set up a terraform module that uses the newly created storage-account for state-tracking.</p> <p>As a final step you must grant any administrative users that are to use the setup permission to read from the created key vault.</p>"},{"location":"DPL-Platform/infrastructure/terraform/#dnsimple","title":"Dnsimple","text":"<p>The setup uses an integration with DNSimple to set a domain name when the environments ingress ip has been provisioned. To use this integration first obtain a api-key for the DNSimple account. Then use <code>scripts/add-dnsimple-apikey.sh</code> to write it to the setups Key Vault and finally add the following section to <code>.dplsh.profile</code> ( get the subscription id and key vault name from existing export for <code>ARM_ACCESS_KEY</code>).</p> <pre><code>export DNSIMPLE_TOKEN=$(az keyvault secret show --subscription \"&lt;subscriptionid&gt;\"\\\n --name dnsimple-api-key --vault-name &lt;key vault-name&gt; --query value -o tsv)\nexport DNSIMPLE_ACCOUNT=\"&lt;dnsimple-account-id&gt;\"\n</code></pre>"},{"location":"DPL-Platform/infrastructure/terraform/#terraform-setups","title":"Terraform Setups","text":"<p>A setup is used to manage a set of environments. We currently have a single that manages all environments.</p>"},{"location":"DPL-Platform/infrastructure/terraform/#alpha","title":"Alpha","text":"<ul> <li>Name: alpha</li> <li>Resource-group: rg-tfstate-alpha</li> <li>Key Vault name: kv-dpltfstatealpha001</li> <li>Storage account: stdpltfstatealpha001</li> </ul>"},{"location":"DPL-Platform/infrastructure/terraform/#terraform-modules","title":"Terraform Modules","text":""},{"location":"DPL-Platform/infrastructure/terraform/#root-module","title":"Root module","text":"<p>The platform environments share a number of general modules, which are then used via a number of root-modules set up for each environment.</p> <p>Consult the general environment documentation for descriptions on which resources you can expect to find in an environment and how they are used.</p> <p>Consult the environment overview for an overview of environments.</p>"},{"location":"DPL-Platform/infrastructure/terraform/#dpl-platform-infrastructure-module","title":"DPL Platform Infrastructure Module","text":"<p>The dpl-platform-environment Terraform module provisions all resources that are required for a single DPL Platform Environment.</p> <p>Inspect variables.tf for a description of the required module-variables.</p> <p>Inspect outputs.tf for a list of outputs.</p> <p>Inspect the individual module files for documentation of the resources.</p> <p>The following diagram depicts (amongst other things) the provisioned resources. Consult the platform environment documentation for more details on the role the various resources plays. </p>"},{"location":"DPL-Platform/infrastructure/terraform/#dpl-platform-site-environment-module","title":"DPL Platform Site Environment Module","text":"<p>The dpl-platform-env-repos Terraform module provisions the GitHub Git repositories that the platform uses to integrate with Lagoon. Each site hosted on the platform has a registry.</p> <p>Inspect variables.tf for a description of the required module-variables.</p> <p>Inspect outputs.tf for a list of outputs.</p> <p>Inspect the individual module files for documentation of the resources.</p> <p>The following diagram depicts how the module gets its credentials for accessing GitHub and what it provisions. </p>"},{"location":"DPL-Platform/runbooks/","title":"DPL Platform Runbooks","text":"<p>This directory contains our operational runbooks for standard procedures you may need to carry out while maintaining and operating a DPL Platform environment.</p> <p>Most runbooks has the following layout.</p> <ul> <li>Title - Short title that follows the name of the markdown file for quick   lookup.</li> <li>When to use - Outlines when this runbook should be used.</li> <li>Prerequisites - Any requirements that should be met before the procedure is   followed.</li> <li>Procedure - Stepwise description of the procedure, sometimes these will   be whole subheadings, sometimes just a single section with lists.</li> </ul> <p>The runbooks should focus on the \"How\", and avoid explaining any unnecessary details.</p>"},{"location":"DPL-Platform/runbooks/access-kubernetes/","title":"Access Kubernetes","text":""},{"location":"DPL-Platform/runbooks/access-kubernetes/#when-to-use","title":"When to use","text":"<p>When you need to gain <code>kubectl</code> access to a platform-environments Kubernetes cluster.</p>"},{"location":"DPL-Platform/runbooks/access-kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li>An authenticated <code>az</code> cli (from the host). This likely means <code>az login   --tenant TENANT_ID</code>, where the tenant id is that of  \"DPL Platform\". See Azure   Portal &gt; Tenant Properties. The logged in user must have permissions to list   cluster credentials.</li> <li><code>docker</code> cli which is authenticated against the GitHub Container Registry.   The access token used must have the <code>read:packages</code> scope.</li> </ul>"},{"location":"DPL-Platform/runbooks/access-kubernetes/#procedure","title":"Procedure","text":"<ol> <li>cd to <code>dpl-platform/infrastructure</code></li> <li>Launch the dpl shell: <code>dplsh</code></li> <li>Set the platform envionment, eg. for \"dplplat01\": <code>export DPLPLAT_ENV=dplplat01</code></li> <li>Authenticate: <code>task cluster:auth</code></li> </ol> <p>Your dplsh session should now be authenticated against the cluster.</p>"},{"location":"DPL-Platform/runbooks/add-generic-site-to-platform/","title":"Add a generic site to the platform","text":""},{"location":"DPL-Platform/runbooks/add-generic-site-to-platform/#when-to-use","title":"When to use","text":"<p>When you want to add a \"generic\" site to the platform. By Generic we mean a site stored in a repository that that is Prepared for Lagoon and contains a <code>.lagoon.yml</code> at its root.</p> <p>The current main example of such as site is dpl-cms which is used to develop the shared DPL install profile.</p>"},{"location":"DPL-Platform/runbooks/add-generic-site-to-platform/#prerequisites","title":"Prerequisites","text":"<ul> <li>An authenticated <code>az</code> cli. The logged in user must have full administrative   permissions to the platforms azure infrastructure.</li> <li>A running dplsh with <code>DPLPLAT_ENV</code> set to the platform   environment name.</li> <li>A Lagoon account on the Lagoon core with your ssh-key associated (created through   the Lagoon UI, on the Settings page)</li> <li>The git-url for the sites environment repository (you don't need to create this   repository - it will be created by the task below - but the URL must match the   Github repository that will be created)</li> <li>A personal access-token that is allowed to pull images from the image-registry   that hosts our images.</li> <li>The platform environment name (Consult the platform environment documentation)</li> </ul>"},{"location":"DPL-Platform/runbooks/add-generic-site-to-platform/#procedure","title":"Procedure","text":"<p>The following describes a semi-automated version of \"Add a Project\" in the official documentation.</p> <pre><code># From within dplsh:\n\n# Set an environment,\n# export DPLPLAT_ENV=&lt;platform environment name&gt;\n# eg.\n$ export DPLPLAT_ENV=dplplat01\n\n# If your ssh-key is passphrase-projected we'll need to setup an ssh-agent\n# instance:\n$ eval $(ssh-agent); ssh-add\n\n# 1. Add a project\n# PROJECT_NAME=&lt;project name&gt;  GIT_URL=&lt;url&gt; task lagoon:project:add\n$ PROJECT_NAME=dpl-cms GIT_URL=git@github.com:danskernesdigitalebibliotek/dpl-cms.git\\\n  task lagoon:project:add\n\n# 1.b You can also run lagoon add project manually, consult the documentation linked\n#     in the beginning of this section for details.\n\n# 2. Deployment key\n# The project is added, and a deployment key is printed. Copy it and configure\n# the GitHub repository. See the official documentation for examples.\n$ PROJECT_NAME=dpl-cms task lagoon:project:deploykey\n\n# 3. Webhook\n# Configure Github to post events to Lagoons webhook url.\n# The webhook url for the environment will be\n#  https://webhookhandler.lagoon.&lt;environment&gt;.dpl.reload.dk\n# eg for the environment dplplat01\n#  https://webhookhandler.lagoon.dplplat01.dpl.reload.dk\n#\n# Referer to the official documentation linked above for an example on how to\n# set up webhooks in github.\n\n# 4. Trigger a deployment manually, this will fail as the repository is empty\n#    but will serve to prepare Lagoon for future deployments.\n# lagoon deploy branch -p &lt;project-name&gt; -b &lt;branch&gt;\n$ lagoon deploy branch -p dpl-cms -b main\n</code></pre>"},{"location":"DPL-Platform/runbooks/add-library-site-to-platform/","title":"Add a new library site to the platform","text":""},{"location":"DPL-Platform/runbooks/add-library-site-to-platform/#when-to-use","title":"When to use","text":"<p>When you want to add a new core-test, editor, webmaster or programmer dpl-cms site to the platform.</p>"},{"location":"DPL-Platform/runbooks/add-library-site-to-platform/#prerequisites","title":"Prerequisites","text":"<ul> <li>An authenticated <code>az</code> cli. The logged in user must have full administrative   permissions to the platforms azure infrastructure.</li> <li>A running dplsh with <code>DPLPLAT_ENV</code> set to the platform   environment name.</li> </ul>"},{"location":"DPL-Platform/runbooks/add-library-site-to-platform/#procedure","title":"Procedure","text":"<p>The following sections describes how to</p> <ul> <li>Add the site to <code>sites.yaml</code></li> <li>Using the <code>sites.yaml</code> specification to provision Github repositories,   create Lagoon projects, tie the two together, and deploy all the   relevant environments.</li> </ul>"},{"location":"DPL-Platform/runbooks/add-library-site-to-platform/#step-1-update-sitesyaml","title":"Step 1, update sites.yaml","text":"<p>Create an entry for the site in <code>sites.yaml</code>.</p> <p>Specify a unique site key (its key in the map of sites), name and description. Leave out the deployment-key, it will be added automatically by the synchronization script.</p> <p>Sample entry (beware that this example be out of sync with the environment you are operating, so make sure to compare it with existing entries from the environment)</p> <pre><code>sites:\n  bib-rb:\n    name: \"Roskilde Bibliotek\"\n    description: \"Roskilde Bibliotek\"\n    primary-domain: \"www.roskildebib.dk\"\n    secondary-domains: [\"roskildebib.dk\"]\n    dpl-cms-release: \"1.2.3\"\n    go-release: \"1.2.3\"\n    &lt;&lt; : *default-release-image-source\n</code></pre> <p>The last entry merges in a default set of properties for the source of release- images. If the site is on the \"programmer\" plan, specify a custom set of properties like so:</p> <pre><code>sites:\n  bib-rb:\n    name: \"Roskilde Bibliotek\"\n    description: \"Roskilde Bibliotek\"\n    primary-domain: \"www.roskildebib.dk\"\n    secondary-domains: [\"roskildebib.dk\"]\n    dpl-cms-release: \"1.2.3\"\n    go-release: \"1.2.3\"\n    # Github package registry used as an example here, but any registry will\n    # work.\n    releaseImageRepository: ghcr.io/some-github-org\n    releaseImageName: some-image-name\n</code></pre> <p>Be aware that the referenced images needs to be publicly available as Lagoon currently only authenticates against ghcr.io.</p> <p>Sites on the \"webmaster\" plan must have the field <code>plan</code> set to <code>\"webmaster\"</code> as this indicates that an environment for testing custom Drupal modules should also be deployed. For example:</p> <pre><code>sites:\n  bib-rb:\n    name: \"Roskilde Bibliotek\"\n    description: \"Roskilde Bibliotek\"\n    primary-domain: \"www.roskildebib.dk\"\n    secondary-domains: [\"roskildebib.dk\"]\n    dpl-cms-release: \"1.2.3\"\n    go-release: \"1.2.3\"\n    plan: webmaster\n    &lt;&lt; : *default-release-image-source\n</code></pre> <p>The field <code>plan</code> defaults to <code>standard</code>.</p> <p>Now you are ready to sync the site state.</p>"},{"location":"DPL-Platform/runbooks/add-library-site-to-platform/#synchronize-site-state-for-site","title":"Synchronize site state for site","text":"<p>You have made a single additive change to the <code>sites.yaml</code> file. It is important to ensure that your branch is otherwise up-to-date with <code>main</code>, as the state you currently have in <code>sites.yaml</code> is what will be ensured exists in the platform.</p> <p>You may end up undoing changes that other people have done, if you don't have their changes to <code>sites.yaml</code> in your branch.</p> <p>Prerequisites:</p> <ul> <li>A Lagoon account on the Lagoon core with your ssh-key associated (created through   the Lagoon UI, on the Settings page)</li> </ul> <p>From within <code>dplsh</code> run the <code>sites:sync</code> task to sync the site state in sites.yaml, creating your new site:</p> <pre><code>task site:full-sync\n</code></pre> <p>Or to sync multiple sites.</p> <pre><code>task sites:sync\n</code></pre> <p>You may be prompted to confirm Terraform plan execution and approve other critical steps. Read and consider these messages carefully and ensure you are not needlessly changing other sites.</p> <p>Be aware that while <code>site:full-sync</code> works with a single site in Lagoon, Terraform always works on all sites, so if you see another site than the one you're adding in the Terraform plan, abort immediately.</p> <p>The synchronization process:</p> <ul> <li>ensures a Github repo is provisioned for the site</li> <li>creates a Lagoon configuration for the site and pushes it to the   relevant branches in the repo (for example, sites with <code>plan:   \"webmaster\"</code> also get a <code>moduletest</code> branch for testing custom   Drupal modules)</li> <li>ensures a Lagoon project is created for the site</li> <li>configures Lagoon to track and deploy all the relevant branches for   the site as environments</li> </ul> <p>If no other changes have been made to <code>sites.yaml</code>, the result is that your new site is created and deployed to all relevant environments.</p>"},{"location":"DPL-Platform/runbooks/changing-and-releasing-new-dplsh-version/","title":"Make changes to DPLSH","text":""},{"location":"DPL-Platform/runbooks/changing-and-releasing-new-dplsh-version/#when-to-use","title":"When to use","text":"<p>When for example the <code>kubectl</code> or other dependencies needs updating</p>"},{"location":"DPL-Platform/runbooks/changing-and-releasing-new-dplsh-version/#make-the-change","title":"Make the change","text":"<ol> <li>Go to the DPLSH directory and make the necessary changes on a new branch</li> <li>Build DPLSH locally by running <code>IMAGE_URL=dplsh IMAGE_TAG=someTagName   task build</code></li> <li>Test that it works by running <code>DPLSH_IMAGE=dplsh:local ./dplsh</code> and running     what ever commands need to be run to test that the change has the desired effect</li> <li>Check what version DPLSH is at here:     https://github.com/danskernesdigitalebibliotek/dpl-platform/releases</li> <li>Push the branch, have it review and merge it into <code>main</code></li> <li>Push a new tag to <code>main</code>. The tag should look like this: <code>dplsh-x.x.x</code>.     (If in doubt about what version to bump to; read this: https://semver.org/)</li> <li>Wait for main to automically build and release the new version</li> <li>Go to your main branch, enter the <code>/infrastructure</code> directory and     run <code>../tools/dplsh/dplsh.sh --update</code>.</li> </ol> <p>You are done and have the newest version of DPLSH on your machine.</p>"},{"location":"DPL-Platform/runbooks/connecting-the-lagoon-cli/","title":"Connecting the Lagoon CLI","text":""},{"location":"DPL-Platform/runbooks/connecting-the-lagoon-cli/#when-to-use","title":"When to use","text":"<p>When you want to use the Lagoon API via the CLI. You can connect from the DPL Shell, or from a local installation of the CLI.</p> <p>Using the DPL Shell requires administrative privileges to the infrastructure while a local CLI may connect using only the ssh-key associated to a Lagoon user.</p> <p>This runbook documents both cases, as well as how an administrator can extract the basic connection details a standard user needs to connect to the Lagoon installation.</p>"},{"location":"DPL-Platform/runbooks/connecting-the-lagoon-cli/#prerequisites","title":"Prerequisites","text":"<ul> <li>Your ssh-key associated with a lagoon user. This has to be done via the Lagoon   UI by either you for your personal account, or by an administrator who has   access to edit your Lagoon account.</li> <li>For local installations of the cli:</li> <li>The Lagoon CLI installed locally</li> <li>Connectivity details for the Lagoon environment</li> <li>For administrative access to extract connection details or use the lagoon cli   from within the dpl shell:</li> <li>A valid dplsh setup to extract the connectivity details</li> </ul>"},{"location":"DPL-Platform/runbooks/connecting-the-lagoon-cli/#procedure","title":"Procedure","text":""},{"location":"DPL-Platform/runbooks/connecting-the-lagoon-cli/#obtain-the-connection-details-for-the-environment","title":"Obtain the connection details for the environment","text":"<p>You can skip this step and go to Configure your local lagoon cli) if your environment is already in Current Platform environments and you just want to have a local lagoon cli working.</p> <p>If it is missing, go through the steps below and update the document if you have access, or ask someone who has.</p> <pre><code># Launch dplsh.\n$ cd infrastructure\n$ dplsh\n\n# 1. Set an environment,\n# export DPLPLAT_ENV=&lt;platform environment name&gt;\n# eg.\n$ export DPLPLAT_ENV=dplplat01\n\n# 2. Authenticate against AKS, needed by infrastructure and Lagoon tasks\n$ task cluster:auth\n\n# 3. Generate the Lagoon CLI configuration and authenticate\n# The Lagoon CLI is authenticated via ssh-keys. DPLSH will mount your .ssh\n# folder from your homedir, but if your keys are passphrase protected, we need\n# to unlock them.\n$ eval $(ssh-agent); ssh-add\n# Authorize the lagoon cli\n$ task lagoon:cli:config\n\n# List the connection details\n$ lagoon config list\n</code></pre>"},{"location":"DPL-Platform/runbooks/connecting-the-lagoon-cli/#configure-your-local-lagoon-cli","title":"Configure your local lagoon cli","text":"<p>Get the details in the angle-brackets from Current Platform environments:</p> <pre><code>$ lagoon config add \\\n    --graphql https://&lt;GraphQL endpoint&gt; \\\n    --ui https://&lt;Lagoon UI&gt; \\\n    --hostname &lt;SSH host&gt; \\\n    --ssh-key &lt;SSH Key Path&gt; \\\n    --port &lt;SSH port&gt;&gt; \\\n    --lagoon &lt;Lagoon name&gt;\n\n# Eg.\n$ lagoon config add \\\n    --graphql https://api.lagoon.dplplat01.dpl.reload.dk/graphql \\\n    --force \\\n    --ui https://ui.lagoon.dplplat01.dpl.reload.dk \\\n    --hostname 20.238.147.183 \\\n    --port 22 \\\n    --lagoon dplplat01\n</code></pre> <p>Then log in:</p> <pre><code># Set the configuration as default.\nlagoon config default --lagoon &lt;Lagoon name&gt;\nlagoon login\nlagoon whoami\n\n# Eg.\nlagoon config default --lagoon dplplat01\nlagoon login\nlagoon whoami\n</code></pre>"},{"location":"DPL-Platform/runbooks/fix-challenges-stuck-in-cert-manager/","title":"Fix challenges stuck in Cert-manager","text":"<p>We sometimes experience that Cert-manager either chokes on some certificate/ challenge or backoffs for such a long time that the challenge doesn't get handled in a timely manner</p> <p>This is guide on how to fix this.</p>"},{"location":"DPL-Platform/runbooks/fix-challenges-stuck-in-cert-manager/#when-to-use","title":"When to use","text":"<p>The development team is usually the first to write that some development environment they have provisioned, doesn't get a certificate. It may sometimes come from the libraries or DDF themselves.</p>"},{"location":"DPL-Platform/runbooks/fix-challenges-stuck-in-cert-manager/#prerequisites","title":"Prerequisites","text":"<p>You'll need <code>kubectl</code> configured to access the cluster. Either use the binary on your machine or use it from <code>DPLSH</code>.</p>"},{"location":"DPL-Platform/runbooks/fix-challenges-stuck-in-cert-manager/#procedure","title":"Procedure","text":"<ol> <li>Check how many challenges exists accross namespaces.</li> <li>Check the logs of the leader Cert-manager pod. If it is handling certicates wait until it is not.</li> <li>Kill the leader pod. It should now start handling the challenges again.</li> </ol> <p>Step 3 might have to be repeated several times until the challenges has all been handled.</p> <p>OBS: We have some subdomains that have not been configured by the libraries themselves yet. Thus we have some 10+ challenges that are always there.</p>"},{"location":"DPL-Platform/runbooks/how-release-a-new-version-for-approval-testing/","title":"Deploy new release for approval testing","text":""},{"location":"DPL-Platform/runbooks/how-release-a-new-version-for-approval-testing/#when-to-use","title":"When to use","text":"<p>When deploying a new release, for approval testing by DDF, on the staging project</p>"},{"location":"DPL-Platform/runbooks/how-release-a-new-version-for-approval-testing/#prerequisites","title":"Prerequisites","text":"<ul> <li>A local checkout of the <code>dpl-platform</code> repository</li> <li>A running dplsh with <code>DPLPLAT_ENV</code> set to the platform   environment name.</li> <li>The version tag you want to deploy. This must correspond to a   tagged version of the <code>dpl-cms-source</code> image.</li> <li>The version tag of Go that you want to deploy. The version generally   follows the cms version unless a hotfix is being released.</li> </ul>"},{"location":"DPL-Platform/runbooks/how-release-a-new-version-for-approval-testing/#procedure-new-release-for-approval-testing","title":"Procedure: New release for approval testing","text":"<p>Use this procedure to deploy a new version for testing before it is released to production library sites.</p> <ol> <li>In your local environment ensure that your checkout of the <code>main</code> branch for    <code>dpl-platform</code> is up to date.</li> <li>Create a new branch from <code>main</code>.</li> <li>Open <code>infrastructure/environments/dplplat01/sites.yaml</code></li> <li>Set the value of <code>dpl-cms-release</code> and <code>moduletest-dpl-cms-release</code> for       <code>staging</code> to the new version</li> <li>Set the value of <code>moduletest-dpl-cms-release</code> for <code>bnf</code> to the new version</li> <li>Set the value of <code>go-release</code> for to the new version.</li> <li>Commit the change and push your branch to GitHub and create a pull request.</li> <li>Request a review for the change and wait for approval.</li> <li>Start <code>dplsh</code> from the <code>/infrastructure</code> directory of your local    environment by running <code>../tools/dplsh/dplsh.sh</code>.</li> <li>Deploy the changes</li> <li>Run <code>SITE=staging task site:sync</code></li> <li>Run <code>SITE=bnf task site:sync</code></li> <li>Wait for the deployments for <code>staging</code> and <code>bnf</code> to complete using Lagoon UI</li> <li>If a deployment does not complete determine if the error relates to the    platform or the application.</li> <li>If it is a platform-related error then try to redeploy the environment from     the Lagoon UI.</li> <li>Merge the pull request once the deployment completes.</li> </ol>"},{"location":"DPL-Platform/runbooks/how-release-a-new-version-for-approval-testing/#procedure-a-site-fails-to-deploy","title":"Procedure: A site fails to deploy","text":"<p>We have experience this quite a lot. We have gathered a list of known issues and how to solve them a troubleshoot runbook</p>"},{"location":"DPL-Platform/runbooks/monthly-release-to-editors-and-webmasters/","title":"Monthly release to editors and webmasters","text":""},{"location":"DPL-Platform/runbooks/monthly-release-to-editors-and-webmasters/#when-to-use","title":"When to use","text":"<p>The last thursday of the month we deploy as usual as well as to the webmasters production environments as well. It is very important to note here. The webmaster production environments will have the latest release - 1. This usually mean last weeks release.</p>"},{"location":"DPL-Platform/runbooks/monthly-release-to-editors-and-webmasters/#prerequisites","title":"Prerequisites","text":"<ul> <li>A local checkout of the <code>dpl-platform</code> repository</li> <li>A running dplsh with <code>DPLPLAT_ENV</code> set to the platform   environment name.</li> <li>The version tag you want to deploy. This must correspond to a   tagged version of the <code>dpl-cms-source</code> image.</li> </ul>"},{"location":"DPL-Platform/runbooks/monthly-release-to-editors-and-webmasters/#procedure-new-version-to-editors-and-moduletest-environments","title":"Procedure: New version to editors and moduletest environments","text":"<ol> <li>In your local environment ensure that your checkout of the <code>main</code> branch for    <code>dpl-platform</code> is up to date.</li> <li>Create a new branch from <code>main</code>.</li> <li>Now update <code>infrastructure/environments/dplplat01/sites.yaml</code>:<ol> <li>The 'x-defaults' anchors' 'dpl-cms-release' tag should be bumped to the new version.</li> <li>Then update the 'moduletest-dpl-cms-release' of 'x-webmaster' and 'x-webmaster-with-defaults' anchor.</li> <li>Now update 'dpl-cms-release' of the 'x-webmaster' anchor to the penultimate version of the cms.</li> </ol> </li> <li>Commit the change and push your branch to GitHub and create a pull request.</li> <li>Request a review for the change and wait for approval.</li> <li>Start <code>dplsh</code> from the <code>/infrastructure</code> directory of your local    environment by running <code>../tools/dplsh/dplsh.sh</code></li> <li>Run <code>task cluster:mode:release</code> to get more nodes while deploying.</li> <li>Run <code>task sites:sync</code> from <code>dplsh</code> to deploy the changes.</li> <li>If there are any Terraform changes then do not apply them, abort the    deployment and consult the platform team.</li> <li>Open the Deployments list page within the Lagoon UI to see all running and    queued deployments.</li> <li>Wait for all the deployments to complete.</li> <li>Run <code>task sites:incomplete-deployments</code> to identify any failed deployments.</li> <li>If some deployments did not complete determine if the error relates to the     platform or the application.</li> <li>For all platform-related errors try to redeploy the environment from     the Lagoon UI.</li> <li>Merge the pull request once the deployment completes.</li> <li>Run <code>task cluster:adjust:resource-request</code> from <code>dplsh</code>.</li> <li>Run <code>task cluster:promote-workloads-to-prod</code> from <code>dplsh</code>.</li> <li>Synchronize moduletest with main, so moduletest becomes an exact     copy of main by running <code>task sites:webmaster:reset-moduletest</code>.</li> <li>Run 'task cluster:mode:reset' from <code>dplsh</code>.</li> </ol>"},{"location":"DPL-Platform/runbooks/monthly-release-to-editors-and-webmasters/#procedure-a-some-sites-fails-to-deploy","title":"Procedure: a some sites fails to deploy","text":"<p>We have experience this quite a lot. We have gathered a list of known issues and how to solve them a troubleshoot runbook</p>"},{"location":"DPL-Platform/runbooks/rabbitmq-broker/","title":"RabbitMQ broker force start","text":""},{"location":"DPL-Platform/runbooks/rabbitmq-broker/#when-to-use","title":"When to use","text":"<p>When the PR environments are no longer being created, and the <code>lagoon-core-broker-&lt;n&gt;</code> pods are missing or not running, and the container logs contain errors like <code>Error while waiting for Mnesia tables: {timeout_waiting_for_tables</code>.</p> <p>This situation is caused by the RabbitMQ broker not starting correctly.</p>"},{"location":"DPL-Platform/runbooks/rabbitmq-broker/#prerequisites","title":"Prerequisites","text":"<ul> <li>A dplsh session with DPLPLAT_ENV exported .</li> </ul>"},{"location":"DPL-Platform/runbooks/rabbitmq-broker/#procedure","title":"Procedure","text":"<p>You are going to exec into the pod and stop the RabbitMQ application, and then start it with the <code>force_boot</code> feature, so that it can perform its Mnesia sync correctly.</p> <p>Exec into the pod:</p> <pre><code>dplsh:~/host_mount$ kubectl -n lagoon-core exec -ti pod/lagoon-core-broker-0 -- sh\n</code></pre> <p>Stop RabbitMQ:</p> <pre><code>/ $ rabbitmqctl stop_app\nStopping rabbit application on node rabbit@lagoon-core-broker-0.lagoon-\ncore-broker-headless.lagoon-core.svc.cluster.local ...\n</code></pre> <p>Start it immediately after using the <code>force_boot</code> flag:</p> <pre><code>/ $ rabbitmqctl force_boot\n</code></pre> <p>Then exit the shell and check the container logs for one of the broker pods. It should start without errors.</p>"},{"location":"DPL-Platform/runbooks/remove-site-from-platform/","title":"Removing a site from the platform","text":""},{"location":"DPL-Platform/runbooks/remove-site-from-platform/#when-to-use","title":"When to use","text":"<p>When you wish to delete a site and all its data from the platform</p> <p>Prerequisites:</p> <ul> <li>A lagoon account with your ssh-key associated (created through   the Lagoon UI, on the Settings page)</li> <li>The site key (its key in sites.yaml)</li> <li>A properly authenticated azure CLI (<code>az</code>) that has administrative access to   the cluster running the lagoon installation</li> </ul>"},{"location":"DPL-Platform/runbooks/remove-site-from-platform/#procedure","title":"Procedure","text":"<p>The procedure consists of the following steps (numbers does not correspond to the numbers in the script below).</p> <ol> <li>Download and archive relevant backups</li> <li>Remove the project from Lagoon</li> <li>Delete the project namespace from kubernetes.</li> <li>Delete the site from sites.yaml</li> <li>Delete the site's environment repository</li> </ol>"},{"location":"DPL-Platform/runbooks/remove-site-from-platform/#1-download-and-archive-relevant-backups","title":"1. Download and archive relevant backups","text":"<p>Your first step should be to secure any backups you think might be relevant to archive. Whether this step is necessary depends on the site. Consult the Retrieve and Restore backups runbook for the operational steps.</p>"},{"location":"DPL-Platform/runbooks/remove-site-from-platform/#2-remove-the-project-from-lagoon","title":"2. Remove the project from Lagoon","text":"<p>You are now ready to perform the actual removal of the site.</p> <pre><code># Launch dplsh.\n$ cd infrastructure\n$ dplsh\n\n# You are assumed to be inside dplsh from now on.\n\n# Set an environment,\n# export DPLPLAT_ENV=&lt;platform environment name&gt;\n# eg.\n$ export DPLPLAT_ENV=dplplat01\n\n# Setup access to ssh-keys so that the lagoon cli can authenticate.\n$ eval $(ssh-agent); ssh-add\n\n# Authenticate against lagoon\n$ task lagoon:cli:config\n\n# Delete the project from Lagoon\n# lagoon delete project --project &lt;site machine-name&gt;\n$ lagoon delete project  --project core-test1\n</code></pre>"},{"location":"DPL-Platform/runbooks/remove-site-from-platform/#3-delete-the-project-namespace-from-kubernetes","title":"3. Delete the project namespace from kubernetes","text":"<pre><code># Authenticate against kubernetes\n$ task cluster:auth\n\n# List the namespaces\n# Identify all the project namespace with the syntax &lt;sitename&gt;-&lt;branchname&gt;\n# eg \"core-test1-main\" for the main branch for the \"core-test1\" site.\n$ kubectl get ns\n\n# Delete each site namespace\n# kubectl delete ns &lt;namespace&gt;\n# eg.\n$ kubectl delete ns core-test1-main\n</code></pre>"},{"location":"DPL-Platform/runbooks/remove-site-from-platform/#4-delete-the-site-from-sitesyaml","title":"4. Delete the site from sites.yaml","text":"<pre><code># 8. Edit sites.yaml, remove the the entry for the site\n$ vi environments/${DPLPLAT_ENV}/sites.yaml\n</code></pre>"},{"location":"DPL-Platform/runbooks/remove-site-from-platform/#5-delete-the-sites-environment-repository","title":"5. Delete the site's environment repository","text":"<pre><code># Then have Terraform delete the sites repository.\n$ task env_repos:provision\n</code></pre>"},{"location":"DPL-Platform/runbooks/reset-moduletest/","title":"Reset moduletest site","text":""},{"location":"DPL-Platform/runbooks/reset-moduletest/#when-to-use","title":"When to use","text":"<p>When a moduletest site need resetting.</p>"},{"location":"DPL-Platform/runbooks/reset-moduletest/#prerequisites","title":"Prerequisites","text":"<ul> <li>An authenticated <code>az</code> cli. The logged in user must have full administrative   permissions to the platforms azure infrastructure.</li> <li>A running dplsh with <code>DPLPLAT_ENV</code> set to the platform   environment name.</li> </ul>"},{"location":"DPL-Platform/runbooks/reset-moduletest/#procedure","title":"Procedure","text":"<p>From within <code>dplsh</code> run:</p> <pre><code>PROJECT=&lt;site&gt; task sites:webmaster:reset-moduletest\n</code></pre> <p>Where <code>&lt;site&gt;</code> is the site to reset.</p>"},{"location":"DPL-Platform/runbooks/retrieve-restore-backup/","title":"Retrieving backups","text":""},{"location":"DPL-Platform/runbooks/retrieve-restore-backup/#when-to-use","title":"When to use","text":"<p>When you wish to download an automatic backup made by Lagoon, and optionally restore it into an existing site.</p>"},{"location":"DPL-Platform/runbooks/retrieve-restore-backup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Administrative access to the site in the Lagoon UI</li> <li>(for restore) administrative cluster-access to the site</li> </ul>"},{"location":"DPL-Platform/runbooks/retrieve-restore-backup/#procedure","title":"Procedure","text":"<p>Step overview:</p> <ol> <li>Download the backup</li> <li>Upload the backup to relevant pods</li> <li>Extract the backup.</li> <li>Cache clearing</li> <li>Cleanup</li> </ol> <p>While most steps are different for file and database backups, step 1 is close to identical for the two guides.</p> <p>Be aware that the guide instructs you to copy the backups to <code>/tmp</code> inside the cli pod. Depending on the resources available on the node <code>/tmp</code> may not have enough space in which case you may need to modify the <code>cli</code> deployment to add a temporary volume, or place the backup inside the existing <code>/site/default/files</code> folder.</p>"},{"location":"DPL-Platform/runbooks/retrieve-restore-backup/#step-1-downloading-the-backup","title":"Step 1, downloading the backup","text":"<p>To download the backup access the Lagoon UI and schedule the retrieval of a  backup. To do this,</p> <ol> <li>Log in to the environments Lagoon UI (consult the  environment documentation for the url)</li> <li>Access the site's project</li> <li>Access the site's environment (\"main\" for production)</li> <li>Click on the \"Backups\" tab</li> <li>Click on the \"Retrieve\" button for the backups you wish to download and/or     restore. Use to \"Source\" column to differentiate the types of backups.     \"nginx\" are backups of the sites files, while \"mariadb\" are backups of the     sites database.</li> <li>The Buttons changes to \"Downloading...\" when pressed, wait for them to     change to \"Download\", then click them again to download the backup</li> </ol>"},{"location":"DPL-Platform/runbooks/retrieve-restore-backup/#step-2a-restore-a-database","title":"Step 2a, restore a database","text":"<p>To restore the database we must first copy the backup into a running cli-pod for a site, and then import the database-dump on top of the running site.</p> <ol> <li>Copy the uncompressed mariadb sql file you want to restore into the <code>dpl-platform/infrastructure</code>    folder from which we will launch dplsh</li> <li>Launch dplsh from the infrastructure folder (instructions)    and follow the procedure below:</li> </ol> <pre><code># 1. Authenticate against the cluster.\n$ task cluster:auth\n\n# 2. List the namespaces to identify the sites namespace\n# The namespace will be on the form &lt;sitename&gt;-&lt;branchname&gt;\n# eg \"bib-rb-main\" for the \"main\" branch for the \"bib-rb\" site.\n$ kubectl get ns\n\n# 3. Export the name of the namespace as SITE_NS\n# eg.\n$ export SITE_NS=bib-rb-main\n\n# 4. Copy the *mariadb.sql file to the CLI pod in the sites namespace\n# eg.\nkubectl cp \\\n  -n $SITE_NS  \\\n  *mariadb.sql \\\n  $(kubectl -n $SITE_NS get pod -l app.kubernetes.io/instance=cli -o jsonpath=\"{.items[0].metadata.name}\"):/tmp/database-backup.sql\n\n# 5. Have drush inside the CLI-pod import the database and clear out the backup\nkubectl exec \\\n  -n $SITE_NS \\\n  deployment/cli \\\n  -- \\\n    bash -c \" \\\n         echo Verifying file \\\n      &amp;&amp; test -s /tmp/database-backup.sql \\\n         || (echo database-backup.sql is missing or empty &amp;&amp; exit 1) \\\n      &amp;&amp; echo Dropping database \\\n      &amp;&amp; drush sql-drop -y \\\n      &amp;&amp; echo Importing backup \\\n      &amp;&amp; drush sqlc &lt; /tmp/database-backup.sql \\\n      &amp;&amp; echo Clearing cache \\\n      &amp;&amp; drush cr \\\n      &amp;&amp; rm /tmp/database-backup.sql\n    \"\n</code></pre>"},{"location":"DPL-Platform/runbooks/retrieve-restore-backup/#step-2b-restore-a-sites-files","title":"Step 2b, restore a sites files","text":"<p>To restore backed up files into a site we must first copy the backup into a running cli-pod for a site, and then rsync the files on top top of the running site.</p> <ol> <li>Copy tar.gz file into the <code>dpl-platform/infrastructure</code> folder from which we    will launch dplsh</li> <li>Launch dplsh from the infrastructure folder (instructions)    and follow the procedure below:</li> </ol> <pre><code># 1. Authenticate against the cluster.\n$ task cluster:auth\n\n# 2. List the namespaces to identify the sites namespace\n# The namespace will be on the form &lt;sitename&gt;-&lt;branchname&gt;\n# eg \"bib-rb-main\" for the \"main\" branch for the \"bib-rb\" site.\n$ kubectl get ns\n\n# 3. Export the name of the namespace as SITE_NS\n# eg.\n$ export SITE_NS=bib-rb-main\n\n# 4. Copy the files tar-ball into the CLI pod in the sites namespace\n# eg.\nkubectl cp \\\n  -n $SITE_NS  \\\n  backup*-nginx-*.tar.gz \\\n  $(kubectl -n $SITE_NS get pod -l app.kubernetes.io/instance=cli -o jsonpath=\"{.items[0].metadata.name}\"):/tmp/files-backup.tar.gz\n\n# 5. Replace the current files with the backup.\n# The following\n# - Verifies the backup exists\n# - Removes the existing sites/default/files\n# - Un-tars the backup into its new location\n# - Fixes permissions and clears the cache\n# - Removes the backup archive\n#\n# These steps can also be performed one by one if you want to.\nkubectl exec \\\n  -n $SITE_NS \\\n  deployment/cli \\\n  -- \\\n    bash -c \" \\\n         echo Verifying file \\\n      &amp;&amp; test -s /tmp/files-backup.tar.gz \\\n         || (echo files-backup.tar.gz is missing or empty &amp;&amp; exit 1) \\\n      &amp;&amp; tar ztf /tmp/files-backup.tar.gz data/nginx &amp;&gt; /dev/null \\\n         || (echo could not verify the tar.gz file files-backup.tar &amp;&amp; exit 1) \\\n      &amp;&amp; test -d /app/web/sites/default/files \\\n         || (echo Could not find destination /app/web/sites/default/files \\\n             &amp;&amp; exit 1) \\\n      &amp;&amp; echo Removing existing sites/default/files \\\n      &amp;&amp; rm -fr /app/web/sites/default/files \\\n      &amp;&amp; echo Unpacking backup \\\n      &amp;&amp; mkdir -p /app/web/sites/default/files \\\n      &amp;&amp; tar --strip 2 --gzip --extract --file /tmp/files-backup.tar.gz \\\n             --directory /app/web/sites/default/files data/nginx \\\n      &amp;&amp; echo Fixing permissions \\\n      &amp;&amp; chmod -R 777 /app/web/sites/default/files \\\n      &amp;&amp; echo Clearing cache \\\n      &amp;&amp; drush cr \\\n      &amp;&amp; echo Deleting backup archive \\\n      &amp;&amp; rm /tmp/files-backup.tar.gz\n    \"\n\n#  NOTE: In some situations some files in /app/web/sites/default/files might\n#  be locked by running processes. In that situations delete all the files you\n#  can from /app/web/sites/default/files manually, and then repeat the step\n#  above skipping the removal of /app/web/sites/default/files\n</code></pre>"},{"location":"DPL-Platform/runbooks/retrieve-sites-logs/","title":"Retrieve site logs","text":""},{"location":"DPL-Platform/runbooks/retrieve-sites-logs/#when-to-use","title":"When to use","text":"<p>When you want to inspects the logs produced by as specific site</p>"},{"location":"DPL-Platform/runbooks/retrieve-sites-logs/#prerequisites","title":"Prerequisites","text":"<ul> <li>Login credentials for Grafana. As a fallback the password for the <code>admin</code> can   password can be fetched from the cluster if it has not been changed via</li> </ul> <pre><code>kubectl get secret \\\n  --namespace grafana \\\n  -o jsonpath=\"{.data.admin-password}\" \\\n  grafana \\\n| base64 -d\n</code></pre> <p>Consult the access-kubernetes Run book for instructions on how to access the cluster.</p>"},{"location":"DPL-Platform/runbooks/retrieve-sites-logs/#procedure-loki-grafana","title":"Procedure - Loki / Grafana","text":"<ol> <li>Access the environments Grafana installation - consult the    platform-environments.md for the url.</li> <li>Select \"Explorer\" in the left-most menu and select the \"Loki\" data source in    the top.</li> <li>Query the logs for the environment by either</li> <li>Use the Log Browser to pick the namespace for the site. It will follow the       pattern <code>&lt;sitename&gt;-&lt;branchname&gt;</code>.</li> <li>Do a custom LogQL, eg. to      fetch all logs from the nginx container for the site \"main\" branch of the      \"rdb\" site do  query on the form      <code>{app=\"nginx-php-persistent\",container=\"nginx\",namespace=\"rdb-main\"}</code></li> <li>Eg, for the  main branch for the site \"rdb\": <code>{namespace=\"rdb-main\"}</code></li> <li>Click \"Inspector\" -&gt; \"Data\" -&gt; \"Download Logs\" to download the log lines.</li> </ol>"},{"location":"DPL-Platform/runbooks/run-a-lagoon-task/","title":"Run a Lagoon Task","text":""},{"location":"DPL-Platform/runbooks/run-a-lagoon-task/#when-to-use","title":"When to use","text":"<p>When you need to run a Lagoon task</p>"},{"location":"DPL-Platform/runbooks/run-a-lagoon-task/#prerequisites","title":"Prerequisites","text":"<p>You need access to the Lagoon UI</p>"},{"location":"DPL-Platform/runbooks/run-a-lagoon-task/#procedure","title":"Procedure","text":"<ul> <li>Login to the Lagoon UI.</li> <li>Navigate to the project you want to access.</li> <li>Choose the environment you want to interact with.</li> <li>Click the \"Tasks\" tab.</li> </ul>"},{"location":"DPL-Platform/runbooks/scale-aks/","title":"Scaling AKS","text":""},{"location":"DPL-Platform/runbooks/scale-aks/#when-to-use","title":"When to use","text":"<p>When the cluster is over or underprovisioned and needs to be scaled.</p>"},{"location":"DPL-Platform/runbooks/scale-aks/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running dplsh launched from <code>./infrastructure</code> with   <code>DPLPLAT_ENV</code> set to the platform environment name.</li> </ul>"},{"location":"DPL-Platform/runbooks/scale-aks/#references","title":"References","text":"<ul> <li>For general information about AKS and node-pools https://learn.microsoft.com/en-us/azure/aks/use-multiple-node-pools</li> </ul>"},{"location":"DPL-Platform/runbooks/scale-aks/#procedure","title":"Procedure","text":"<p>There are multiple approaches to scaling AKS. We run with the auto-scaler enabled which means that in most cases the thing you want to do is to adjust the max or minimum configuration for the autoscaler.</p> <ul> <li>Adjusting the autoscaler</li> </ul>"},{"location":"DPL-Platform/runbooks/scale-aks/#adjusting-the-autoscaler","title":"Adjusting the autoscaler","text":"<p>Edit the infrastructure configuration for your environment. Eg for <code>dplplat01</code> edit <code>infrastructure/environments/dplplat01/infrastructure/main.tf</code>.</p> <p>Adjust the <code>..._count_min</code> / <code>..._count_min</code> corresponding to the node-pool you want to grow/shrink.</p> <p>Then run <code>infra:provision</code> to have terraform effect the change.</p> <pre><code>task infra:provision\n</code></pre>"},{"location":"DPL-Platform/runbooks/set-environment-variable/","title":"Set an environment variable for a site","text":""},{"location":"DPL-Platform/runbooks/set-environment-variable/#when-to-use","title":"When to use","text":"<p>When you wish to set an environment variable on a site. The variable can be available for either all sites in the project, or for a specific site in the project.</p> <p>The variables are safe for holding secrets, and as such can be used both for \"normal\" configuration values, and secrets such as api-keys.</p> <p>The variabel will be available to all containers in the environment can can be picked up and parsed eg. in Drupals <code>settings.php</code>.</p>"},{"location":"DPL-Platform/runbooks/set-environment-variable/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running dplsh with <code>DPLPLAT_ENV</code> set to the platform   environment name and ssh-agent running if your ssh-keys are passphrase   protected.</li> <li>A Lagoon account on the Lagoon core with your ssh-key associated (created   through the Lagoon UI, on the Settings page)</li> </ul>"},{"location":"DPL-Platform/runbooks/set-environment-variable/#procedure","title":"Procedure","text":"<pre><code># From within a dplsh session authorized to use your ssh keys:\n\n# 1. Authenticate against the cluster and lagoon\n$ task cluster:auth\n$ task lagoon:cli:config\n\n# 2. Refresh your Lagoon token.\n$ lagoon login\n\n# 3a. For project-level variables, use the following command, which creates\n#     the variable if it does not yet exist, or updates it otherwise:\n$ task lagoon:ensure:environment-variable \\\n  PROJECT_NAME=&lt;project name&gt; \\\n  VARIABLE_SCOPE=RUNTIME \\\n  VARIABLE_NAME=&lt;your variable name&gt; \\\n  VARIABLE_VALUE=&lt;your variable value&gt;\n\n# 3b. Or, to similarly ensure the value of an environment-level variable:\n$ task lagoon:ensure:environment-variable \\\n  PROJECT_NAME=&lt;project name&gt; \\\n  ENVIRONMENT_NAME=&lt;environment name&gt; \\\n  VARIABLE_SCOPE=RUNTIME \\\n  VARIABLE_NAME=&lt;your variable name&gt; \\\n  VARIABLE_VALUE=&lt;your variable value&gt;\n\n# If you get a \"Invalid Auth Token\" your token has probably expired, generated a\n# new with \"lagoon login\" and try again.\n</code></pre>"},{"location":"DPL-Platform/runbooks/troubleshoot-release-deployment/","title":"Troubleshooting","text":"<p>During the deployment process you may encounter problems. This is a list of known problems and how to address them.</p> <p>If a problem is not mentioned explicitly it should be raised to team which is most likely responsible:</p> <ul> <li>All deployment steps prefixed with Post-Rollout: Application developers</li> <li>All other errors: Platform administrators</li> </ul>"},{"location":"DPL-Platform/runbooks/troubleshoot-release-deployment/#platform-related-errors","title":"Platform-related errors","text":"<p>A Lagoon deployment can end up with a Failed status. If this is the case then the following errors may show up in the log output shown for the deployment in the Lagoon UI.</p>"},{"location":"DPL-Platform/runbooks/troubleshoot-release-deployment/#tls-handshake-timeout","title":"TLS handshake timeout","text":"<p>If the log output of a deployment contains the following during various stages of the deployment then redeploy the environment from the Lagoon UI:</p> <p><code>Unable to connect to the server: net/http: TLS handshake timeout</code></p> <p>This may be caused by a spike in load during deployment.</p>"},{"location":"DPL-Platform/runbooks/troubleshoot-release-deployment/#context-deadline-exceeded","title":"Context deadline exceeded","text":"<p>If the log output of a deployment contains the following then redeploy the environment from the Lagoon UI:</p> <p><code>failed to call webhook: Post \"[Ingress url]\": context deadline exceeded</code></p> <p>This may be caused by a spike in load during deployment.</p>"},{"location":"DPL-Platform/runbooks/troubleshoot-release-deployment/#harbor-authentication-failed","title":"Harbor authentication failed","text":"<p>If log output for the deployment contains the following during the Image Push to Registry stage of the deployment then redeploy the environment from the Lagoon UI:</p> <p><code>level=fatal msg=\"copying system image from manifest list: trying to reuse blob sha256:[SHA] at destination: checking whether a blob sha256:[SHA] exists in [Harbor url]: authentication required\"</code></p> <p>This is caused by a bug in Harbor. It should be addressed in a future update of this part of the platform.</p>"},{"location":"DPL-Platform/runbooks/troubleshoot-release-deployment/#mysql-server-has-gone-away","title":"MySQL server has gone away","text":"<p>If log output for the deployment contains the following during the Post-Rollout drush deploy stage of the deployment then redeploy the environment from the Lagoon UI:</p> <p><code>Drupal\\Core\\Database\\DatabaseExceptionWrapper: SQLSTATE[HY000]: General error: 2006 MySQL server has gone away</code></p> <p>This may be caused by intermittent database problems caused by spike in load during deployments.</p>"},{"location":"DPL-Platform/runbooks/troubleshoot-release-deployment/#service-deployment-exceeded-its-progress-deadline","title":"Service deployment exceeded its progress deadline","text":"<p>If log output for the deployment contains the following during the Applying Deployments stage of the deployment then redeploy the environment from the Lagoon UI:</p> <pre><code>Waiting for deployment \"[Service]\" rollout to finish: 1 old replicas are pending\ntermination...\nerror: deployment \"varnish\" exceeded its progress deadline`\n</code></pre> <p>This may be caused by a spike in load during deployment.</p>"},{"location":"DPL-Platform/runbooks/troubleshoot-release-deployment/#drush-bootstrap-failed","title":"Drush bootstrap failed","text":"<p>If log output for the deployment contains the following during the Post-Rollout drush deploy stage of the deployment then redeploy the environment from the Lagoon UI:</p> <pre><code>The command \"/app/vendor/bin/drush updatedb --uri=https://[Domain]\" failed.`\nExit Code: 1(General error)\n[...]\nBootstrap failed. [...]\n</code></pre> <p>This may be caused by intermittent database problems caused by spike in load during deployments.</p>"},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/","title":"Troubleshoot StatusCake alert","text":""},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#when-to-use","title":"When to use","text":"<p>When StatusCake raises an alert about a library site being down.</p>"},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to the used StatusCake account</li> <li>Access to Grafana</li> <li>Access to Lagoon UI</li> </ul>"},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#procedure","title":"Procedure","text":"<ol> <li>Determine whether the site is accessible by accessing the <code>/health</code> endpoint    for the site</li> <li>Note the response</li> <li>Log into StatusCake</li> <li>Locate the uptime test for the library site in question</li> <li>Find the downtime root cause for alert based on the time the alert.</li> <li>Click Extra details</li> <li>Note the error</li> <li>Log into Grafana</li> <li>Click Explore</li> <li>Add a label filter with the label namespace and value being the name of     the site being down.</li> <li>Add a label filter with the label app and value php</li> <li>Set the time range to include the start of the alert</li> <li>Note any errors</li> </ol>"},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#known-problems","title":"Known problems","text":"<p>These are a list of known problems and how to address them.</p>"},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#dns-lookup-errors","title":"DNS lookup errors","text":""},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#observations","title":"Observations","text":"<ul> <li>StatusCake reports Request timeout and <code>EAI_AGAIN</code> additional data.</li> </ul>"},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#action","title":"Action","text":"<p>You can ignore this error. StatusCake is experiencing DNS lookup issues. These are outside the scope of the platform.</p>"},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#corrupt-drupal-cache","title":"Corrupt Drupal cache","text":""},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#observations_1","title":"Observations","text":"<ul> <li><code>/health</code> reports HTTP status code 500</li> <li>Grafana logs contain PHP exceptions</li> </ul>"},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#action_1","title":"Action","text":"<ol> <li>Log into Lagoon UI</li> <li>Locate the project in question</li> <li>Locate the environment in question</li> <li>Locate Tasks</li> <li>Run the task Clear Drupal caches for the environment in question</li> <li>Wait for the task to finish</li> <li>Verify that <code>/health</code> reports HTTP status code 200</li> </ol>"},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#database-connection-errors","title":"Database connection errors","text":""},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#observations_2","title":"Observations","text":"<ul> <li><code>/health</code> reports HTTP status code 500</li> <li>Grafana logs contain <code>PDOException: SQLSTATE[HY000]</code></li> </ul>"},{"location":"DPL-Platform/runbooks/troubleshoot-statuscake-monitor/#actions","title":"Actions","text":"<p>Do nothing. This is likely caused by a restart of the underlying database. Experience shows that it takes about 20 minutes for the restart to complete.</p> <p>Note that such errors will affect all sites running on the platform and will result in multiple alerts being raised.</p>"},{"location":"DPL-Platform/runbooks/ui-reset-moduletest/","title":"UI: Reset moduletest site","text":""},{"location":"DPL-Platform/runbooks/ui-reset-moduletest/#when-to-use","title":"When to use","text":"<p>When a moduletest site need resetting</p> <p>Warning: This procedure does not sync the site properly, as it doesn't delete files/tables that exists on moduletest but not in production. In many cases, this isn't a problem, but i some instances it could be. If in doubt, ask a platform engineer to do the sync.</p>"},{"location":"DPL-Platform/runbooks/ui-reset-moduletest/#procedure","title":"Procedure","text":"<ol> <li>Find the moduletest site of the project that need resetting in the Lagoon UI</li> <li>Go to tasks</li> <li>Select \"Copy files between environments\", and set the source   to \"main\" and run the task. This queues the task and lagoon will run it when   it can.   There's some role/user restrictions in place by lagoon, which   makes the task fail, as it can't set the correct time on the files. If that   is the only   error, then it copied the files, and this step is done. Otherwise run it again</li> <li>Select \"Copy database between environments\", and set the source   to \"main\" and run the task</li> <li>Go to deployments and pres \"deploy\", this ensures that the state matches that   of the environment.</li> <li>Go the page URL and check that it looks alright</li> </ol> <p>The moduletest site is now reset</p>"},{"location":"DPL-Platform/runbooks/ui-sync-site-state/","title":"UI: Synchronize site state","text":""},{"location":"DPL-Platform/runbooks/ui-sync-site-state/#when-to-use","title":"When to use","text":"<p>If you want to synchronize state from one environment into another environment.</p> <p>For example, you may want to synchronize state to a PR environment to run your code in a more realistic setup.</p> <p>Or you may want to synchronize a main (production) environment state to a moduletest environment, if requested by the customer.</p>"},{"location":"DPL-Platform/runbooks/ui-sync-site-state/#prerequisites","title":"Prerequisites","text":"<ul> <li>A user with access to the relevant project through the   Lagoon UI</li> </ul> <p>If you have access to the dpl-platform setup and can run task in the taskfile (for platform engineers, not developers of the CMS) you may want to synchronize site state using the related task (runbook WIP).</p>"},{"location":"DPL-Platform/runbooks/ui-sync-site-state/#procedure","title":"Procedure","text":"<ol> <li>Go to the [Lagoon UI] website and log in</li> <li>Navigate to the relevant project by selecting in the list</li> <li>Pick the target environment in the list of environments. E.g. if you are    synchronizing state from <code>main</code> to <code>pr-775</code> you should select <code>pr-775</code>.</li> <li>In the left-hand side pick the \"Tasks\" menu point</li> </ol> <p>Now you are at the tasks UI and can execute tasks for this environment. It should look something like this:</p> <p></p> <p>Now we need to execute 3 tasks to synchronize the whole state and make it available on visits to the target site:</p> <ol> <li> <p>Run task \"Copy database between environments [drush sql-sync]\":</p> <ul> <li>Select the task in the \"Select a task...\" dropdown.</li> <li>Select the source environment. E.g. if you are synchronizing from <code>main</code> to   <code>pr-775</code> select <code>main</code> in the dropdown.</li> <li>Click \"Run task\" to start the task.   The task appears in the top of the list of tasks. You can click it to see   log output. Once the task completes verify that the log output states that   the synchronization worked.</li> </ul> </li> </ol> <ol> <li> <p>Run task \"Copy files between environments [drush rsync]\":</p> <ul> <li>Select the task in the \"Select a task...\" dropdown.</li> <li>Select the source environment as above.</li> <li>Click \"Run task\" to start the task.   The task output can be viewed as described in point 5.   The task will fail. Verify that the error is a list of statements saying   <code>&gt; rsync: [receiver] failed to set times on ...</code>. As long as these are the   only errors in the output, the synchronization succeeded.</li> </ul> </li> </ol> <ol> <li> <p>Run task \"Clear Drupal caches [drupal cache-clear]\" to clear the caches:</p> <ul> <li>Select the task in the \"Select a task...\" dropdown.</li> <li>Select the source environment as above.</li> <li>Click \"Run task\" to start the task.   Once the task completes the environment has been fully synced and caches   are cleared so the state will be reflected when you visit the site.   E.g. if you were synchronizing state from <code>main</code> to <code>pr-775</code>, the <code>pr-775</code>   environment will now have the same state as <code>main</code>.</li> </ul> </li> </ol>"},{"location":"DPL-Platform/runbooks/update-upgrade-status/","title":"Update the support workload upgrade status","text":""},{"location":"DPL-Platform/runbooks/update-upgrade-status/#when-to-use","title":"When to use","text":"<p>When you need to update the support workload version sheet.</p>"},{"location":"DPL-Platform/runbooks/update-upgrade-status/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to the version status sheet</li> <li>Access to run dplsh</li> </ul>"},{"location":"DPL-Platform/runbooks/update-upgrade-status/#procedure","title":"Procedure","text":"<p>Run dplsh to extract the current and latest version for all support workloads</p> <pre><code># First authenticate against the cluster\ntask cluster:auth\n# Then pull the status\ntask ops:get-versions\n</code></pre> <p>Then access the version status sheet and update the status from the output.</p>"},{"location":"DPL-Platform/runbooks/upgrading-aks/","title":"Upgrading AKS","text":""},{"location":"DPL-Platform/runbooks/upgrading-aks/#when-to-use","title":"When to use","text":"<p>When you want to upgrade Azure Kubernetes Service to a newer version.</p>"},{"location":"DPL-Platform/runbooks/upgrading-aks/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running dplsh launched from <code>./infrastructure</code> with   <code>DPLPLAT_ENV</code> set to the platform environment name.</li> <li>Knowledge about the version of AKS you wish to upgrade to.</li> <li>Consult AKS Kubernetes Release Calendar     for a list of the various versions and when they are End of Life</li> </ul>"},{"location":"DPL-Platform/runbooks/upgrading-aks/#references","title":"References","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/aks/upgrade-cluster</li> <li>https://learn.microsoft.com/en-us/azure/aks/use-multiple-node-pools#upgrade-a-node-pool</li> <li>https://learn.microsoft.com/en-us/azure/aks/supported-kubernetes-versions</li> </ul>"},{"location":"DPL-Platform/runbooks/upgrading-aks/#procedure","title":"Procedure","text":"<p>Read the changelog of the desired new version. Pay attention to any breaking changes! We use Terraform to upgrade AKS. Should you need to do a manual upgrade consult Azures documentation on upgrading a cluster and on upgrading node pools. Be aware in both cases that the Terraform state needs to be brought into sync via some means, so this is not a recommended approach.</p>"},{"location":"DPL-Platform/runbooks/upgrading-aks/#find-out-which-versions-of-kubernetes-an-environment-can-upgrade-to","title":"Find out which versions of kubernetes an environment can upgrade to","text":"<p>In order to find out which versions of kubernetes we can upgrade to, we need to use the following command:</p> <pre><code>task cluster:get-upgrades\n</code></pre> <p>This will output a table of in which the column \"Upgrades\" lists the available upgrades for the highest available minor versions.</p> <p>A Kubernetes cluster can can at most be upgraded to the nearest minor version, which means you may be in a situation where you have several versions between you and the intended version.</p> <p>Minor versions can be skipped, and AKS will accept a cluster being upgraded to a version that does not specify a patch version. So if you for instance want to go from <code>1.20.9</code> to <code>1.22.15</code>, you can do <code>1.21</code>, and then <code>1.22.15</code>. When upgrading to <code>1.21</code> Azure will substitute the version for an the hightest available patch version, e.g. <code>1.21.14</code>.</p> <p>You should know know which version(s) you need to upgrade to, and can continue to the actual upgrade.</p>"},{"location":"DPL-Platform/runbooks/upgrading-aks/#ensuring-the-terraform-state-is-in-sync","title":"Ensuring the Terraform state is in sync","text":"<p>As we will be using Terraform to perform the upgrade we want to make sure it its state is in sync. Execute the following task and resolve any drift:</p> <pre><code>task infra:provision\n</code></pre>"},{"location":"DPL-Platform/runbooks/upgrading-aks/#upgrade-the-cluster","title":"Upgrade the cluster","text":"<p>Initiate a cluster upgrade. This will upgrade the control plane and node pools together. See the AKS documentation for background info on this operation.</p> <ol> <li> <p>Update the <code>control_plane_version</code> reference in <code>infrastructure/environments/&lt;environment&gt;/infrastructure/main.tf</code>   and run <code>task infra:provision</code> to apply. You can skip patch-versions, but you   can only do one minor-version at the time</p> </li> <li> <p>Monitor the upgrade as it progresses. The control-plane upgrade is usually    performed in under 5 minutes. Monitor via eg. <code>watch -n 5 kubectl version</code>.</p> </li> <li> <p>AKS will then automatically upgrade the system, admin and application    node-pools.</p> </li> <li> <p>Monitor the upgrade as it progresses. Expect the provisioning of and workload    scheduling to a single node to take about 5-10 minutes. In particular be    aware that the admin node-pool where harbor runs has a tendency to take a    long time as the harbor pvcs are slow to migrate to the new node.</p> <p>Monitor via eg.</p> <pre><code>watch -n 5 kubectl get nodes\n</code></pre> </li> <li> <p>Go to <code>dplsh's</code> Dockerfile and update the <code>KUBECTL_VERSION</code> version to     match that of the upgraded AKS version</p> </li> </ol>"},{"location":"DPL-Platform/runbooks/upgrading-lagoon/","title":"Upgrading Lagoon","text":""},{"location":"DPL-Platform/runbooks/upgrading-lagoon/#when-to-use","title":"When to use","text":"<p>When there is a need to upgrade Lagoon to a new patch or minor version.</p>"},{"location":"DPL-Platform/runbooks/upgrading-lagoon/#references","title":"References","text":"<ul> <li>Official Updating documentation.</li> <li>Lagoon Releases</li> </ul>"},{"location":"DPL-Platform/runbooks/upgrading-lagoon/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running dplsh launched from <code>./infrastructure</code> with   <code>DPLPLAT_ENV</code> set to the platform environment name.</li> <li>Knowledge about the version of Lagoon you want to upgrade to.</li> <li>You can extract version (= chart version) and appVersion (= lagoon release     version) for the lagoon-remote / lagoon-core charts via the following commands     (replace lagoon-core for lagoon-remote if necessary).</li> </ul> <p>Lagoon-core:</p> <pre><code>curl -s https://uselagoon.github.io/lagoon-charts/index.yaml \\\n  | yq '.entries.lagoon-core[] | [.name, .appVersion, .version, .created] | @tsv'\n</code></pre> <p>Lagoon-remote:</p> <pre><code>curl -s https://uselagoon.github.io/lagoon-charts/index.yaml \\\n  | yq '.entries.lagoon-remote[] | [.name, .appVersion, .version, .created] | @tsv'\n</code></pre> <ul> <li>Knowledge of any breaking changes or necessary actions that may affect the   platform when upgrading. See chart release notes for all intermediate chart   releases.</li> </ul>"},{"location":"DPL-Platform/runbooks/upgrading-lagoon/#procedure","title":"Procedure","text":"<ol> <li>Upgrade Lagoon core<ol> <li>Backup the API and Keycloak dbs as described in the official documentation</li> <li>Bump the chart version <code>VERSION_LAGOON_CORE</code> in    <code>infrastructure/environments/&lt;env&gt;/lagoon/lagoon-versions.env</code></li> <li>Perform a helm diff<ul> <li><code>DIFF=1 task lagoon:provision:core</code></li> </ul> </li> <li>Perform the actual upgrade<ul> <li><code>task lagoon:provision:core</code></li> </ul> </li> </ol> </li> <li>Upgrade Lagoon remote<ol> <li>Bump the chart version <code>VERSION_LAGOON_REMOTE</code> in   <code>infrastructure/environments/&lt;env&gt;/lagoon/lagoon-versions.env</code></li> <li>Perform a helm diff<ul> <li><code>DIFF=1 task lagoon:provision:remote</code></li> </ul> </li> <li>Perform the actual upgrade<ul> <li><code>task lagoon:provision:remote</code></li> </ul> </li> <li>Take note in the output from Helm of any CRD updates that may be required</li> </ol> </li> </ol>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/","title":"Upgrading Support Workloads","text":""},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#when-to-use","title":"When to use","text":"<p>When you want to upgrade support workloads in the cluster. This includes.</p> <ul> <li>Cert-manager</li> <li>Grafana</li> <li>Harbor</li> <li>Ingress Nginx</li> <li>K8up</li> <li>Loki</li> <li>Minio</li> <li>Prometheus</li> <li>Promtail</li> </ul> <p>This document contains general instructions for how to upgrade support workloads, followed by specific instructions for each workload (linked above).</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>dplsh</code> instance authorized against the cluster. See Using the DPL Shell.</li> </ul>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#general-procedure","title":"General Procedure","text":"<ol> <li> <p>Identify the version you want to bump in the <code>environment/configuration</code> directory    eg. for dplplat01 infrastructure/environments/dplplat01/configuration/versions.env.    The file contains links to the relevant Artifact Hub pages for the individual    projects and can often be used to determine both the latest version, but also    details about the chart such as how a specific manifest is used.    You can find the latest version of the support workload in the    Version status    sheet which itself is updated via the procedure described in the    Update Upgrade status runbook.</p> </li> <li> <p>Consult any relevant changelog to determine if the upgrade will require any    extra work beside the upgrade itself.    To determine which version to look up in the changelog, be aware of the difference    between the chart version    and the app version.    We currently track the chart versions, and not the actual version of the    application inside the chart. In order to determine the change in <code>appVersion</code>    between chart releases you can do a diff between releases, and keep track of the    <code>appVersion</code> property in the charts <code>Chart.yaml</code>. Using using grafana as an example:    https://github.com/grafana/helm-charts/compare/grafana-6.55.1...grafana-6.56.0.    The exact way to do this differs from chart to chart, and is documented in the    Specific producedures and tests below.</p> </li> <li> <p>Carry out any chart-specific preparations described in the charts update-procedure.    This could be upgrading a Custom Resource Definition that the chart does not    upgrade.</p> </li> <li> <p>Identify the relevant task in the main Taskfile    for upgrading the workload. For example, for cert-manager, the task is called    <code>support:provision:cert-manager</code> and run the task with <code>DIFF=1</code>, eg    <code>DIFF=1 task support:provision:cert-manager</code>.</p> </li> <li> <p>If the diff looks good, run the task without <code>DIFF=1</code>, eg <code>task support:provision:cert-manager</code>.</p> </li> <li> <p>Then proceeded to perform the verification test for the relevant workload. See    the following section for known verification tests.</p> </li> <li> <p>Finally, it is important to verify that Lagoon deployments still work. Some    breaking changes will not manifest themselves until an environment is    rebuilt, at which point it may subsequently fail. An example is the    disabling of user snippets in the ingress-nginx controller    v1.9.0. To verify    deployments still work, log in to the Lagoon UI and select an environment to    redeploy.</p> </li> </ol>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#specific-producedures-and-tests","title":"Specific producedures and tests","text":"<ul> <li>Cert-manager</li> <li>Grafana</li> <li>Harbor</li> <li>Ingress Nginx</li> <li>K8up</li> <li>Loki</li> <li>Minio</li> <li>Prometheus</li> <li>Promtail</li> </ul>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#cert-manager","title":"Cert Manager","text":""},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#comparing-cert-manager-versions","title":"Comparing cert-manager versions","text":"<p>The project project versions its Helm chart together with the app itself. So, simply use the chart version in the following checks.</p> <p>Cert Manager keeps Release notes for the individual minor releases of the project. Consult these for every upgrade past a minor version.</p> <p>As both are versioned in the same repository, simply use the following link for looking up the release notes for a specific patch release, replacing the example tag with the version you wish to upgrade to.</p> <p>https://github.com/cert-manager/cert-manager/releases/tag/v1.11.2</p> <p>To compare two reversions, do the same using the following link:</p> <p>https://github.com/cert-manager/cert-manager/compare/v1.11.1...v1.11.2</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#upgrade-cert-manager","title":"Upgrade cert-manager","text":"<p>Commands</p> <pre><code># Diff\nDIFF=1 task support:provision:cert-manager\n\n# Upgrade\ntask support:provision:cert-manager\n</code></pre>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#verify-cert-manager-upgrade","title":"Verify cert-manager upgrade","text":"<p>Verify that cert-manager itself and webhook pods are all running and healthy.</p> <pre><code>task support:verify:cert-manager\n</code></pre>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#grafana","title":"Grafana","text":""},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#comparing-grafana-versions","title":"Comparing Grafana versions","text":"<p>Insert the chart version in the following link to see the release note.</p> <p>https://github.com/grafana/helm-charts/releases/tag/grafana-6.52.9</p> <p>The note will most likely be empty. Now diff the chart version with the current version, again replacing the version with the relevant for your releases.</p> <p>https://github.com/grafana/helm-charts/compare/grafana-6.43.3...grafana-6.52.9</p> <p>As the repository contains a lot of charts, you will need to do a bit of digging. Look for at least <code>charts/grafana/Chart.yaml</code> which can tell you the app version.</p> <p>With the app-version in hand, you can now look at the release notes for the grafana app itself.</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#upgrade-grafana","title":"Upgrade grafana","text":"<p>Diff command</p> <pre><code>DIFF=1 task support:provision:grafana\n</code></pre> <p>Upgrade command</p> <pre><code>task support:provision:grafana\n</code></pre>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#verify-grafana-upgrade","title":"Verify grafana upgrade","text":"<p>Verify that the Grafana pods are all running and healthy.</p> <pre><code>kubectl get pods --namespace grafana\n</code></pre> <p>Access the Grafana UI and see if you can log in. If you do not have a user but have access to read secrets in the <code>grafana</code> namespace, you can retrive the admin password with the following command:</p> <pre><code># Password for admin\nUI_NAME=grafana task ui-password\n\n# Hostname for grafana\nkubectl -n grafana get -o jsonpath=\"{.spec.rules[0].host}\" ingress grafana ; echo\n</code></pre>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#harbor","title":"Harbor","text":""},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#comparing-harbor-versions","title":"Comparing Harbor versions","text":"<p>Harbor has different app and chart versions.</p> <p>An overview of the chart versions can be retrived from Github. the chart does not have a changelog.</p> <p>Link for comparing two chart releases: https://github.com/goharbor/harbor-helm/compare/v1.10.1...v1.12.0</p> <p>Having identified the relevant appVersions, consult the list of Harbor releases to see a description of the changes included in the release in question. If this approach fails you can also use the diff-command described below to determine which image-tags are going to change and thus determine the version delta.</p> <p>Harbor is a quite active project, so it may make sense mostly to pay attention to minor/major releases and ignore the changes made in patch-releases.</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#upgrade-harbor","title":"Upgrade Harbor","text":"<p>Harbor documents the general upgrade procedure for non-kubernetes upgrades for minor versions on their website. This documentation is of little use to our Kubernetes setup, but it can be useful to consult the page for minor/major version upgrades to see if there are any special considerations to be made.</p> <p>The Harbor chart repository has upgrade instructions as well. The instructions asks you to do a snapshot of the database and backup the tls secret. Snapshotting the database is currently out of scope, but could be a thing that is considered in the future. The tls secret is handled by cert-manager, and as such does not need to be backed up.</p> <p>With knowledge of the app version, you can now update <code>versions.env</code> as described in the General Procedure section, diff to see the changes that are going to be applied, and finally do the actual upgrade.</p> <p>Diff command</p> <pre><code>DIFF=1 task support:provision:harbor\n</code></pre> <p>Upgrade command</p> <pre><code>task support:provision:harbor\n</code></pre>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#verify-harbor-upgrade","title":"Verify Harbor upgrade","text":"<p>First verify that pods are coming up</p> <pre><code>kubectl -n harbor get pods\n</code></pre> <p>When Harbor seems to be working, you can verify that the UI is working by accessing https://harbor.lagoon.dplplat01.dpl.reload.dk/. The password for the user <code>admin</code> can be retrived with the following command:</p> <pre><code>UI_NAME=harbor task ui-password\n</code></pre> <p>If everything looks good, you can consider to deploying a site. One way to do this is to identify an existing site of low importance, and re-deploy it. A re-deploy will require Lagoon to both fetch and push images. Instructions for how to access the lagoon UI is out of scope of this document, but can be found in the runbook for running a lagoon task. In this case you are looking for the \"Deploy\" button on the sites \"Deployments\" tab.</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#ingress-nginx","title":"Ingress-nginx","text":""},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#comparing-ingress-nginx-versions","title":"Comparing ingress-nginx versions","text":"<p>When working with the <code>ingress-nginx</code> chart we have at least 3 versions to keep track off.</p> <p>The chart version tracks the version of the chart itself. The charts <code>appVersion</code> tracks a <code>controller</code> application which dynamically configures a bundles <code>nginx</code>. The version of <code>nginx</code> used is determined configuration-files in the controller. Amongst others the ingress-nginx.yaml.</p> <p>Link for diffing two chart versions: https://github.com/kubernetes/ingress-nginx/compare/helm-chart-4.6.0...helm-chart-4.6.1</p> <p>The project keeps a quite good changelog for the chart</p> <p>Link for diffing two controller versions: https://github.com/kubernetes/ingress-nginx/compare/controller-v1.7.1...controller-v1.7.0</p> <p>Consult the individual GitHub releases for descriptions of what has changed in the controller for a given release.</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#upgrade-ingress-nginx","title":"Upgrade ingress-nginx","text":"<p>With knowledge of the app version, you can now update <code>versions.env</code> as described in the General Procedure section, diff to see the changes that are going to be applied, and finally do the actual upgrade.</p> <p>Diff command</p> <pre><code>DIFF=1 task support:provision:ingress-nginx\n</code></pre> <p>Upgrade command</p> <pre><code>task support:provision:ingress-nginx\n</code></pre>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#verify-ingress-nginx-upgrade","title":"Verify ingress-nginx upgrade","text":"<p>The ingress-controller is very central to the operation of all public accessible parts of the platform. It's area of resposibillity is on the other hand quite narrow, so it is easy to verify that it is working as expected.</p> <p>First verify that pods are coming up</p> <pre><code>kubectl -n ingress-nginx get pods\n</code></pre> <p>Then verify that the ingress-controller is able to serve traffic. This can be done by accessing the UI of one of the apps that are deployed in the platform.</p> <p>Access eg. https://ui.lagoon.dplplat01.dpl.reload.dk/.</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#k8up","title":"K8up","text":"<p>We can currently not upgrade to version 2.x of K8up as Lagoon is not yet ready</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#loki","title":"Loki","text":""},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#comparing-loki-versions","title":"Comparing Loki versions","text":"<p>The Loki chart is versioned separatly from Loki. The version of Loki installed by the chart is tracked by its <code>appVersion</code>. So when upgrading, you should always look at the diff between both the chart and app version.</p> <p>The general upgrade procedure will give you the chart version, access the following link to get the release note for the chart. Remember to insert your version:</p> <p>https://github.com/grafana/loki/releases/tag/helm-loki-5.5.1</p> <p>Notice that the Loki helm-chart is maintained in the same repository as Loki itself. You can find the diff between the chart versions by comparing two chart release tags.</p> <p>https://github.com/grafana/loki/compare/helm-loki-5.5.0...helm-loki-5.5.1</p> <p>As the repository contains changes to Loki itself as well, you should seek out the file <code>production/helm/loki/Chart.yaml</code> which contains the <code>appVersion</code> that defines which version of Loki a given chart release installes.</p> <p>Direct link to the file for a specific tag: https://github.com/grafana/loki/blob/helm-loki-3.3.1/production/helm/loki/Chart.yaml</p> <p>With the app-version in hand, you can now look at the release notes for Loki to see what has changed between the two appVersions.</p> <p>Last but not least the Loki project maintains a upgrading guide that can be found here: https://grafana.com/docs/loki/latest/upgrading/</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#upgrade-loki","title":"Upgrade Loki","text":"<p>Diff command</p> <pre><code>DIFF=1 task support:provision:loki\n</code></pre> <p>Upgrade command</p> <pre><code>task support:provision:loki\n</code></pre>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#verify-loki-upgrade","title":"Verify Loki upgrade","text":"<p>List pods in the <code>loki</code> namespace to see if the upgrade has completed successfully.</p> <pre><code>  kubectl --namespace loki get pods\n</code></pre> <p>Next verify that Loki is still accessibel from Grafana and collects logs by logging in to Grafana. Then verify the Loki datasource, and search out some logs for a site. See the validation steps for Grafana for instructions on how to access the Grafana UI.</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#minio","title":"MinIO","text":"<p>We can currently not upgrade MinIO without loosing the Azure blob gateway. see:</p> <ul> <li>https://blog.min.io/deprecation-of-the-minio-gateway/</li> <li>https://github.com/minio/minio/issues/14331</li> <li>https://github.com/bitnami/charts/issues/10258#issuecomment-1132929451</li> </ul>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#prometheus","title":"Prometheus","text":""},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#comparing-prometheus-versions","title":"Comparing Prometheus versions","text":"<p>The <code>kube-prometheus-stack</code> helm chart is quite well maintained and is versioned and developed separately from the application itself.</p> <p>A specific release of the chart can be accessed via the following link:</p> <p>https://github.com/prometheus-community/helm-charts/releases/tag/kube-prometheus-stack-45.27.2</p> <p>The chart is developed alongside a number of other community driven prometheus- related charts in https://github.com/prometheus-community/helm-charts.</p> <p>This means that the following comparison between two releases of the chart will also contain changes to a number of other charts. You will have to look for changes in the <code>charts/kube-prometheus-stack/</code> directory.</p> <p>https://github.com/prometheus-community/helm-charts/compare/kube-prometheus-stack-45.26.0...kube-prometheus-stack-45.27.2</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#upgrade-prometheus","title":"Upgrade Prometheus","text":"<p>The Readme for the chart contains a good Upgrading Chart section that describes things to be aware of when upgrading between specific minor and major versions. The same documentation can also be found on artifact hub.</p> <p>Consult the section that matches the version you are upgrading from and to. Be aware that upgrades past a minor version often requires a CRD update. The CRDs may have to be applied before you can do the diff and upgrade. Once the CRDs has been applied you are committed to the upgrade as there is no simple way to downgrade the CRDs.</p> <p>Diff command</p> <pre><code>DIFF=1 task support:provision:prometheus\n</code></pre> <p>Upgrade command</p> <pre><code>task support:provision:prometheus\n</code></pre>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#verify-prometheus-upgrade","title":"Verify Prometheus upgrade","text":"<p>List pods in the <code>prometheus</code> namespace to see if the upgrade has completed successfully. You should expect to see two types of workloads. First a single a single <code>promstack-kube-prometheus-operator</code> pod that runs Prometheus, and then a <code>promstack-prometheus-node-exporter</code> pod for each node in the cluster.</p> <pre><code>  kubectl --namespace prometheus get pods -l \"release=promstack\"\n</code></pre> <p>As the Prometheus UI is not directly exposed, the easiest way to verify that Prometheus is running is to access the Grafana UI and verify that the dashboards that uses Prometheus are working, or as a minimum that the prometheus datasource passes validation. See the validation steps for Grafana for instructions on how to access the Grafana UI.</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#promtail","title":"Promtail","text":""},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#comparing-promtail-versions","title":"Comparing Promtail versions","text":"<p>The Promtail chart is versioned separatly from Promtail which itself is a part of Loki. The version of Promtail installed by the chart is tracked by its appVersion. So when upgrading, you should always look at the diff between both the chart and app version.</p> <p>The general upgrade procedure will give you the chart version, access the following link to get the release note for the chart. Remember to insert your version:</p> <p>https://github.com/grafana/helm-charts/releases/tag/promtail-6.6.0</p> <p>The note will most likely be empty. Now diff the chart version with the current version, again replacing the version with the relevant for your releases.</p> <p>https://github.com/grafana/helm-charts/compare/promtail-6.6.0...promtail-6.6.1</p> <p>As the repository contains a lot of charts, you will need to do a bit of digging. Look for at least <code>charts/promtail/Chart.yaml</code> which can tell you the app version.</p> <p>With the app-version in hand, you can now look at the release notes for Loki (which promtail is part of). Look for notes in the Promtail sections of the release notes.</p>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#upgrade-promtail","title":"Upgrade Promtail","text":"<p>Diff command</p> <pre><code>DIFF=1 task support:provision:promtail\n</code></pre> <p>Upgrade command</p> <pre><code>task support:provision:promtail\n</code></pre>"},{"location":"DPL-Platform/runbooks/upgrading-support-workloads/#verify-promtail-upgrade","title":"Verify Promtail upgrade","text":"<p>List pods in the <code>promtail</code> namespace to see if the upgrade has completed successfully.</p> <pre><code>  kubectl --namespace promtail get pods\n</code></pre> <p>With the pods running, you can verify that the logs are being collected seeking out logs via Grafana. See the validation steps for Grafana for details on how to access the Grafana UI.</p> <p>You can also inspect the logs of the individual pods via</p> <pre><code>kubectl --namespace promtail logs -l \"app.kubernetes.io/name=promtail\"\n</code></pre> <p>And verify that there are no obvious error messages.</p>"},{"location":"DPL-Platform/runbooks/using-dplsh/","title":"Using the DPL Shell","text":"<p>The Danish Public Libraries Shell (dplsh) is a container-based shell used by platform-operators for all cli operations.</p>"},{"location":"DPL-Platform/runbooks/using-dplsh/#when-to-use","title":"When to use","text":"<p>Whenever you perform any administrative actions on the platform. If you want to know more about the shell itself? Refer to tools/dplsh.</p>"},{"location":"DPL-Platform/runbooks/using-dplsh/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>jq</li> <li>Bash 4 or newer</li> <li>An authorized Azure <code>az</code> cli. The version should match the version found in   <code>FROM mcr.microsoft.com/azure-cli:version</code> in the dplsh Dockerfile   You can choose to authorize the az cli from within dplsh, but your session   will only last as long as the shell-session. The use you authorize as must   have permission to read the Terraform state from the Terraform setup   , and Contributor permissions on the environments   resource-group in order to provision infrastructure.</li> <li><code>dplsh.sh</code> symlinked into your path as <code>dplsh</code>, see Launching the Shell  (optional, but assumed below)</li> </ul>"},{"location":"DPL-Platform/runbooks/using-dplsh/#procedure","title":"Procedure","text":"<pre><code># Launch dplsh.\n$ cd infrastructure\n$ dplsh\n\n# 1. Set an environment,\n# export DPLPLAT_ENV=&lt;platform environment name&gt;\n# eg.\n$ export DPLPLAT_ENV=dplplat01\n\n# 2a. Authenticate against AKS, needed by infrastructure and Lagoon tasks\n$ task cluster:auth\n\n# 2b - if you want to use the Lagoon CLI)\n# The Lagoon CLI is authenticated via ssh-keys. DPLSH will mount your .ssh\n# folder from your homedir, but if your keys are passphrase protected, we need\n# to unlock them.\n$ eval $(ssh-agent); ssh-add\n# Then authorize the lagoon cli\n$ task lagoon:cli:config\n</code></pre>"},{"location":"DPL-Platform/runbooks/weekly-release/","title":"Deploy new version to editors and webmaster moduletests","text":""},{"location":"DPL-Platform/runbooks/weekly-release/#when-to-use","title":"When to use","text":"<p>When doing a regular release, where editors and webmaster moduletest sites are updated to latest release, and webmaster production is updated to second latest release.</p>"},{"location":"DPL-Platform/runbooks/weekly-release/#prerequisites","title":"Prerequisites","text":"<ul> <li>A local checkout of the <code>dpl-platform</code> repository</li> <li>A running dplsh with <code>DPLPLAT_ENV</code> set to the platform   environment name.</li> <li>The version tag you want to deploy. This must correspond to a   tagged version of the <code>dpl-cms-source</code> image.</li> <li>The second latest version</li> </ul>"},{"location":"DPL-Platform/runbooks/weekly-release/#procedure-make-the-release-pull-request","title":"Procedure: Make the release Pull Request","text":"<ol> <li>In your local environment ensure that your checkout of the <code>main</code>    branch for <code>dpl-platform</code> is up to date, by doing a <code>git pull origin main</code>.</li> <li>Create a new branch from <code>main</code>.</li> <li>Now update <code>infrastructure/environments/dplplat01/sites.yaml</code>. The     <code>x-defaults</code> anchors' <code>dpl-cms-release</code> tag should be bumped to     the latest version. Then update the <code>moduletest-dpl-cms-release</code> of     <code>x-webmasters-on-weekly-release-cycle</code> to the same version. Now update     <code>dpl-cms-release</code> of <code>x-webmasters-on-weekly-release-cycle</code> to the second     latest release. Lastly update cms-school's, the canary sites' and     bibliotek-test's to the lastest release for both moduletest and production.</li> <li>Commit the change and push your branch to GitHub and create a pull    request.</li> <li>Request a review for the change and wait for approval.</li> </ol>"},{"location":"DPL-Platform/runbooks/weekly-release/#procedure-release-the-approved-release","title":"Procedure: Release the approved release","text":"<ol> <li>Merge the approved Pull Request and pull your local main branch to match.</li> <li>Start <code>dplsh</code> from the <code>/infrastructure</code> directory of your local    environment by running <code>../tools/dplsh/dplsh.sh</code></li> <li>Run <code>task sites:sync</code> from <code>dplsh</code> to deploy the changes.</li> <li>If there are any Terraform changes then do not apply them, abort    the deployment and consult the platform team.</li> <li>Open the Deployments list page within the Lagoon UI to see all    running and queued deployments.</li> <li>Wait for all the deployment to complete.</li> <li>Run <code>sites:redeploy-failed-deployments</code> to identify and redeploy any failed     deployments.</li> <li>If some deployments did not complete determine if the error     relates to the platform or the application.</li> <li>For all platform-related errors try to redeploy the environment     from the Lagoon UI.</li> <li>Run <code>task cluster:promote-workloads-to-prod</code> from <code>dplsh</code>.</li> <li>The moduletest sites should now be reset so their state matches their     production counter parts. This is done by running the synchonization tasks     in the Lagoon UI for each moduletest site.</li> </ol>"},{"location":"DPL-Platform/runbooks/weekly-release/#procedure-a-some-sites-fails-to-deploy","title":"Procedure: a some sites fails to deploy","text":"<p>We have experience this quite a lot. We have gathered a list of known issues and how to solve them a troubleshoot runbook</p>"},{"location":"DPL-React/","title":"DPL React","text":"<p>A set of React components and applications providing self-service features for Danish public libraries.</p>"},{"location":"DPL-React/#development","title":"Development","text":""},{"location":"DPL-React/#requirements","title":"Requirements","text":"<ul> <li>go-task</li> <li>nvm</li> <li>yarn 1.x</li> <li>Docker</li> <li>Dory</li> </ul> <p>Before you can install the project you need to create the file <code>~/.npmrc</code> to access the GitHub package registry as described using a personal access token. The token must be created with the required scopes: <code>repo</code> and <code>read:packages</code></p> <p>If you have npm installed locally this can be achieved by running the following command and using the token when prompted for password.</p> <pre><code>npm login --registry=https://npm.pkg.github.com\n</code></pre>"},{"location":"DPL-React/#howto","title":"Howto","text":"<ol> <li>Ensure that your Node version matches what is specified in <code>.nvmrc</code>.</li> <li>Run <code>task dev:start</code></li> <li>Storybook will open automatically in a browser</li> <li>The console will contain build and lint information</li> <li>If you need to log in through Adgangsplatformen:</li> <li>Add <code>127.0.0.1 dpl-react.docker</code> to your <code>/etc/hosts</code> file</li> <li>Ensure that Node can bind to port 80</li> <li>Use http://dpl-react.docker/ instead of e.g. http://localhost:8080</li> <li>If you want to use Wiremock instead of production systems run    <code>task dev:mocks:start</code></li> </ol>"},{"location":"DPL-React/#step-debugging-in-visual-studio-code-no-docker","title":"Step Debugging in Visual Studio Code (no docker)","text":"<p>If you want to enable step debugging you need to:</p> <ul> <li>Copy .vscode.example/launch.json into .vscode/</li> <li>Mark 1 or more breakpoints on a line in the left gutter on an open file</li> <li>In the top menu in VS Code choose: Run -&gt; Start Debugging</li> <li>Type in your user password if ask to</li> <li>Start debugging \ud83e\udd16\u223f\ud83d\udcbb</li> </ul>"},{"location":"DPL-React/#access-tokens","title":"Access tokens","text":"<p>Access token must be retrieved from Adgangsplatformen, a single sign-on solution for public libraries in Denmark, and OpenPlatform, an API for danish libraries.</p> <p>Usage of these systems require a valid client id and secret which must be obtained from your library partner or directly from DBC, the company responsible for running Adgangsplatformen and OpenPlatform.</p> <p>This project include a client id that matches the storybook setup which can be used for development purposes. You can use the <code>/auth</code> story to sign in to Adgangsplatformen for the storybook context.</p> <p>(Note: if you enter Adgangsplatformen again after signing it, you will get signed out, and need to log in again. This is not a bug, as you stay logged in otherwise.)</p>"},{"location":"DPL-React/#library-token","title":"Library token","text":"<p>To test the apps that is indifferent to wether the user is authenticated or not it is possible to set a library token via the library component in Storybook. Workflow:</p> <ul> <li>Retrieve a library token via OpenPlatform</li> <li>Insert the library token in the Library Token story in storybook</li> </ul>"},{"location":"DPL-React/#standard-and-style","title":"Standard and style","text":""},{"location":"DPL-React/#javascript-jsx","title":"JavaScript + JSX","text":"<p>For static code analysis, we use the recommended ESLint rules included in the ESLint package, and for formatting we make use of Prettier with the default configuration. The above choices have been influenced by a multitude of factors:</p> <ul> <li>Historically Drupal core have been making use of the Airbnb JavaScript Style Guide.</li> <li>We previously used Airbnb for linting. This change is documented in   ADR-009: Remove Airbnb Lint Rules.</li> </ul> <p>This makes future adoption easier for onboarding contributors and support is to be expected for a long time.</p>"},{"location":"DPL-React/#create-a-new-application","title":"Create a new application","text":"1. Create a new application component <pre><code>// ./src/apps/my-new-application/my-new-application.jsx\nimport React from \"react\";\nimport PropTypes from \"prop-types\";\n\nexport function MyNewApplication({ text }) {\n  return &lt;h2&gt;{text}&lt;/h2&gt;;\n}\n\nMyNewApplication.defaultProps = {\n  text: \"The fastest man alive!\"\n};\n\nMyNewApplication.propTypes = {\n  text: PropTypes.string\n};\n\nexport default MyNewApplication;\n</code></pre> 2. Create the entry component <pre><code>// ./src/apps/my-new-application/my-new-application.entry.jsx\nimport React from \"react\";\nimport PropTypes from \"prop-types\";\nimport MyNewApplication from \"./my-new-application\";\n\n// The props of an entry is all of the data attributes that were\n// set on the DOM element. See the section on \"Naive app mount.\" for\n// an example.\nexport function MyNewApplicationEntry(props) {\n  return &lt;MyNewApplication text=\"Might be from a server?\" /&gt;;\n}\n\nexport default MyNewApplicationEntry;\n</code></pre> 3. Create the mount <pre><code>// ./src/apps/my-new-application/my-new-application.mount.js\nimport addMount from \"../../core/addMount\";\nimport MyNewApplication from \"./my-new-application.entry\";\n\naddMount({ appName: \"my-new-application\", app: MyNewApplication });\n</code></pre> 4. Add a story for local development <pre><code>// ./src/apps/my-new-application/my-new-application.dev.jsx\nimport React from \"react\";\nimport MyNewApplicationEntry from \"./my-new-application.entry\";\nimport MyNewApplication from \"./my-new-application\";\n\nexport default { title: \"Apps|My new application\" };\n\nexport function Entry() {\n  // Testing the version that will be shipped.\n  return &lt;MyNewApplicationEntry /&gt;;\n}\n\nexport function WithoutData() {\n  // Play around with the application itself without server side data.\n  return &lt;MyNewApplication /&gt;;\n}\n</code></pre> 5. Run the development environment <pre><code>  yarn dev\n</code></pre>  OR depending on your dev environment (docker or not)  <pre><code>  sudo yarn dev\n</code></pre> <p>Voila! You browser should have opened and a storybook environment is ready for you to tinker around.</p>"},{"location":"DPL-React/#application-state-machine","title":"Application state-machine","text":"<p>Most applications will have multiple internal states, so to aid consistency, it's recommended to:</p> <pre><code>const [status, setStatus] = useState(\"&lt;initial state&gt;\");\n</code></pre> <p>and use the following states where appropriate:</p> <p><code>initial</code>: Initial state for applications that require some sort of initialization, such as making a request to see if a material can be ordered, before rendering the order button. Errors in initialization can go directly to the failed state, or add custom states for communication different error conditions to the user. Should render either nothing or as a skeleton/spinner/message.</p> <p><code>ready</code>: The general \"ready state\". Applications that doesn't need initialization (a generic button for instance) can use <code>ready</code> as the initial state set in the <code>useState</code> call. This is basically the main waiting state.</p> <p><code>processing</code>: The application is taking some action. For buttons this will be the state used when the user has clicked the button and the application is waiting for reply from the back end. More advanced applications may use it while doing backend requests, if reflecting the processing in the UI is desired. Applications using optimistic feedback will render this state the same as the <code>finished</code> state.</p> <p><code>failed</code>: Processing failed. The application renders an error message.</p> <p><code>finished</code>: End state for one-shot actions. Communicates success to the user.</p> <p>Applications can use additional states if desired, but prefer the above if appropriate.</p>"},{"location":"DPL-React/#style-your-application","title":"Style your application","text":"1. Create an application specific stylesheet <pre><code>// ./src/apps/my-new-application/my-new-application.scss\n.dpl-warm {\n  color: maroon;\n}\n</code></pre> 2. Add the class to your application <pre><code>// ./src/apps/my-new-application/my-new-application.jsx\nimport React from \"react\";\nimport PropTypes from \"prop-types\";\n\nexport function MyNewApplication({ text }) {\n  return &lt;h2 className=\"warm\"&gt;{text}&lt;/h2&gt;;\n}\n\nMyNewApplication.defaultProps = {\n  text: \"The fastest man alive!\"\n};\n\nMyNewApplication.propTypes = {\n  text: PropTypes.string\n};\n\nexport default MyNewApplication;\n</code></pre> 3. Import the scss into your story <pre><code>// ./src/apps/my-new-application/my-new-application.dev.jsx\nimport React from \"react\";\nimport MyNewApplicationEntry from \"./my-new-application.entry\";\nimport MyNewApplication from \"./my-new-application\";\n\nimport \"./my-new-application.scss\";\n\nexport default { title: \"Apps|My new application\" };\n\nexport function Entry() {\n  // Testing the version that will be shipped.\n  return &lt;MyNewApplicationEntry /&gt;;\n}\n\nexport function WithoutData() {\n  // Play around with the application itself without server side data.\n  return &lt;MyNewApplication /&gt;;\n}\n</code></pre> <p>Cowabunga! You now got styling in your application</p>"},{"location":"DPL-React/#style-using-the-dpl-design-system","title":"Style using the DPL design system","text":"<p>This project includes styling created by its sister repository - the design system as a npm package.</p> <p>By default the project should include a release of the design system matching the current state of the project.</p> <p>To update the design system to the latest stable release of the design system run:</p> <pre><code>yarn add @danskernesdigitalebibliotek/dpl-design-system@latest\n</code></pre> <p>This command installs the latest released version of the package. Whenever a new version of the design system package is released, it is necessary to reinstall the package in this project using the same command to get the newest styling, because yarn adds a specific version number to the package name in package.json.</p>"},{"location":"DPL-React/#using-unreleased-design","title":"Using unreleased design","text":"<p>If you need to work with published but unreleased code from a specific branch of the design system, you can also use the branch name as the tag for the npm package, replacing all special characters with dashes (<code>-</code>).</p> <p>Example: To use the latest styling from a branch in the design system called <code>feature/availability-label</code>, run:</p> <pre><code>yarn add @danskernesdigitalebibliotek/dpl-design-system@feature-availability-label\n</code></pre> <p>If the branch resides in a fork (usually before a pull request is merged) you can use aliasing and run:</p> <pre><code>yarn config set \"@my-fork:registry\" \"https://npm.pkg.github.com\"\nyarn add @danskernesdigitalebibliotek/dpl-design-system@npm:@my-fork/dpl-design-system@feature-availability-label\n</code></pre> <p>If the branch is updated and you want the latest changes to take effect locally update the release used:</p> <pre><code>yarn upgrade @danskernesdigitalebibliotek/dpl-design-system\n</code></pre> <p>Note that references to unreleased code should never make it into official versions of the project.</p>"},{"location":"DPL-React/#cross-application-components","title":"Cross application components","text":"<p>If the component is simple enough to be a primitive you would use in multiple occasions it's called an 'atom'. Such as a button or a link. If it's more specific that that and to be used across apps we just call it a component. An example would be some type of media presented alongside a header and some text.</p> <p>The process when creating an atom or a component is more or less similar, but some structural differences might be needed.</p>"},{"location":"DPL-React/#creating-an-atom","title":"Creating an atom","text":"1. Create the atom <pre><code>// ./src/components/atoms/my-new-atom/my-new-atom.jsx\nimport React from \"react\";\nimport PropTypes from \"prop-types\";\n\n/**\n * A simple button.\n *\n * @export\n * @param {object} props\n * @returns {ReactNode}\n */\nexport function MyNewAtom({ className, children }) {\n  return &lt;button className={`btn ${className}`}&gt;{children}&lt;/button&gt;;\n}\n\nMyNewAtom.propTypes = {\n  className: PropTypes.string,\n  children: PropTypes.node.isRequired\n};\n\nMyNewAtom.defaultProps = {\n  className: \"\"\n};\n\nexport default MyNewAtom;\n</code></pre> 2. Create styles for the atom <pre><code>// ./src/components/atoms/my-new-atom/my-new-atom.scss\n.dpl-btn {\n  color: blue;\n}\n</code></pre> 3. Import the atom's styles into the component stylesheet <pre><code>// ./src/components/components.scss\n@import \"atoms/button/button.scss\";\n@import \"atoms/my-new-atom/my-new-atom.scss\";\n</code></pre> 4. Create a story for your atom <pre><code>// ./src/components/atoms/my-new-atom/my-new-atom.dev.jsx\nimport React from \"react\";\nimport MyNewAtom from \"./my-new-atom\";\n\nexport default { title: \"Atoms|My new atom\" };\n\nexport function WithText() {\n  return &lt;MyNewAtom&gt;Cick me!&lt;/MyNewAtom&gt;;\n}\n</code></pre> 5. Import the atom into the applications or other components where you would want to use it <pre><code>// ./src/apps/my-new-application/my-new-application.jsx\nimport React, { Fragment } from \"react\";\nimport PropTypes from \"prop-types\";\n\nimport MyNewAtom from \"../../components/atom/my-new-atom/my-new-atom\";\n\nexport function MyNewApplication({ text }) {\n  return (\n    &lt;Fragment&gt;\n      &lt;h2 className=\"warm\"&gt;{text}&lt;/h2&gt;\n      &lt;MyNewAtom className=\"additional-class\" /&gt;\n    &lt;/Fragment&gt;\n  );\n}\n\nMyNewApplication.defaultProps = {\n  text: \"The fastest man alive!\"\n};\n\nMyNewApplication.propTypes = {\n  text: PropTypes.string\n};\n\nexport default MyNewApplication;\n</code></pre> <p>Finito! You now know how to share code across applications</p>"},{"location":"DPL-React/#creating-a-component","title":"Creating a component","text":"<p>Repeat all of the same steps as with an atom but place it in it's own directory inside <code>components</code>.</p> <p>Such as <code>./src/components/my-new-component/my-new-component.jsx</code></p>"},{"location":"DPL-React/#editor-example-configuration","title":"Editor example configuration","text":"<p>If you use Code we provide some easy to use and nice defaults for this project. They are located in <code>.vscode.example</code>. Simply rename the directory from <code>.vscode.example</code> to <code>.vscode</code> and you are good to go. This overwrites your global user settings for this workspace and suggests som extensions you might want.</p>"},{"location":"DPL-React/#usage","title":"Usage","text":"<p>There are two ways to use the components provided by this project:</p> <ol> <li>As standalone JavaScript applications mounted within HTML pages generated by    a separate system.</li> <li>As components within a larger JavaScript application (Under development)</li> </ol>"},{"location":"DPL-React/#naive-app-mount","title":"Naive app mount","text":"<p>So let's say you wanted to make use of an application within an existing HTML page such as what might be generated serverside by platforms like Drupal, WordPress etc.</p> <p>For this use case you should download the <code>dist.zip</code> package from the latest release of the project and unzip somewhere within the web root of your project. The package contains a set of artifacts needed to use one or more applications within an HTML page.</p> HTML Example  A simple example of the required artifacts and how they are used looks like this:  <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"UTF-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /&gt;\n    &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\" /&gt;\n    &lt;title&gt;Naive mount&lt;/title&gt;\n    &lt;!-- Include CSS files to provide default styling --&gt;\n    &lt;link rel=\"stylesheet\" href=\"/dist/components.css\" /&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;b&gt;Here be dragons!&lt;/b&gt;\n    &lt;!-- Data attributes will be camelCased on the react side aka.\n         props.errorText and props.text --&gt;\n    &lt;div\n      data-dpl-app=\"add-to-checklist\"\n      data-text=\"Chromatic dragon\"\n      data-error-text=\"Minor mistake\"\n    &gt;&lt;/div&gt;\n    &lt;div data-dpl-app=\"a-none-existing-app\"&gt;&lt;/div&gt;\n\n    &lt;!-- Load order og scripts is of importance here --&gt;\n    &lt;script src=\"/dist/runtime.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"/dist/bundle.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"/dist/mount.js\"&gt;&lt;/script&gt;\n    &lt;!-- After the necessary scripts you can start loading applications --&gt;\n    &lt;script src=\"/dist/add-to-checklist.js\"&gt;&lt;/script&gt;\n    &lt;script&gt;\n      // For making successful requests to the different services we need one or\n      // more valid tokens.\n      window.dplReact.setToken(\"user\", \"XXXXXXXXXXXXXXXXXXXXXX\");\n      window.dplReact.setToken(\"library\", \"YYYYYYYYYYYYYYYYYYYYYY\");\n\n      // If this function isn't called no apps will display.\n      // An app will only be displayed if there is a container for it\n      // and a corresponding application loaded.\n      window.dplReact.mount(document);\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>As a minimum you will need the <code>runtime.js</code> and <code>bundle.js</code>. For styling of atoms and components you will need to import <code>components.css</code>.</p> <p>Each application also has its own JavaScript artifact and it might have a CSS artifact as well. Such as <code>add-to-checklist.js</code> and <code>add-to-checklist.css</code>.</p> <p>To mount the application you need an HTML element with the correct data attribute.</p> <pre><code>&lt;div data-dpl-app=\"add-to-checklist\"&gt;&lt;/div&gt;\n</code></pre> <p>The name of the data attribute should be <code>data-dpl-app</code> and the value should be the name of the application - the value of the <code>appName</code> parameter assigned in the application <code>.mount.js</code> file.</p>"},{"location":"DPL-React/#data-attributes-and-props","title":"Data attributes and props","text":"<p>As stated above, every application needs the corresponding <code>data-dpl-app</code> attribute to even be mounted and shown on the page. Additional data attributes can be passed if necessary. Examples would be contextual ids etc. Normally these would be passed in by the serverside platform e.g. Drupal, Wordpress etc.</p> <pre><code>&lt;div\n  data-dpl-app=\"add-to-checklist\"\n  data-id=\"870970-basis:54172613\"\n  data-error-text=\"A mistake was made\"\n&gt;&lt;/div&gt;\n</code></pre> <p>The above <code>data-id</code> would be accessed as <code>props.id</code> and <code>data-error-text</code> as <code>props.errorText</code> in the entrypoint of an application.</p> Example <pre><code>// ./src/apps/my-new-application/my-new-application.entry.jsx\nimport React from \"react\";\nimport PropTypes from \"prop-types\";\nimport MyNewApplication from './my-new-application.jsx';\n\nexport function MyNewApplicationEntry({ id }) {\n  return (\n    &lt;MyNewApplication\n      // 870970-basis:54172613\n      id={id}\n    /&gt;\n}\n\nexport default MyNewApplicationEntry;\n</code></pre> <p>To fake this in our development environment we need to pass these same data attributes into our entrypoint.</p> Example <pre><code>// ./src/apps/my-new-application/my-new-application.dev.jsx\nimport React from \"react\";\nimport MyNewApplicationEntry from \"./my-new-application.entry\";\nimport MyNewApplication from \"./my-new-application\";\n\nexport default { title: \"Apps|My new application\" };\n\nexport function Entry() {\n  // Testing the version that will be shipped.\n  return &lt;MyNewApplicationEntry id=\"870970-basis:54172613\" /&gt;;\n}\n\nexport function WithoutData() {\n  // Play around with the application itself without server side data.\n  return &lt;MyNewApplication /&gt;;\n}\n</code></pre>"},{"location":"DPL-React/#extending-the-project","title":"Extending the project","text":"<p>If you want to extend this project - either by introducing new components or expand the functionality of the existing ones - and your changes can be implemented in a way that is valuable to users in general, please submit pull requests.</p> <p>Even if that is not the case and you have special needs the infrastructure of the project should also be helpful to you.</p> <p>In such a situation you should fork this project and extend it to your own needs by implementing new applications. New applications can reuse various levels of infrastructure provided by the project such as:</p> <ol> <li>Integration with various webservices</li> <li>User authentication and token management</li> <li>Visual atoms or components</li> <li>Visual representations of existing applications</li> <li>Styling using SCSS</li> <li>Test infrastructure</li> <li>Application mounting</li> </ol> <p>Once the customization is complete the result can be packaged for distribution by pushing the changes to the forked repository:</p> <ol> <li>Changes pushed to the <code>master</code> branch of the forked repository will    automatically update the latest release of the fork.</li> <li>Tags pushed to the forked repository also will be published as new releases    in the fork.</li> </ol> <p>The result can be used in the same ways as the original project.</p>"},{"location":"DPL-React/campaigns/","title":"Campaigns","text":"<p>Campaigns are elements that are shown on the search result page above the search result list. There are three types of campaigns:</p> <ol> <li>Full campaigns - containing an image and a some text.</li> <li>Text-only campaigns - they don't show any images.</li> <li>Image-only campaigns - they don't show any text.</li> </ol> <p>However, they are only shown in case certain criteria are met. We check for this by contacting the dpl-cms API.</p>"},{"location":"DPL-React/campaigns/#how-campaign-setup-works-in-dpl-cms","title":"How campaign setup works in dpl-cms","text":"<p>Dpl-cms is a cms system based on Drupal, where the system administrators can set up campaigns they want to show to their users. Drupal also allows the cms system to act as an API endpoint that we then can contact from our apps.</p> <p>The cms administrators can specify the content (image, text) and the visibility criteria for each campaign they create. The visibility criteria is based on search filter facets. Does that sound familiar? Yes, we use another API to get that very data in THIS project - in the search result app. The facets differ based on the search string the user uses for their search.</p> <p>As an example, the dpl-cms admin could wish to show a Harry Potter related campaign to all the users whose search string retreives search facets which have \"Harry Potter\" as one of the most relevant subjects. Campaigns in dpl-cms can use triggers such as subject, main language, etc.</p>"},{"location":"DPL-React/campaigns/#react-code-example","title":"React code example","text":"<p>An example code snippet for retreiving a campaign from our react apps would then look something like this:</p> <pre><code>  // Creating a state to store the campaign data in.\n  const [campaignData, setCampaignData] = useState&lt;CampaignMatchPOST200 | null&gt;(\n    null\n  );\n\n  // Retreiving facets for the campaign based on the search query and existing\n  // filters.\n  const { facets: campaignFacets } = useGetFacets(q, filters);\n\n  // Using the campaign hook generated by Orval from src/core/dpl-cms/dpl-cms.ts\n  // in order to get the mutate function that lets us retreive campaigns.\n  const { mutate } = useCampaignMatchPOST();\n\n  // Only fire the campaign data call if campaign facets or the mutate function\n  // change their value.\n  useDeepCompareEffect(() =&gt; {\n    if (campaignFacets) {\n      mutate(\n        {\n          data: campaignFacets as CampaignMatchPOSTBodyItem[],\n           params: {\n            _format: \"json\"\n          }\n        },\n        {\n          onSuccess: (campaign) =&gt; {\n            setCampaignData(campaign);\n          },\n          onError: () =&gt; {\n            // Handle error.\n          }\n        }\n      );\n    }\n  }, [campaignFacets, mutate]);\n</code></pre>"},{"location":"DPL-React/campaigns/#showing-campaigns-in-dpl-react-when-in-development-mode","title":"Showing campaigns in dpl-react when in development mode","text":"<p>You first need to make sure to have a campaign set up in your locally running dpl-cms ( run this repo locally) Then, in order to see campaigns locally in dpl-react in development mode, you will most likely need a browser plugin such as Google Chrome's \"Allow CORS: Access-Control-Allow-Origin\" in order to bypass CORS policy for API data calls.</p>"},{"location":"DPL-React/code_guidelines/","title":"React Code guidelines","text":"<p>The following guidelines describe best practices for developing code for React components for the Danish Public Libraries CMS project. The guidelines should help achieve:</p> <ul> <li>A stable, secure and high quality foundation for building and maintaining   client-side TypeScript components for library websites</li> <li>Consistency across multiple developers participating in the project</li> <li>The best possible conditions for sharing components between library websites</li> <li>The best possible conditions for the individual library website to customize   configuration and appearance</li> </ul> <p>Contributions to the DPL React project will be reviewed by members of the Core team. These guidelines should inform contributors about what to expect in such a review. If a review comment cannot be traced back to one of these guidelines it indicates that the guidelines should be updated to ensure transparency.</p>"},{"location":"DPL-React/code_guidelines/#coding-standards","title":"Coding standards","text":"<p>The project follows the Airbnb JavaScript Style Guide and Airbnb React/JSX Style Guide. They have been extended to support TypeScript.</p> <p>This choice is based on multiple factors:</p> <ol> <li>Historically the community of developers working with the Danish Public    Libraries has ties to the Drupal project.    Drupal has adopted the Airbnb JavaScript Style Guide    so this choice should ensure consistency between the two projects.</li> <li>Airbnb's standard is one of the best known and most used in the JavaScript    coding standard landscape.</li> <li>Airbnb\u2019s standard is both comprehensive and well documented.</li> <li>Airbnb\u2019s standards cover both JavaScript in general React/JSX specifically.    This avoids potential conflicts between multiple standards.</li> </ol> <p>The following lists significant areas where the project either intentionally expands or deviates from the official standards or areas which developers should be especially aware of.</p>"},{"location":"DPL-React/code_guidelines/#general","title":"General","text":"<ul> <li>The default language for all code and comments is English.</li> <li>Components must be compatible with the latest stable version of the following   browsers:</li> <li>Desktop<ul> <li>Microsoft Edge</li> <li>Google Chrome</li> <li>Safari</li> <li>Firefox</li> </ul> </li> <li>Mobile<ul> <li>Google Chrome</li> <li>Safari</li> <li>Firefox</li> <li>Samsung Browser</li> </ul> </li> </ul>"},{"location":"DPL-React/code_guidelines/#typescript","title":"TypeScript","text":""},{"location":"DPL-React/code_guidelines/#named-functions-vs-anonymous-arrow-functions","title":"Named functions vs. anonymous arrow functions","text":"<p>AirBnB's only guideline towards this is that anonymous arrow function nation is preferred over the normal anonymous function notation.</p> <p>This project sticks to the above guideline as well. If we need to pass a function as part of a callback or in a promise chain and we on top of that need to pass some contextual variables that are not passed implicitly from either the callback or the previous link in the promise chain we want to make use of an anonymous arrow function as our default.</p> <p>This comes with the build in disclaimer that if an anonymous function isn't required the implementer should heavily consider moving the logic out into its own named function expression.</p> <p>The named function is primarily desired due to it's easier to debug nature in stacktraces.</p>"},{"location":"DPL-React/code_guidelines/#react","title":"React","text":"<ul> <li>Configuration must be passed as props for components. This allows the host   system to modify how a component works when it is inserted.</li> <li>All components should be provided with skeleton screens.   This ensures that the user interface reflects the final state even when data   is loaded asynchronously. This reduces load time frustration.</li> <li>Components should be optimistic.   Unless we have reason to believe that an operation may fail we should provide   fast response to users.</li> <li>All interface text must be implemented as props for components. This allows   the host system to provide a suitable translation/version when using the   component.</li> </ul>"},{"location":"DPL-React/code_guidelines/#css","title":"CSS","text":"<ul> <li>All classes must have the dpl- prefix. This makes them distinguishable from   classes provided by the host system.</li> <li>Class names should follow the Block-Element-Modifier architecture.</li> <li>Components must use and/or provide a default style sheet which at least   provides a minimum of styling showing the purpose of the component.</li> <li>Elements must be provided with meaningful classes even though they are not   targeted by the default style sheet. This helps host systems provide   additional styling of the components. Consider how the component consists of   blocks and elements with modifiers and how these can be nested within each   other.</li> <li>Components must use SCSS for styling. The project uses PostCSS   and PostCSS-SCSS within Webpack for   processing.</li> </ul>"},{"location":"DPL-React/code_guidelines/#html","title":"HTML","text":"<ul> <li>Components must use semantic HTML5 markup.</li> <li>Components must provide configuration to set a top headline level for the   component. This helps provide a proper document outline to ensure the   accessibility of the system.</li> </ul>"},{"location":"DPL-React/code_guidelines/#naming","title":"Naming","text":""},{"location":"DPL-React/code_guidelines/#files","title":"Files","text":"<p>Files provided by components must be placed in the following folders and have the extensions defined here.</p> <ul> <li>Components (React applications)</li> <li>apps/[component-name]/[component-name].tsx<ul> <li>Core TSX component.</li> </ul> </li> <li>components/[component-name]/[component-name].scss<ul> <li>Stylesheet for the component.</li> </ul> </li> <li>apps/[component-name]/[component-name].entry.tsx<ul> <li>Main application entrypoint.</li> <li>This will usually also be where state management is implemented.</li> <li>This must not include the default stylesheet.</li> </ul> </li> <li>apps/[component-name]/[component-name].dev.tsx<ul> <li>Storybook entry for the component.</li> <li>If the component has a stylesheet this must also be included here.</li> </ul> </li> <li>apps/[component-name]/[component-name].mount.ts<ul> <li>Code for registering the application to be booted when a page is loaded on   the host system.</li> </ul> </li> <li>apps/[component-name]/[component-name].test.ts<ul> <li>Test of the component implemented with Cypress</li> </ul> </li> <li>Reusable elements (React components)</li> <li>components/[component-name]/[component-name].dev.tsx</li> <li>components/[component-name]/[component-name].tsx</li> <li>components/[component-name]/[component-name].scss</li> <li>Reusable functions and classes</li> <li>core/[function].ts</li> <li>core/[Class].ts</li> </ul>"},{"location":"DPL-React/code_guidelines/#third-party-code","title":"Third party code","text":"<p>The project uses Yarn as a package manager to handle code which is developed outside the project repository. Such code must not be committed to the Core project repository.</p> <p>When specifying third party package versions the project follows these guidelines:</p> <ul> <li>Use the ^ next significant release operator   for packages which follow semantic versioning.</li> <li>The version specified must be the latest known working and secure version. We   do not want accidental downgrades.</li> <li>We want to allow easy updates to all working releases within the same major   version.</li> <li>Packages which are not intended to be executed at runtime in the production   environment should be marked as development dependencies.</li> </ul>"},{"location":"DPL-React/code_guidelines/#reusing-dependencies","title":"Reusing dependencies","text":"<p>Components must reuse existing dependencies in the project before adding new ones which provide similar functionality. This ensures consistency and avoids unnecessary increases in the package size of the project.</p> <p>The reasoning behind the choice of key dependencies have been documented in the architecture directory.</p>"},{"location":"DPL-React/code_guidelines/#altering-third-party-code","title":"Altering third party code","text":"<p>The project uses patches rather than forks to modify third party packages. This makes maintenance of modified packages easier and avoids a collection of forked repositories within the project.</p> <ul> <li>Use an appropriate method for the corresponding package manager for managing   the patch.</li> <li>Patches should be external by default. In rare cases it may be needed to   commit them as a part of the project.</li> <li>When providing a patch you must document the origin of the patch e.g. through   an url in a commit comment or preferably in the package manager configuration   for the project.</li> </ul>"},{"location":"DPL-React/code_guidelines/#code-comments","title":"Code comments","text":"<p>Code comments which describe what an implementation does should only be used for complex implementations usually consisting of multiple loops, conditional statements etc.</p> <p>Inline code comments should focus on why an unusual implementation has been implemented the way it is. This may include references to such things as business requirements, odd system behavior or browser inconsistencies.</p>"},{"location":"DPL-React/code_guidelines/#commit-messages","title":"Commit messages","text":"<p>Commit messages in the version control system help all developers understand the current state of the code base, how it has evolved and the context of each change. This is especially important for a project which is expected to have a long lifetime.</p> <p>Commit messages must follow these guidelines:</p> <ol> <li>Each line must not be more than 72 characters long</li> <li>The first line of your commit message (the subject) must contain a short    summary of the change. The subject should be kept around 50 characters long.</li> <li>The subject must be followed by a blank line</li> <li>Subsequent lines (the body) should explain what you have changed and why the    change is necessary. This provides context for other developers who have not    been part of the development process. The larger the change the more    description in the body is expected.</li> <li>If the commit is a result of an issue in a public issue tracker,    platform.dandigbib.dk, then the subject must start with the issue number    followed by a colon (:). If the commit is a result of a private issue tracker    then the issue id must be kept in the commit body.</li> </ol> <p>When creating a pull request the pull request description should not contain any information that is not already available in the commit messages.</p> <p>Developers are encouraged to read How to Write a Git Commit Message by Chris Beams.</p>"},{"location":"DPL-React/code_guidelines/#tool-support","title":"Tool support","text":"<p>The project aims to automate compliance checks as much as possible using static code analysis tools. This should make it easier for developers to check contributions before submitting them for review and thus make the review process easier.</p> <p>The following tools pay a key part here:</p> <ol> <li>Eslint with the following rulesets and plugins:<ol> <li>Airbnb JavaScript Style Guide</li> <li>Airbnb React/JSX Style Guide</li> <li>Prettier</li> <li>Cypress</li> </ol> </li> <li>Stylelint with the following rulesets and plugins<ol> <li>Recommended SCSS</li> <li>Prettier</li> <li>BEM support</li> </ol> </li> </ol> <p>In general all tools must be able to run locally. This allows developers to get quick feedback on their work.</p> <p>Tools which provide automated fixes are preferred. This reduces the burden of keeping code compliant for developers.</p> <p>Code which is to be exempt from these standards must be marked accordingly in the codebase - usually through inline comments (Eslint, Stylelint). This must also include a human readable reasoning. This ensures that deviations do not affect future analysis and the project should always pass through static analysis.</p> <p>If there are discrepancies between the automated checks and the standards defined here then developers are encouraged to point this out so the automated checks or these standards can be updated accordingly.</p>"},{"location":"DPL-React/code_guidelines/#writing-frontend-tests","title":"Writing frontend tests","text":"<p>The frontend tests are executed in Cypress.</p> <p>The test files are placed alongside the application components and are named following pattern: \"*.test.ts\". Eg.: material.test.ts.</p>"},{"location":"DPL-React/code_guidelines/#test-structuring","title":"Test structuring","text":"<p>After quite a lot of bad experiences with unstable tests and reading both the official documentation and articles about the best practices we have ended up with a recommendation of how to write the tests.</p> <p>According to this article it is important to distinguish between commands and assertions. Commands are used in the beginning of a statement and yields a chainable element that can be followed by one or more assertions in the end.</p> <p>So first we target an element. Next we can make one or more assertions on the element.</p> <p>We have created some helper commands for targeting an element: getBySel, getBySelLike and getBySelStartEnd. They look for elements as advised by the Selecting Elements section from the Cypress documentation about best practices.</p> <p>Example of a statement:</p> <pre><code>// Targeting.\ncy.getBySel(\"reservation-success-title-text\")\n  // Assertion.\n  .should(\"be.visible\")\n  // Another assertion.\n  .and(\"contain\", \"Material is available and reserved for you!\");\n</code></pre>"},{"location":"DPL-React/code_guidelines/#writing-unit-tests","title":"Writing Unit Tests","text":"<p>We are using Vitest as framework for running unit tests. By using that we can test functions (and therefore also hooks) and classes.</p>"},{"location":"DPL-React/code_guidelines/#where-do-i-place-my-tests","title":"Where do I place my tests?","text":"<p>They have to be placed in <code>src/tests/unit</code>.</p> <p>Or they can also be placed next to the code at the end of a file as described here.</p> <pre><code>export const sum = (...numbers: number[]) =&gt;\n  numbers.reduce((total, number) =&gt; total + number, 0);\n\nif (import.meta.vitest) {\n  const { describe, expect, it } = import.meta.vitest;\n\n  describe(\"sum\", () =&gt; {\n    it(\"should sum numbers\", () =&gt; {\n      expect(sum(1, 2, 3)).toBe(6);\n    });\n  });\n}\n</code></pre> <p>In that way it helps us to test and mock unexported functions.</p>"},{"location":"DPL-React/code_guidelines/#testing-hooks","title":"Testing hooks","text":"<p>For testing hooks we are using the library <code>@testing-library/react-hooks</code> and you can also take a look at the text test to see how it can be done.</p>"},{"location":"DPL-React/error_handling/","title":"Error Handling","text":"<p>Error handling is something that is done on multiple levels: Eg.: Form validation, network/fetch error handling, runtime errors. You could also argue that the fact that the codebase is making use of typescript and code generation from the various http services (graphql/REST) belongs to the same idiom of error handling in order to make the applications more robust.</p>"},{"location":"DPL-React/error_handling/#error-boundary","title":"Error Boundary","text":"<p>Error boundary was introduced in React 16 and makes it possible to implement a \"catch all\" feature catching \"uncatched\" errors and replacing the application with a component to the users that something went wrong. It is meant ato be a way of having a safety net and always be able to tell the end user that something went wrong. The apps are being wrapped in the error boundary handling which makes it possible to catch thrown errors at runtime.</p>"},{"location":"DPL-React/error_handling/#fetch-and-error-boundary","title":"Fetch and Error Boundary","text":"<p>Async operations and therby also <code>fetch</code> are not being handled out of the box by the React Error Boundary. But fortunately <code>react-query</code>, which is being used both by the REST services (Orval) and graphql (Graphql Code Generator), has a way of addressing the problem. The <code>QueryClient</code> can be configured to trigger the Error Boundary system if an error is thrown. So that is what we are doing.</p>"},{"location":"DPL-React/error_handling/#fetch-error-classes","title":"Fetch error classes","text":"<p>Two different types of error classes have been made in order to handle errors in the fetchers: http errors and fetcher errors.</p> <p>Http errors are the ones originating from http errors and have a status code attached.</p> <p>Fetcher errors are whatever else bad that could apart from http errors. Eg. JSON parsing gone wrong.</p> <p>Both types of errors comes in two variants: \"normal\" and \"critical\". The idea is that only critical errors will trigger an Error Boundary.</p> <p>For instance if you look at the DBC Gateway fetcher it throws a <code>DbcGateWayHttpError</code> in case of a http error occurs. <code>DbcGateWayHttpError</code> extends the <code>FetcherCriticalHttpError</code> which makes sure to trigger the Error Boundary system.</p>"},{"location":"DPL-React/error_handling/#using-errorname","title":"Using Error.name","text":"<p>The reason why *.name is set in the errors is to make it clear which error was thrown. If we don't do that the name of the parent class is used in the error log. And then it is more difficult to trace where the error originated from.</p>"},{"location":"DPL-React/error_handling/#future-considerations","title":"Future considerations","text":"<p>The initial implementation is handling http errors on a coarse level: If <code>response.ok</code> is false then throw an error. If the error is critical the error boundary is triggered. In future version you could could take more things into consideration regarding the error:</p> <ul> <li>Should all status codes trigger an error?</li> <li>Should we have different types of error level depending on request and/or http method?</li> </ul>"},{"location":"DPL-React/fbs-adapter-client/","title":"FBS adapter client","text":"<p>The FBS client adapter is autogenerated base the swagger 1.2 json files from FBS. But Orval requires that we use swagger version 2.0 and the adapter has some changes in paths and parameters. So some conversion is need at the time of this writing.</p> <p>FBS documentation can be found here.</p> <p>All this will hopefully be changed when/or if the adapter comes with its own specifications.</p>"},{"location":"DPL-React/fbs-adapter-client/#api-spec-converter","title":"API spec converter","text":"<p>A repository dpl-fbs-adapter-tool tool build around PHP and NodeJS can translate the FBS specifikation into one usable for Orval client generator. It also filters out all the FBS calls not need by the DPL project.</p> <p>The tool uses go-task to simply the execution of the command.</p>"},{"location":"DPL-React/fbs-adapter-client/#setup","title":"Setup","text":"<p>Simple use the installation task.</p> <pre><code>task dev:install\n</code></pre>"},{"location":"DPL-React/fbs-adapter-client/#convert-swagger","title":"Convert swagger","text":"<p>First convert the swagger 1.2 (located in <code>/fbs/externalapidocs</code>) to swagger 2.0 using the <code>api-spec-converter</code> tool.</p> <pre><code>dev:swagger2yaml\n</code></pre>"},{"location":"DPL-React/fbs-adapter-client/#build-the-adapter-specifications","title":"Build the Adapter specifications","text":"<p>Build the swagger specification usable by Orval then run Orval.</p> <pre><code>task dev:convert\n</code></pre>"},{"location":"DPL-React/fbs-adapter-client/#fbs-adapter","title":"FBS Adapter","text":"<p>The FSB adapter lives at: https://github.com/DBCDK/fbs-cms-adapter</p>"},{"location":"DPL-React/request_mocking_wiremock/","title":"Request mocking / Wiremock","text":"<p>We use Wiremock to enable request mocking and reduce dependency on external systems during development.</p>"},{"location":"DPL-React/request_mocking_wiremock/#background","title":"Background","text":"<p>The React components generally work with data from external systems. Having data in a specific state may be necessary in the development of some features but it can be cumbersome so set up exactly such a state. Many factors can also reduce the usefulness of this state. Development may change it once a reservation is deleted. Time affects where a loan is current or overdue.</p> <p>With all this in mind it is useful for us to be able to recreate specific states. We do so using Wiremock. Wiremock is a system which allows us to mock external APIs. It can be configured or instrumented to return predefined responses given predefined requests.</p> <p>We use Wiremock through Docker based on the following setup:</p> <ol> <li>One Wiremock instance per external API we want to mock</li> <li>Wiremock Studio provides a UI for managing    Wiremock instances</li> <li>Mocked requests/responses are persisted as files in the repository for    sharing between developers</li> <li>Wiremock is exposed to local browsers through a Docker DNS proxy like Dory</li> <li>Storybook can be preconfigured to use Wiremock instead of production    webservices in <code>.env</code></li> </ol>"},{"location":"DPL-React/request_mocking_wiremock/#howtos","title":"Howtos","text":""},{"location":"DPL-React/request_mocking_wiremock/#use-wiremock-instead-of-production-services-during-development","title":"Use Wiremock instead of production services during development","text":"<ol> <li>Start Wiremock Docker containers: <code>docker compose up -d</code></li> <li>If available: Enable a Docker DNS proxy like Dory: <code>dory up</code></li> <li>Create/update a <code>.env</code> file with hostnames (and ports if necessary) for    Wiremock Docker containers e.g.:</li> </ol> <pre><code>PUBLIZON_BASEURL=http://publizon-mock.docker`\nFBS_BASEURL=http://fbs-mock.docker`\nCMS_BASEURL=http://cms-mock.docker\n</code></pre> <ol> <li>Start Storybook: <code>yarn run dev</code></li> </ol>"},{"location":"DPL-React/request_mocking_wiremock/#set-up-a-mocked-response-in-wiremock","title":"Set up a mocked response in Wiremock","text":"<p>To set up a mocked response for a new request to FBS do the following:</p> <ol> <li>Open Wiremock Studio at http://dpl-mock.docker/</li> <li>Click \"FBS\" to manage the Wiremock instance for FBS</li> <li>Click \"Stubs\" to see a list of existing requests/responses</li> <li>Click \"New\" to create a new request/response set and provide a name</li> <li>Provide the HTTP method, path and other parts of the request to match (note: make sure to check \"advanced\" option out, and match either path, or path AND the query.)</li> <li>Provide the response HTTP status code and body to return</li> <li>Click \"Save\"</li> <li>See that the stub has been persisted as a new file in    <code>.docker/wiremock/fbs/mappings</code></li> <li>Restart Wiremock docker images to load the updated stub</li> </ol> <p>The following example shows how one might create a mock which returns an error if the client tries to delete a specific reservation.</p> <p></p>"},{"location":"DPL-React/skeleton_screens/","title":"Skeleton screens","text":"<p>In order to improve both UX and the performance score you can choose to use skeleton screens in situation where you need to fill the interface with data from a requests to an external service.</p>"},{"location":"DPL-React/skeleton_screens/#main-purpose","title":"Main Purpose","text":"<p>The skeleton screens are being showed instantly in order to deliver some content to the end user fast while loading data. When the data is arriving the skeleton screens are being replaced with the real data.</p>"},{"location":"DPL-React/skeleton_screens/#how-to-use-it","title":"How to use it","text":"<p>The skeleton screens are rendered with help from the skeleton-screen-css library.  By using ssc classes  you can easily compose screens  that simulate the look of a \"real\" rendering with real data.</p>"},{"location":"DPL-React/skeleton_screens/#example","title":"Example","text":"<p>In this example we are showing a search result item as a skeleton screen. The skeleton screen consists of a cover, a headline and two lines of text. In this case we wanted to maintain the styling of the .card-list-item wrapper. And show the skeleton screen elements by using ssc classes.</p> <p>```tsx import React from \"react\";</p> <p>const SearchResultListItemSkeleton: React.FC = () =&gt; {   return (         ); };</p> <p>export default SearchResultListItemSkeleton;</p>"},{"location":"DPL-React/ui_text_handling/","title":"UI Text Handling","text":"<p>This document describes how to use the text functionality that is partly defined in src/core/utils/text.tsx and in src/core/text.slice.ts.</p>"},{"location":"DPL-React/ui_text_handling/#main-purpose","title":"Main Purpose","text":"<p>The main purpose of the functionality is to be able to access strings defined at app level inside of sub components without passing them all the way down via props. You can read more about the decision and considerations here.</p>"},{"location":"DPL-React/ui_text_handling/#how-to-use-it","title":"How to use it","text":"<p>In order to use the system the component that has the text props needs to be wrapped with the <code>withText</code> high order function. The texts can hereafter be accessed by using the <code>useText</code> hook.</p>"},{"location":"DPL-React/ui_text_handling/#simple-example","title":"Simple example","text":"<p>In this example we have a HelloWorld app with three text props attached:</p> <pre><code>import React from \"react\";\nimport { withText } from \"../../core/utils/text\";\nimport HelloWorld from \"./hello-world\";\n\nexport interface HelloWorldEntryProps {\n  titleText: string;\n  introductionText: string;\n  whatText: string;\n}\n\nconst HelloWorldEntry: React.FC&lt;HelloWorldEntryProps&gt; = (\n  props: HelloWorldEntryProps\n) =&gt; &lt;HelloWorld /&gt;;\n\nexport default withText(HelloWorldEntry);\n</code></pre> <p>Now it is possible to access the strings like this:</p> <pre><code>import * as React from \"react\";\nimport { Hello } from \"../../components/hello/hello\";\nimport { useText } from \"../../core/utils/text\";\n\nconst HelloWorld: React.FC = () =&gt; {\n  const t = useText();\n  return (\n    &lt;article&gt;\n      &lt;h2&gt;{t(\"titleText\")}&lt;/h2&gt;\n      &lt;p&gt;{t(\"introductionText\")}&lt;/p&gt;\n      &lt;p&gt;\n        &lt;Hello shouldBeEmphasized /&gt;\n      &lt;/p&gt;\n    &lt;/article&gt;\n  );\n};\nexport default HelloWorld;\n</code></pre>"},{"location":"DPL-React/ui_text_handling/#placeholder-example","title":"Placeholder example","text":"<p>It is also possible to use placeholders in the text strings. They can be handy when you want dynamic values embedded in the text.</p> <p>A classic example is the welcome message to the authenticated user. Let's say you have a text with the key: <code>welcomeMessageText</code>. The value from the data prop is: <code>Welcome @username, today is @date</code>. You would the need to reference it like this:</p> <pre><code>import * as React from \"react\";\nimport { useText } from \"../../core/utils/text\";\n\nconst HelloUser: React.FC = () =&gt; {\n  const t = useText();\n  const username = getUsername();\n  const currentDate = getCurrentDate();\n\n  const message = t(\"welcomeMessageText\", {\n    placeholders: {\n      \"@user\": username,\n      \"@date\": currentDate\n    }\n  });\n\n  return (\n    &lt;div&gt;{message}&lt;/div&gt;\n  );\n};\nexport default HelloUser;\n</code></pre>"},{"location":"DPL-React/ui_text_handling/#plural-example","title":"Plural example","text":"<p>Sometimes you want two versions of a text be shown depending on if you have one or multiple items being referenced in the text.</p> <p>That can be accommodated by using the plural text definition.</p> <p>Let's say that an authenticated user has a list of unread messages in an inbox. You could have a text key called: <code>inboxStatusText</code>. The value from the data prop is:</p> <pre><code>{\"type\":\"plural\",\"text\":[\"You have 1 message in the inbox\",\n\"You have @count messages in the inbox\"]}.\n</code></pre> <p>You would then need to reference it like this:</p> <pre><code>import * as React from \"react\";\nimport { useText } from \"../../core/utils/text\";\n\nconst InboxStatus: React.FC = () =&gt; {\n  const t = useText();\n  const user = getUser();\n  const inboxMessageCount = getUserInboxMessageCount(user);\n\n  const status = t(\"inboxStatusText\", {\n    count: inboxMessageCount,\n    placeholders: {\n      \"@count\": inboxMessageCount\n    }\n  });\n\n  return (\n    &lt;div&gt;{status}&lt;/div&gt;\n    // If count == 1 the texts will be:\n    // \"You have 1 message in the inbox\"\n\n    // If count == 5  the texts will be:\n    // \"You have 5 messages in the inbox\"\n  );\n};\nexport default InboxStatus;\n</code></pre>"},{"location":"DPL-React/architecture/","title":"Architecture decision records","text":"<p>We document decisions regarding the architecture of the project using ADRs.</p> <p>We follow the format suggested by Michael Nygaard containing the sections: Title, Context, Decision and Consequences.</p> <p>We do not use the Status section. If an ADR is merged into the main branch it is by default accepted.</p> <p>We have added an Alternatives considered section to ensure we document alternative solutions and the pros and cons for these.</p>"},{"location":"DPL-React/architecture/adr-001-rehydration/","title":"Rehydration","text":""},{"location":"DPL-React/architecture/adr-001-rehydration/#context","title":"Context","text":"<p>We are not able to persist and execute a users intentions across page loads. This is expressed through a number of issues. The main agitator is maintaining intent whenever a user tries to do anything that requires them to be authenticated. In these situations they get redirected off the page and after a successful login they get redirected back to the origin page but without the intended action fulfilled.</p> <p>One example is the <code>AddToChecklist</code> functionality. Whenever a user wants to add a material to their checklist they click the \"Tilf\u00f8j til huskelist\" button next to the material presentation. They then get redirected to Adgangsplatformen. After a successful login they get redirected back to the material page but the material has not been added to their checklist.</p>"},{"location":"DPL-React/architecture/adr-001-rehydration/#decision","title":"Decision","text":"<p>After an intent has been stated we want the intention to be executed even though a page reload comes in the way.</p> <p>We move to implementing what we define as an explicit intention before the actual action is tried for executing.</p> <ol> <li>User clicks the button.</li> <li>Intent state is generated and committed.</li> <li>Implementation checks if the intended action meets all the requirements. In    this case, being logged in and having the necessary payload.</li> <li>If the intention meets all requirements we then fire the addToChecklist    action.</li> <li>Material is added to the users checklist.</li> </ol> <p>The difference between the two might seem superfluous but the important distinction to make is that with our current implementation we are not able to serialize and persist the actions as the application state across page loads. By defining intent explicitly we are able to serialize it and persist it between page loads.</p> <p>This resolves in the implementation being able to rehydrate the persisted state, look at the persisted intentions and have the individual application implementations decide what to do with the intention.</p> <p>A mock implementation of the case by case business logic looks as follows.</p> <pre><code>const initialStore = {\n  authenticated: false,\n  intent: {\n    status: '',\n    payload: {}\n  }\n}\n\nconst fulfillAction = store.authenticated &amp;&amp;\n    (store.intent.status === 'pending' || store.intent.status === 'tried')\nconst getRequirements = !store.authenticated &amp;&amp; store.intent.status === 'pending'\nconst abandonIntention = !store.authenticated &amp;&amp; store.intent.status === 'tried'\n\nfunction AddToChecklist ({ materialId, store }) {\n  useEffect(() =&gt; {\n    if (fulfillAction) {\n      // We fire the actual functionality required to add a material to the\n      // checklist and we remove the intention as a result of it being\n      // fulfilled.\n      addToChecklistAction(store, materialId)\n    } else if (getRequirements) {\n      // Before we redirect we set the status to be \"tried\".\n      redirectToLogin(store)\n    } else if (abandonIntention) {\n      // We abandon the intent so that we won't have an infinite loop of retries\n      // at every page load.\n      abandonAddToChecklistIntention(store)\n    }\n  }, [materialId, store.intent.status])\n  return (\n    &lt;button\n      onClick={() =&gt; {\n        // We do not fire the actual logic that is required to add a material to\n        // the checklist. Instead we add the intention of said action to the\n        // store. This is when we would set the status of the intent to pending\n        // and provide the payload.\n        addToChecklistIntention(store, materialId)\n      }}\n    &gt;\n      Tilf\u00f8j til huskeliste\n    &lt;/button&gt;\n  )\n}\n</code></pre> <p>We utilize session storage to persist the state on the client due to it's short lived nature and porous features.</p> <p>We choose Redux as the framework to implemenent this. Redux is a blessed choice in this instance. It has widespread use, an approachable design and is well-documented. The best way to go about a current Redux implementation as of now is <code>@reduxjs/toolkit</code>. Redux is a sufficiently advanced framework to support other uses of application state and even co-locating shared state between applications.</p> <p>For our persistence concerns we want to use the most commonly used tool for that, <code>redux-persist</code>. There are some implementation details to take into consideration when integrating the two.</p>"},{"location":"DPL-React/architecture/adr-001-rehydration/#alternatives-considered","title":"Alternatives considered","text":""},{"location":"DPL-React/architecture/adr-001-rehydration/#persistence-in-url","title":"Persistence in URL","text":"<p>We could persist the intentions in the URL that is delivered back to the client after a page reload. This would still imply some of the architectural decisions described in Decision in regards to having an \"intent\" state, but some of the different status flags etc. would not be needed since state is virtually shared across page loads in the url. However this simpler solution cannot handle more complex situations than what can be described in the URL feasibly.</p>"},{"location":"DPL-React/architecture/adr-001-rehydration/#usecontext","title":"useContext","text":"<p>React offers <code>useContext()</code> for state management as an alternative to Redux.</p> <p>We prefer Redux as it provides a more complete environment when working with state management. There is already a community of established practices and libraries which integrate with Redux. One example of this is our need to persist actions. When using Redux we can handle this with <code>redux-persist</code>. With <code>useContext()</code> we would have to roll our own implementation.</p> <p>Some of the disadvantages of using Redux e.g. the amount of required boilerplate code are addressed by using <code>@reduxjs/toolkit</code>.</p>"},{"location":"DPL-React/architecture/adr-001-rehydration/#status","title":"Status","text":"<p>Accepted</p>"},{"location":"DPL-React/architecture/adr-001-rehydration/#consequences","title":"Consequences","text":"<ul> <li>We are able to support most if not all of our rehydration cases and therefore   pick up user flow from where we left it.</li> <li>Heavy degree of complexity is added to tasks that requires an intention   instead of a simple action.</li> <li>Saving the immediate state to the session storage makes for yet another place   to \"clear cache\".</li> </ul>"},{"location":"DPL-React/architecture/adr-002-ui-text-handling/","title":"UI Text Handling","text":""},{"location":"DPL-React/architecture/adr-002-ui-text-handling/#context","title":"Context","text":"<p>It has been decided that app context/settings should be passed from the server side rendered mount points via data props. One type of settings is text strings that is defined by the system/host rendering the mount points. Since we are going to have quite some levels of nested components it would be nice to have a way to extract the string without having to define them all the way down the tree.</p>"},{"location":"DPL-React/architecture/adr-002-ui-text-handling/#decision","title":"Decision","text":"<p>A solution has been made that extracts the props holding the strings and puts them in the Redux store under the index: <code>text</code> at the app entry level. That is done with the help of the <code>withText()</code> High Order Component. The solution of having the strings in redux enables us to fetch the strings at any point in the tree. A hook called: <code>useText()</code> makes it simple to request a certain string inside a given component.</p>"},{"location":"DPL-React/architecture/adr-002-ui-text-handling/#alternatives-considered","title":"Alternatives considered","text":"<p>One major alternative would be not doing it and pass down the props. But that leaves us with text props all the way down the tree which we would like to avoid. Some translation libraries has been investigated but that would in most cases give us a lot of tools and complexity that is not needed in order to solve the relatively simple task.</p>"},{"location":"DPL-React/architecture/adr-002-ui-text-handling/#consequences","title":"Consequences","text":"<p>Since we omit the text props down the tree it leaves us with fewer props and a cleaner component setup. Although some \"magic\" has been introduced with text prop matching and storage in redux, it is outweighed by the simplicity of the HOC wrapper and useText hook.</p>"},{"location":"DPL-React/architecture/adr-003-downshift/","title":"Downshift","text":""},{"location":"DPL-React/architecture/adr-003-downshift/#context","title":"Context","text":"<p>As a part of the project, we need to implement a dropdown autosuggest component. This component needs to be complient with modern website accessibility rules:</p> <ul> <li>The component dropdown items are accessible by keyboard by using arrow keys - down and up.</li> <li>The items visibly change when they are in current focus.</li> <li>Items are selectable using the keyboard.</li> </ul> <p>Apart from these accessibility features, the component needs to follow a somewhat complex design described in this Figma file. As visible in the design this autosuggest dropdown doesn't consist only of single line text items, but also contains suggestions for specific works - utilizing more complex suggestion items with cover pictures, and release years.</p>"},{"location":"DPL-React/architecture/adr-003-downshift/#decision","title":"Decision","text":"<p>Our research on the most popular and supported javascript libraries heavily leans on this specific article. In combination with our needs described above in the <code>context section</code>, but also considering  what it would mean to build this component from scratch without any libraries, the decision taken favored a library called <code>Downshift</code>.</p> <p>This library is the second most popular JS library used to handle autsuggest dropdowns, multiselects, and select dropdowns with a large following and continuous support. Out of the box, it follows the ARIA principles, and handles problems that we would normally have to solve ourselves (e.g. opening and closing of the dropdown/which item is currently in focus/etc.).</p> <p>Another reason why we choose <code>Downshift</code> over its peer libraries is the amount of flexibility that it provides. In our eyes, this is a strong quality of the library that allows us to also implement more complex suggestion dropdown items.</p>"},{"location":"DPL-React/architecture/adr-003-downshift/#alternatives-considered","title":"Alternatives considered","text":""},{"location":"DPL-React/architecture/adr-003-downshift/#building-the-autosuggest-dropdown-not-using-javascript-libraries","title":"Building the autosuggest dropdown not using javascript libraries","text":"<p>In this case, we would have to handle accessibility and state management of the component with our own custom solutition.</p>"},{"location":"DPL-React/architecture/adr-003-downshift/#status","title":"Status","text":"<p>Accepted.</p>"},{"location":"DPL-React/architecture/adr-003-downshift/#consequences","title":"Consequences","text":"<ul> <li>We are able to comply with ARIA accesibility design principles for autosuggest dropdowns/comboboxes.</li> <li>We introduced complexity to the project for initial project integration of the library.</li> <li>After initial integration, this library can be utilized for all other select, multiselect, and autosuggest/combobox solutions.</li> </ul>"},{"location":"DPL-React/architecture/adr-004-relative-ci/","title":"RelativeCI","text":""},{"location":"DPL-React/architecture/adr-004-relative-ci/#context","title":"Context","text":"<p>Staying informed about how the size of the JavaScript we require browsers to download to use the project plays an important part in ensuring a performant solution.</p> <p>We currently have no awareness of this in this project and the result surfaces down the line when the project is integrated with the CMS, which is tested with Lighthouse.</p> <p>To address this we want a solution that will help us monitor the changes to the size of the bundle we ship for each PR.</p>"},{"location":"DPL-React/architecture/adr-004-relative-ci/#decision","title":"Decision","text":"<p>We add integration to RelativeCI to the project. RelativeCI supports our primary use case and has a number of qualities which we value:</p> <ul> <li>Support for GitHub actions and reporting as GitHub status checks</li> <li>Support for fork-based development workflows</li> <li>A free tier for open source projects</li> <li>Other types of analysis e.g. duplicate packages, continual monitoring</li> </ul>"},{"location":"DPL-React/architecture/adr-004-relative-ci/#alternatives-considered","title":"Alternatives considered","text":""},{"location":"DPL-React/architecture/adr-004-relative-ci/#bundlewatch","title":"Bundlewatch","text":"<p>Bundlewatch and its ancestor, bundlesize combine a CLI tool and a web app to provide bundle analysis and feedback on GitHub as status checks.</p> <p>These solutions no longer seem to be actively maintained. There are several bugs that would affect us and fixes remain unmerged. The project relies on a custom secret instead of <code>GITHUB_TOKEN</code>. This makes supporting our fork-based development workflow harder.</p>"},{"location":"DPL-React/architecture/adr-004-relative-ci/#bundle-comparison","title":"Bundle comparison","text":"<p>This is a GitHub Action which can be used in configurations where statistics for two bundles are compared e.g. for the base and head of a pull request. This results in a table of changes displayed as a comment in the pull request. This is managed using <code>GITHUB_TOKEN.</code></p>"},{"location":"DPL-React/architecture/adr-004-relative-ci/#status","title":"Status","text":"<p>Accepted.</p>"},{"location":"DPL-React/architecture/adr-004-relative-ci/#consequences","title":"Consequences","text":"<ul> <li>We can determine the effect of adding a new JavaScript library to our project</li> <li>We add another dependency to a third party system</li> </ul>"},{"location":"DPL-React/architecture/adr-005-react-use/","title":"React Use","text":""},{"location":"DPL-React/architecture/adr-005-react-use/#context","title":"Context","text":"<p>The decision of obtaining <code>react-use</code> as a part of the project originated from the problem that arose from having an useEffect hook with an object as a dependency.</p> <p><code>useEffect</code> does not support comparison of objects or arrays and we needed a method for comparing such natives.</p>"},{"location":"DPL-React/architecture/adr-005-react-use/#decision","title":"Decision","text":"<p>We decided to go for the react-use package react-use. The reason is threefold:</p> <ul> <li>It could solve the problem with deep comparison of dependencies by using   <code>useDeepCompareEffect</code></li> <li>It offered an alternative to the  react-hook-inview viewport handling.  So we did not need to use two packages.</li> <li>It has a range of other utility hooks that we can make use of in the future.</li> </ul>"},{"location":"DPL-React/architecture/adr-005-react-use/#alternatives-considered","title":"Alternatives considered","text":"<p>We could have used our own implementation of the problem. But since it is a common problem we might as well use a community backed solution. And <code>react-use</code> gives us a wealth of other tools.</p>"},{"location":"DPL-React/architecture/adr-005-react-use/#consequences","title":"Consequences","text":"<p>We can now use <code>useDeepCompareEffect</code> instead of <code>useEffect</code> in cases where we have arrays or objects amomg the dependencies. And we can make use of all the other utility hooks that the package provides.</p>"},{"location":"DPL-React/architecture/adr-006-unit-tests/","title":"Unit Tests","text":""},{"location":"DPL-React/architecture/adr-006-unit-tests/#context","title":"Context","text":"<p>The code base is growing and so does the number of functions and custom hooks.</p> <p>While we have a good coverage in our UI tests from Cypress we are lacking something to tests the inner workings of the applications.</p> <p>With unit tests added we can test bits of functionality that is shared between different areas of the application and make sure that we get the expected output giving different variations of input.</p>"},{"location":"DPL-React/architecture/adr-006-unit-tests/#decision","title":"Decision","text":"<p>We decided to go for Vitest which is an easy to use and very fast unit testing tool.</p> <p>It has more or less the same capabilities as Jest which is another popular testing framework which is similar.</p> <p>Vitest is framework agnostic so in order to make it possible to test hooks we found <code>@testing-library/react-hooks</code> that works in conjunction with Vitest.</p>"},{"location":"DPL-React/architecture/adr-006-unit-tests/#alternatives-considered","title":"Alternatives considered","text":"<p>We could have used Jest. But trying that we experienced major problems with having both Jest and Cypress in the same codebase. They have colliding test function names and Typescript could not figure it out.</p> <p>There is probably a solution but at the same time we got Vitest recommended. It seemed very fast and just as capable as Jest. And we did not have the colliding issues of shared function/object names.</p>"},{"location":"DPL-React/architecture/adr-006-unit-tests/#consequences","title":"Consequences","text":"<p>We now have unit test as a part of the mix which brings more stability and certainty that the individual pieces of the application work.</p>"},{"location":"DPL-React/architecture/adr-007-configuration/","title":"Configuration","text":""},{"location":"DPL-React/architecture/adr-007-configuration/#context","title":"Context","text":"<p>This project provides a set of React applications which can be placed in a mounting application to provide self service features for Danish public libraries. To support variations in the appearance and behavior of these applications, the applications support configuration. In practice these are provided through data attributes on the root element of the application set by the mounting application.</p> <p>A configuration value can use one of three different types:</p> <ol> <li>String value</li> <li>Multiple string values</li> <li>A JSON object</li> </ol> <p>Also, a configuration value may be required or optional for the application to function.</p> <p>Our use of TypeScript should match our handling of configuration.</p> <p>Practice has shown that string values are relatively easy to handle but JSON values are not. This leads to errors which can be hard to debug. Consequently, we need to clarify our handling of configuration and especially for JSON values.</p>"},{"location":"DPL-React/architecture/adr-007-configuration/#decision","title":"Decision","text":"<p>We will use the following rules for handling configuration:</p> <ol> <li>The mounting application is responsible for providing configuration values    in the correct format.</li> <li>If a configuration value is optional and does not have a value then the    corresponding data attribute must not be set by the mounting application.</li> <li>If a configuration value is optional the application can alternately specify    a required configuration value with an <code>enabled</code> boolean property. If no    configuration is provided <code>{ enabled: false }</code> is an acceptable value.</li> </ol>"},{"location":"DPL-React/architecture/adr-007-configuration/#alternatives-considered","title":"Alternatives considered","text":""},{"location":"DPL-React/architecture/adr-007-configuration/#make-configuration-optional","title":"Make configuration optional","text":"<p>We could make all configuration optional and introduce suitable handling if no value is provided. This could introduce default values, errors or workarounds.</p> <p>This would make the React application code more complex. We would rather push this responsibility to the mounting application.</p>"},{"location":"DPL-React/architecture/adr-007-configuration/#consequences","title":"Consequences","text":"<p>This approach provides a first step for improving our handling of configuration. Potential improvements going forward are:</p> <ol> <li>Runtime validation of JSON values using libraries like Zod,    io-ts or Runtypes.</li> <li>Specification of configuration values in a separate file which can be used    by the mounting application to provide configuration values and by the    React application to validate the provided values. One format for doing so    would be JSON Schema which is widely supported in JavaScript, TypeScript and    PHP (used by DPL CMS).</li> </ol>"},{"location":"DPL-React/architecture/adr-008-error-handling/","title":"Architecture Decision Record: Error handling in the React Apps","text":""},{"location":"DPL-React/architecture/adr-008-error-handling/#context","title":"Context","text":"<p>We needed to handle errors thrown in the apps, both in requests by fetchers but also exceptions thrown at runtime.</p> <p>Errors were already handled in the initial implementation of this project. An Error Boundary was already implemented but we were lacking two important features:</p> <ul> <li>Every app shouldn't show its error in their own scope. We wanted to centralise the error rendering for the end user</li> <li>All errors should NOT be caught by the Error Boundary an thereby block the whole app.</li> </ul>"},{"location":"DPL-React/architecture/adr-008-error-handling/#decision","title":"Decision","text":""},{"location":"DPL-React/architecture/adr-008-error-handling/#show-the-errors-in-one-place","title":"Show the errors in one place","text":"<p>To solve the problem with each app showing its own error, we decided to make use of React's Portal system. The host (most likely dpl-cms) that includes the apps tells the apps via a config data prop what the container id of error wrapper is. Then the Error boundary system makes sure to use that id when rendering the error.</p>"},{"location":"DPL-React/architecture/adr-008-error-handling/#handle-errors-differently-depending-on-type","title":"Handle errors differently depending on type","text":"<p>Each app is wrapped with an Error Boundary. In the initial implementation that meant that if any exception was thrown the Error Boundary would catch any exception and showing an error message to the end user. Furthermore the error boundary makes sure the errors are being logged to <code>error.log</code>.</p> <p>Exceptions can be thrown in the apps at runtime both as a result of a failing request to a service or on our side. The change made to the error system in this context was to distinguish between the request errors. Some data for some services are being considered to be essential for the apps to work, others are not. To make sure that not all fetching errors are being caught we have created a <code>queryErrorHandler</code> in <code>src/components/store.jsx</code>. The <code>queryErrorHandler</code> looks at the type of error/instance of error that is being thrown and decides if the Error Boundary should be used or not. At the moment of this writing there are two type of errors: critical and non-critical. The critical ones are being caught by the Error Boundary and the non-critical are only ending up in the error log and are not blocking the app.</p>"},{"location":"DPL-React/architecture/adr-008-error-handling/#consequences","title":"Consequences","text":"<p>By using the Portal system we have solved the problem about showing multiple errors instead of a single global one.</p> <p>By choosing to distinguish different error types by looking at their instance name we can decide which fetch errors should be blocking the app and which should not. In both cases the errors are being logged and we can trace them in our logs.</p>"},{"location":"DPL-React/architecture/adr-008-error-handling/#alternatives-considered","title":"Alternatives considered","text":"<p>None.</p>"},{"location":"DPL-React/architecture/adr-009-remove-airbnb-lint-rules/","title":"Removal of Airbnb Linting Rules","text":""},{"location":"DPL-React/architecture/adr-009-remove-airbnb-lint-rules/#context","title":"Context","text":"<p>Airbnb's linting rules have not been updated to support ESLint 9 and have not received significant updates for an extended period. This has led to a loss of confidence in their continued maintenance and relevance. Consequently, we began exploring alternative linting solutions and sought out the most widely recognized and respected linting rules.</p> <p>Determining the industry standard for linting rules is challenging due to the variety of opinions and preferences within the developer community. While some developers prefer the strictness and comprehensiveness of Airbnb's rules, others may favor the flexibility and simplicity of the <code>eslint:recommended</code> ruleset.</p>"},{"location":"DPL-React/architecture/adr-009-remove-airbnb-lint-rules/#decision","title":"Decision","text":"<p>After careful consideration, we have chosen to implement the <code>eslint:recommended</code> ruleset. This decision was based on its widespread recognition within the developer community, ensuring a dependable basis for our linting requirements. We removed the Airbnb linting rules from our ESLint configuration and supplemented it with custom rules to retain some of the beneficial practices previously enforced by Airbnb.</p>"},{"location":"DPL-React/architecture/adr-009-remove-airbnb-lint-rules/#consequences","title":"Consequences","text":"<ul> <li>Removal of Airbnb ESlint ruleset</li> <li>Adding recommended eslint ruleset - eslint:recommended</li> </ul>"},{"location":"DPL-React/architecture/adr-009-remove-airbnb-lint-rules/#alternatives-considered","title":"Alternatives Considered","text":"<ul> <li>Wait for Airbnb: Postpone the update to ESLint until Airbnb supports version 9.   This delay affects other dependent update tasks. The rationale is that Airbnb's   coding standard is well-established and respected in the industry.</li> <li>Temporarily drop Airbnb: Since Airbnb does not support ESLint v9, the linting   rules are not functioning correctly. This means deviations are not being caught.   If this is the case, we can proceed with eslint:recommended, wait for Airbnb and   effectively run without Airbnb for a period. When Airbnb ESLint v9 support is   available, we could hope is that we can adjust our toolchain to automatically   align the code with the current standard, making it easy to correct afterward.</li> </ul>"},{"location":"DPL-Go/","title":"Index","text":"<p>    Website for the public library that uses Next.js with the App Router for the frontend and Drupal for content management.      Drupal CMS can be accessed through Lagoon by generating a one-time login in the preferred environment.    </p>"},{"location":"DPL-Go/#urls","title":"URLs","text":"Description URL Demo site (may change) https://node.pr-1707.dpl-cms.dplplat01.dpl.reload.dk/ Demo site Drupal CMS (may change, login through lagoon) https://varnish.pr-1707.dpl-cms.dplplat01.dpl.reload.dk/"},{"location":"DPL-Go/#table-of-contents","title":"Table of contents","text":"<ul> <li>URLs</li> <li>Table of contents</li> <li>Getting started</li> <li>Prerequisites</li> <li>Setup</li> <li>Technical Overview</li> <li>Project structure</li> <li>git workflows</li> <li>Development</li> <li>UI components from shadcn/ui</li> <li>Tailwind</li> <li>Codegen</li> <li>Codegen types</li> <li>Custom types</li> <li>xState</li> <li>Config handling</li> <li>Storybook</li> <li>Cypress</li> <li>Test production database locally</li> <li>Deployment</li> <li>git branches and pull requests</li> <li>Create pull request</li> <li>Reviewing a PR</li> <li>Updating the demo site<ul> <li>Create a release tag in dpl-go based on sprint number</li> <li>Deploying a release</li> </ul> </li> <li>Quality assurance</li> <li>GitHub Workflows for quality assurance</li> <li>Developers</li> </ul>"},{"location":"DPL-Go/#getting-started","title":"Getting started","text":""},{"location":"DPL-Go/#prerequisites","title":"Prerequisites","text":"<ul> <li>Use the node version registered in the [<code>.nvm</code>] file</li> <li>Preferably managed by <code>nvm</code></li> <li>This project uses <code>yarn</code> make sure to have this installed globally on your machine</li> </ul>"},{"location":"DPL-Go/#setup","title":"Setup","text":"<ol> <li>Make sure you are using the correct Node version:</li> </ol> <pre><code>nvm use\n</code></pre> <ol> <li>Install dependencies:</li> </ol> <pre><code>yarn\n</code></pre> <ol> <li>Start the development server:</li> </ol> <p>We run the server in an experimental https state to not get blocked by CORS policy when developing locally.</p> <pre><code>yarn dev:https\n</code></pre> <p>The application is now running at https://localhost:3000</p> <ol> <li>Set up the Drupal CMS (dpl-cms) locally to access configuration variables for the Go app. Ensure the <code>NEXT_PUBLIC_GRAPHQL_SCHEMA_ENDPOINT_DPL_CMS</code> variable in the <code>.env.local</code> file points to the correct endpoint.</li> </ol>"},{"location":"DPL-Go/#technical-overview","title":"Technical Overview","text":"<ul> <li>Next.js with the App Router</li> <li>React</li> <li>React Query</li> <li>TypeScript</li> <li>shadcn/ui</li> <li>Tailwind CSS</li> <li>ESLint &amp; Prettier</li> <li>Storybook &amp; Chromatic</li> <li>cypress</li> <li>Vitest</li> </ul>"},{"location":"DPL-Go/#project-structure","title":"Project structure","text":"<p>In the project, you'll see the following folders and files:</p> File(s) Description __tests__/* Unit tests for the application components and utilities. Using vitest .github/* GitHub configuration files and workflows .storybook/* Configuration files and stories for Storybook .vscode.example/* Example settings and recommendations for Visual Studio Code workspace app/* Routes for the App Router components/global/* Components that should always be rendered on the page components/pages/* For page layout components used as children on routes (Useful when making API calls server-side on the route and render page layouts as child components) components/paragraphs/* Components named according to Drupal CMS conventions for editorial sections on a page components/shadcn/* Imported shadcn/ui components components/shared/* Reusable components that can be optionally used across various other components hooks/* Custom React hooks lagoon/* TODO: add description lib/* Library utilities and configurations lib/config/* Centralized access to environment variables and CMS configuration settings lib/graphql/* GraphQL-related utilities and configurations lib/graphql/fetchers/* Custom fetch functions used for fetching data through React Query lib/graphql/fragments/* GraphQL fragments used to define reusable pieces of data queries lib/graphql/generated/* Auto-generated GraphQL types and queries based on the GraphQL schema lib/graphql/queries/* Custom GraphQL queries used throughout the application lib/helpers/* Utility functions and helpers used across the application lib/machines/* State machines and related logic for managing complex state transitions through xstate lib/providers/* Context providers and related logic for managing global state and dependencies lib/rest/* REST API-related utilities and configurations lib/rest/publizon-api/* Custom functions and configurations for interacting with the Publizon API lib/session/* Session management utilities and configurations lib/shadcn/* Utilities for shadcn/ui components lib/types/* Manually added types used throughout the application public/* Non-code, unprocessed assets (fonts, icons, etc.) styles/* Global Tailwind CSS files .editorconfig/ Configuration file for maintaining consistent coding styles between different editors and IDEs .env.example Example environment variables file .env.local Local environment variables file, specific to your development environment (git ignored) .env.test Environment variables file for testing .eslintignore File specifying which files and directories to ignore by ESLint .eslintrc.json Configuration file for ESLint rules and settings .gitignore Specifies intentionally untracked files to ignore .nvmrc Node version manager configuration file .prettierignore File specifying which files and directories to ignore by Prettier codegen.ts Configuration file for generating code based on GraphQL schema components.json Configuration file for defining and managing component metadata (necessary when installing shadcn components) next.config.mjs Next.js configuration file orval.config.ts Configuration file for Orval, a tool for generating API clients from OpenAPI specifications package.json Contains metadata about the project and its dependencies, scripts, and other configurations postcss.config.mjs Configuration file for PostCSS, a tool for transforming CSS with JavaScript plugins (necessary for Tailwind to compile) tailwind.config.ts Tailwind CSS configuration file Taskfile.yml Task automation file used to define and run tasks tsconfig.json TypeScript configuration file vitest.config.ts Configuration file for Vitest yarn.lock Lockfile for Yarn, ensuring consistent installs across different environments"},{"location":"DPL-Go/#git-workflows","title":"git workflows","text":"File(s) Description accessibility-test.yml Runs accessibility tests using Axe and Lighthouse to ensure the application meets accessibility standards. chromatic.yml Runs Chromatic to visualize and test UI components in Storybook, ensuring that changes do not introduce visual regressions. eslint-check.yml Runs ESLint to check for code quality and adherence to coding standards. prettier-check.yml Runs Prettier to ensure code formatting consistency across the project. publish-source.yml Publishes the source code to the specified repository or platform. type-check.yml Runs TypeScript to check for type errors and ensure type safety across the project. unit-test.yml Runs unit tests using Vitest to ensure that individual components and utilities function correctly."},{"location":"DPL-Go/#development","title":"Development","text":""},{"location":"DPL-Go/#ui-components-from-shadcnui","title":"UI components from shadcn/ui","text":"<p>We use shadcn/ui to speed up the development of UI components. Imported components are located in /components/shadcn.</p> <p>If you're developing a component that requires additional logic beyond the initially installed component styles or logic, copy the imported code into a new component in the /components/shared folder. This ensures that components imported by shadcn do not replace previously installed components.</p>"},{"location":"DPL-Go/#tailwind","title":"Tailwind","text":"<p>We use Tailwind CSS to style the project. Tailwind's predefined classes are configured in the <code>tailwind.config.ts</code> file, which references CSS variables defined in the <code>globals.css</code> file. This ensures that CSS variables are accessible through both CSS and Tailwind classes when developing components. When creating new variables, add the style as a variable in the <code>globals.css</code> file and include this variable in the Tailwind config. Additionally, the Tailwind CSS IntelliSense extension provides autocomplete features for a smoother development experience.</p> <p>When introducing new classes, make sure to reuse existing ones and maintain consistency. Use the grid system for placing elements and apply the predefined spacing variables consistently.</p>"},{"location":"DPL-Go/#codegen","title":"Codegen","text":"<p>In this project, we use codegen to generate GraphQL types and queries, which helps streamline the development process and maintain a high level of code quality.</p>"},{"location":"DPL-Go/#codegen-types","title":"Codegen types","text":"<p>Codegen types are automatically generated TypeScript types based on the specific API schema. These types ensure type safety and provide autocompletion features when working with GraphQL queries and mutations.</p> <p>To generate the types, run the following command:</p> <pre><code>yarn codegen:all-rest-services\n</code></pre> <p>The custom functions and configurations for these services are located in the <code>lib/rest</code> directory.</p> <pre><code>yarn codegen:graphql\n</code></pre> <p>This will create or update the types in the <code>lib/graphql/generated</code> directory.</p> <pre><code>yarn codegen:publizon\n</code></pre> <p>The <code>lib/rest/publizon-api</code> directory contains functions and configurations for interacting with the Publizon API. This API is used to manage and retrieve information about digital publications.</p>"},{"location":"DPL-Go/#custom-types","title":"Custom types","text":"<p>Custom types are manually defined TypeScript types that are used throughout the application to ensure type safety and improve code readability.</p> <p>When creating a custom type, please write a <code>T</code> before the name of the type to refer to the variable as a type. This helps to avoid confusion between types and other functionalities.</p> <p>To add a custom type, create a new file in the <code>lib/types</code> directory and define your types using TypeScript's <code>type</code> or <code>interface</code> keywords.</p>"},{"location":"DPL-Go/#xstate","title":"xState","text":"<p>Read about xState here.</p>"},{"location":"DPL-Go/#config-handling","title":"Config handling","text":"<p>Read about configuration here.</p>"},{"location":"DPL-Go/#storybook","title":"Storybook","text":"<p>Storybook is an essential tool in our development workflow for several reasons:</p> <ol> <li>Component Isolation: It allows us to develop and test UI components in isolation, ensuring that each component works as expected without dependencies on the rest of the application.</li> <li>Visual Documentation: Storybook provides a visual representation of our components, making it easier for developers and designers to understand and collaborate on the UI.</li> <li>Automated Testing: With integrations like Chromatic, we can automate visual regression testing to catch UI changes and bugs early in the development process.</li> <li>Reusable Components: By documenting components in Storybook, we promote reusability and consistency across the application, reducing duplication and improving maintainability.</li> </ol> <p>To start Storybook, run the following command:</p> <pre><code>yarn storybook\n</code></pre> <p>This will launch the Storybook server, and you can view the component library in your browser at http://localhost:6006.</p> <p>We create Storybook stories strictly on a \"render component\" basis. This means focusing on smaller atomic components that are not specific to generated code.</p>"},{"location":"DPL-Go/#cypress","title":"Cypress","text":"<p>Cypress is an end-to-end testing framework that allows us to write and run tests for the application's core user journeys.</p> <p>To start Cypress, run the following commands:</p> <pre><code>yarn dev # Start the development server\nyarn cypress:run # Run all Cypress tests\n</code></pre>"},{"location":"DPL-Go/#test-production-database-locally","title":"Test production database locally","text":"<p>To test the production database locally, you need to download a copy of the production database to your local development environment. This allows you to debug issues and verify features using real data, while ensuring that your local changes do not affect the live environment.</p> <p>Follow these steps to test the production database locally:</p> <ol> <li>Obtain a database and if necessary a file dump and import it to local dpl-cms environment:</li> </ol> <p>Follow the guide in DPL-CMS:</p> <p>https://danskernesdigitalebibliotek.github.io/dpl-docs/DPL-CMS/local-development/?h=database#download-database-and-files-from-lagoon</p> <ol> <li> <p>Update CMS user credentials: In your local DPL-CMS, go to the \"People\" tab and find the user named <code>go_graphql</code>. Click \"Edit\" for this user. Update the password field to match the value of <code>NEXT_PUBLIC_GO_GRAPHQL_CONSUMER_USER_PASSWORD</code> from your <code>.env.local</code> file in this project.    Note: Occasionally, the password update may not take effect as expected. If you encounter this issue, repeat step 1 and try again.</p> </li> <li> <p>Start the application: Launch your development server as usual. The application should now display data from the imported production database.</p> </li> </ol>"},{"location":"DPL-Go/#notes","title":"NOTES","text":"<ul> <li>Before testing, clear all cookies from your browser to avoid potential issues caused by existing cookies.</li> <li>If you are testing login functionality, make sure the local domain is whitelisted for the relevant library in STIL.</li> <li>To configure \"adgangsplatform\" tokens and Unilogin for local development, run:   <pre><code>task dev:openid:configure &amp;&amp; task dev:unilogin:configure\n</code></pre>   This will set up authentication using materials from the Copenhagen library.</li> <li>As webmaster libraries can add their own modules, it can sometimes be necessary to download a copy of their file system as well as the database. The reason for this is, that a database import will not be successful if it can't find all modules.</li> </ul>"},{"location":"DPL-Go/#deployment","title":"Deployment","text":""},{"location":"DPL-Go/#git-branches-and-pull-requests","title":"git branches and pull requests","text":"<p>We follow a specific Git branching model as the project is currently in development mode and not yet live. The main branches are:</p> <ul> <li><code>main</code>: Used for deploying code to the demo environment. (In the future this will be the production branch)</li> <li>Feature branches are created from <code>main</code> and are named using the following convention:</li> </ul> <pre><code>DDFBRA-220-opstart-dokumentation\n</code></pre> <ul> <li><code>DDFBRA</code>: Workspace in Jira</li> <li><code>220</code>: Ticket number</li> <li><code>opstart-dokumentation</code>: Ticket title</li> </ul> <p>Gotha: When creating a branch, use the Jira tickets <code>create branch</code> button to create a branch that refers to the ticket.</p> <p>When a feature is complete, all test are passing and a review process has taken place, it is merged back into <code>main</code>.</p>"},{"location":"DPL-Go/#create-pull-request","title":"Create pull request","text":"<p>When creating a pull request, follow these steps:</p> <ol> <li>Create a new pull request <code>feature</code>-&gt;<code>main</code>.</li> <li>Add relevant information to the PR template. At a minimum, include a link to the ticket (if available) and a description.</li> <li>Resolve any merge conflicts, fix errors in workflows, and check Chromatic to ensure new changes do not cause conflicts in the app. The PR creator is responsible for these checks.</li> <li>Await approval from a relevant team member for the changes to the code.</li> <li>Merge the code to <code>main</code> and delete the feature branch to clean up.</li> </ol>"},{"location":"DPL-Go/#reviewing-a-pr","title":"Reviewing a PR","text":"<ol> <li>Review the code changes to ensure they meet the project's coding standards and requirements.</li> <li>Provide constructive feedback and request any necessary changes. Suggestions are optional for the creator to fix, while change requests must be addressed.</li> <li>Approve the PR if everything looks good, or request changes if there are issues that need to be addressed.</li> </ol>"},{"location":"DPL-Go/#updating-the-demo-site","title":"Updating the demo site","text":""},{"location":"DPL-Go/#create-a-release-tag-in-dpl-go-based-on-sprint-number","title":"Create a release tag in dpl-go based on sprint number","text":"<ol> <li>Navigate to the github project and go to the releases page</li> <li>Click Draft a new release</li> <li>Create a new release with the tag format <code>0.&lt;sprint_number&gt;.&lt;incremental_release_count&gt;</code> (e.g., for sprint 7, the tag would be <code>0.7.0</code>).</li> <li>This will trigger the <code>publish-source</code> workflow in the repository</li> </ol>"},{"location":"DPL-Go/#deploying-a-release","title":"Deploying a release","text":"<ol> <li>Navigate to the <code>dpl-cms</code> repository.</li> <li>Pull the latest changes from the <code>develop</code> branch locally.</li> <li>Switch to the <code>dpl-go-demo</code> branch and merge the <code>develop</code> branch into it.</li> <li>Update the <code>dpl-go</code> package version locally to the latest release and push the changes upstream.</li> <li>Edit the <code>Docker-compose.yml</code> file with the following changes:</li> </ol> <pre><code>node:\n  image: ghcr.io/danskernesdigitalebibliotek/dpl-go-node:0.&lt;sprint_number&gt;.&lt;incremental_release_count&gt;\n  labels:\n    lagoon.type: node\n    provenance: false\n</code></pre>"},{"location":"DPL-Go/#quality-assurance","title":"Quality assurance","text":"<p>Quality assurance (QA) is a critical aspect of our development process, ensuring that the application meets the required standards and functions correctly. By implementing robust QA practices, we can identify and address issues early, maintain code quality, and deliver a reliable product to users.</p> <p>As an overall rule of thumb, we add different levels of quality assurance depending on the given problem.</p>"},{"location":"DPL-Go/#github-workflows-for-quality-assurance","title":"GitHub Workflows for quality assurance","text":"<ol> <li> <p>Automated Unit Testing: Workflows like <code>unit-test.yml</code> run unit tests automatically, ensuring that individual components and utilities function correctly. This helps catch bugs early in the development process.</p> </li> <li> <p>End-to-end Testing: Workflows such as <code>e2e-test.yml</code> run end-to-end tests using Cypress to simulate user interactions and test the application's core user journeys. This helps ensure that the application and important features work as expected. This also helps catch bugs early in the development process.</p> </li> <li> <p>Code Quality Checks: Workflows such as <code>eslint-check.yml</code> and <code>prettier-check.yml</code> enforce coding standards and consistent formatting. This ensures that the codebase remains clean, readable, and maintainable.</p> </li> <li> <p>Type Safety: The <code>type-check.yml</code> workflow runs TypeScript checks to ensure type safety across the project. This helps prevent type-related errors and improves code reliability.</p> </li> <li> <p>Accessibility Testing: The <code>accessibility-test.yml</code> workflow runs accessibility tests on each Storybook Story using Axe through Playwright. This ensures that the application meets accessibility standards and provides a better user experience for all users.</p> </li> <li> <p>Visual Regression Testing: The <code>chromatic.yml</code> workflow runs Chromatic to visualize and test UI components in Storybook. This helps catch visual regressions and ensures that UI changes do not introduce unexpected issues.</p> </li> </ol>"},{"location":"DPL-Go/#developers","title":"Developers","text":"<ul> <li>Adam Antal - adam@reload.dk</li> <li>Mikkel Jakobsen - mikkel@reload.dk</li> <li>Thomas Gross Rasmussen - tgr@reload.dk</li> <li>Jacob Pihl - jacob@reload.dk</li> </ul>"},{"location":"DPL-Go/authentication/","title":"Authentication","text":""},{"location":"DPL-Go/authentication/#general","title":"General","text":"<p>There are two ways of logging into the Go application:</p> <ul> <li>Via Adgangsplatformen</li> <li>Via Unilogin</li> </ul>"},{"location":"DPL-Go/authentication/#go-session","title":"Go session","text":"<p>The Go session is maintained by using the iron-session tool. The session data is stored in a cookie encrypted and is only readable server side. The session architecture was decided upon as a part of developing the Unilogin login flow which is documented in an ADR in the docs/architecture section. The go session cookie is used for patrons logging in with either Unilogin or Adgangsplatformen and has two major attributes:</p> <p>isLoggedIn - can be either <code>true</code> or <code>false</code></p> <p>and</p> <p>type - can be either:</p> <ul> <li><code>anonymous</code></li> <li><code>unilogin</code></li> <li><code>adgangsplatformen</code></li> </ul> <p>The overall authorization behavior of the Go application is controlled by these parameters.</p>"},{"location":"DPL-Go/authentication/#go-session-type-cookie","title":"Go session type cookie","text":"<p>Because we need a different behavior of the application depending of the session type (\"unilogin\" or \"adgangsplatformen\") and we don't want to call our session endpoint every time we decided to create a cookie called \"go-session:type\". Since it is not sensitive data we can make it accessible both client and server side. The cookie is for instance used to decide whether we need to contacting our own Pubhub API or the Publizon adapter when requesting Publizon data.</p>"},{"location":"DPL-Go/authentication/#login","title":"Login","text":""},{"location":"DPL-Go/authentication/#login-via-unilogin","title":"Login via Unilogin","text":"<p>The login flow is mainly controlled via the openid-client package. It is a tool to ease the setup of the Oauth 2 flows. The decision behind the choice of tools for the login handling is described in an ADR in the docs/architecture section.</p> <pre><code>sequenceDiagram\n    actor Patron\n    participant Go\n    participant Unilogin Login\n    participant Unilogin WS\n    participant Adgangsplatformen\n    Patron-&gt;&gt;Go: Patron opens the Login sheet\n    Patron-&gt;&gt;Go: Clicks Unilogin login button\n    Go-&gt;&gt;Go: Go redirects to login auth route (/auth/login/unilogin)\n    Note over Go: See chapter: \"Building the Unilogin authorization url\"\n    Go-&gt;&gt;Go: Go builds an authorization url\n    Note over Go: code_verifier is used for validating authenticity of the redirect back from the Unilogin login\n    Go-&gt;&gt;Go: Stores code_verifier value in Go session cookie\n    Go-&gt;&gt;Unilogin Login: Go is redirecting to the external Unilogin form by using the authorization url\n    Unilogin Login-&gt;&gt;Go: After successful login the patron is redirected to the unilogin callback route (/auth/callback/unilogin)\n    Unilogin WS--&gt;&gt;Go: Go requests access token, refresh token and expire timestamps and validates the expected response\n    Note over Unilogin WS: Introspection data contains uniid and institution_ids of the user\n    Unilogin WS--&gt;&gt;Go: Go requests and validates introspection data from the access token\n    Note over Unilogin WS: Userinfo data contains the sub that in this case is a GUID\n    Unilogin WS--&gt;&gt;Go: Go requests and validates user info\n    Note over Go: See chapter: \"Unilogin login authorization check\"\n    Go--&gt;&gt;Go: Checks if user is authorized to log in\n    Go--&gt;&gt;Go: Saves the go session with tokens and user info\n    Go--&gt;&gt;Go: Saves the go session type cookie\n    Go--&gt;&gt;Go: Redirects the Patron to the user profile page</code></pre>"},{"location":"DPL-Go/authentication/#building-the-unilogin-authorization-url","title":"Building the Unilogin authorization url","text":"<p>In order to follow the Oauth2 standard and the Unilogin STIL specification an authorization url is constructed with the help of the openid-client tool.</p> <p>A PKCE code verifier is generated (the <code>GO_SESSION_SECRET</code> is used as salt).</p> <p>The code verifier is stored in the session for future validation of the authenticity of the request from Unilogin coming back from the external login form.</p> <p>And the code verifier is also used in order to create the code challenge needed as an url parameter for the authorization url.</p>"},{"location":"DPL-Go/authentication/#unilogin-login-authorization-check","title":"Unilogin login authorization check","text":"<p>As a part of the Unilogin flow when coming back form a successful login we check if the municipality id (<code>kommunenr</code>) of the first institution in the userinfo matches the one that is configured to the site (<code>UNILOGIN_MUNICIPALITY_ID</code>). If the id's are identical the user is allowed to login in otherwise a logout is forced both in the SSO and locally.</p>"},{"location":"DPL-Go/authentication/#login-via-adgangsplatformen","title":"Login via Adgangsplatformen","text":"<pre><code>sequenceDiagram\n    actor Patron\n    participant Go\n    participant CMS\n    participant CMS Graphql API\n    participant Adgangsplatformen\n    Note over CMS: The login url contains the route to the login route in the CMS&lt;br/&gt;and a url parameter (current_path which is an internal CMS url) is attached.&lt;br /&gt; current_path instructs the CMS where to go  after the external SSO login\n    CMS--&gt;&gt;Go: Go fetches the login url from the CMS\n    Patron-&gt;&gt;Go: Patron opens the Login sheet\n    Patron-&gt;&gt;Go: Clicks Adgangsplatformen login button\n    Go-&gt;&gt;CMS: Go redirects patron to /login at the CMS\n    Note over Adgangsplatformen: NB: The Adgangsplatformen Oauth flow&lt;br /&gt;is described in the dpl-cms documentation\n    CMS-&gt;&gt;Adgangsplatformen: Patron is sent to login form at Adgangsplatformen\n    Adgangsplatformen-&gt;&gt;CMS: After successful login the patron is redirected to the CMS\n    Note over CMS,CMS: The Go specific route&lt;br /&gt;(dpl_go.post_adgangsplatformen_login) in the CMS&lt;br /&gt;is specified via the current_path url parameter\n    CMS-&gt;&gt;CMS: The CMS redirects to the Go specific route\n    Note over Go: The callback endpoint in Go is at /auth/callback/adgangsplatformen\n    CMS-&gt;&gt;Go: The CMS redirects to a callback endpoint in Go\n    Note over CMS,Go: By passing the Drupal SESS* cookie in the header&lt;br/&gt;Go is authorized and identified as the Drupal Patron user\n    CMS Graphql API--&gt;&gt;Go: Go fetches the user token from CMS API\n    Go-&gt;&gt;Go: Go instantiates a go session with the user token attached\n    Go-&gt;&gt;Go: Patron is redirected to the user profile page</code></pre>"},{"location":"DPL-Go/authentication/#logout","title":"Logout","text":"<p>When a user click logout we need to handle that the current session either can be:</p> <ul> <li>Adgangsplatformen</li> <li>Unilogin</li> <li>Anonymous</li> <li>In a, for some reason, broken state</li> </ul> <p>This chart shows how we handle the various types:</p> <pre><code>flowchart TD\n    UserClicksLogout[User clicks logout] --&gt;\n    RedirectToLogoutEndpoint[User gets redirected to logout endpoint] --&gt;\n    SessionExist{Is there an active go-session?}\n    SessionExist --&gt; |Yes| CheckType{Check type}\n\n    CheckType --&gt; IsUnknown[Unknown]\n    CheckType --&gt; IsUnilogin[Unilogin]\n    CheckType --&gt; IsAdgangsplatformen[Adgangsplatformen]\n\n    IsUnknown --&gt; DestroySession[Destroy Go session - and id token]\n\n    IsAdgangsplatformen --&gt; DestroySessionBeforeRedirect[\n      Destroy Go session - and id token, if it exist\n    ]\n    DestroySessionBeforeRedirect ---&gt; RedirectToAdgangsplatformenLogout[\n      Redirect to CMS Adgangsplatformen logout - with current_path url arg\n    ]\n    RedirectToAdgangsplatformenLogout --&gt; LogoutRemoteAdgangsplatformen[\n      Logging out of Adgangsplatformen remotely\n    ]\n    LogoutRemoteAdgangsplatformen ---&gt; RedirectBackToGo[\n      CMS redirects back to Go frontpage\n    ]\n\n    IsUnilogin --&gt; CallUniloginLogout[Call Unilogin Logout service]\n\n    CallUniloginLogout --&gt; DestroySession[Destroy Go session - and id token]\n\n    DestroySession --&gt; RedirectToFrontpage[Redirect to frontpage]\n\n    SessionExist --&gt;|No| RedirectToFrontpage</code></pre>"},{"location":"DPL-Go/authentication/#token-handling","title":"Token handling","text":""},{"location":"DPL-Go/authentication/#token-types","title":"Token types","text":"<p>We have four different token types:</p> <ul> <li>Access token</li> <li>Refresh token</li> <li>Id token</li> <li>Library token</li> </ul>"},{"location":"DPL-Go/authentication/#access-token","title":"Access token","text":"<p>Access tokens exist in both Unilogin and Adgangsplatformen session. The Unilogin access token is only used to get user information as a part of the login process but apart from that is is not used in the rest of the application.</p> <p>The Adgangsplatformen access token is a part of the <code>go-session</code> iron-session cookie. Whenever a fetch is fired and service requested needs an Adgangsplatformen access token as bearer token, the access token is fetched from the internal <code>/auth/session</code> route.</p>"},{"location":"DPL-Go/authentication/#refresh-token","title":"Refresh token","text":"<p>Is used as a part of the Unilogin session. When access token is expired the refresh token is used to issue a new access token.</p>"},{"location":"DPL-Go/authentication/#id-token","title":"Id token","text":"<p>Is used when a user logs out of an active Unilogin session to terminate the remote SSO session. See the <code>handleUniloginLogout()</code> function.</p>"},{"location":"DPL-Go/authentication/#library-token","title":"Library token","text":"<p>The documentation of the library token does not really belong here since it is not a part of the session or authentication process. But since we document all the token here it is worth mentioning.</p> <p>The library token is fetched regularly in the middleware and set as a cookie. Whenever it expired a new library token is fetched and the cookie is updated. As mentioned before it is a separate system and not coupled to the session handling.</p>"},{"location":"DPL-Go/expexted-session-behaviour/","title":"Expected session behaviour","text":""},{"location":"DPL-Go/expexted-session-behaviour/#the-purpose-of-this-document","title":"The purpose of this document","text":"<p>This document both aims to be a help for developers if unexpected events occurs around user session handling in production sites but also serves as documentation of intended session behaviour.</p>"},{"location":"DPL-Go/expexted-session-behaviour/#terminology","title":"Terminology","text":"<ul> <li>Primary Library - The main library site (FB CMS)</li> <li>Go Site - the Go web site residing as a subsite of the Primary Library</li> <li>Session types - The current available session types are: Adgangsplatformen,   Unilogin and anonymous. anonymous is the default type.</li> </ul>"},{"location":"DPL-Go/expexted-session-behaviour/#logged-in-identification","title":"Logged-in identification","text":"<p>Primary Library: When a user is logged in, the name of the user is written under the user icon in the header.</p> <p>Go Site: You can see a user is logged in by clicking on the user icon in the header.</p> <ul> <li>If a login sheet is shown with button the user is NOT logged.</li> <li>If you are redirected to the profile page the user is logged in.</li> </ul>"},{"location":"DPL-Go/expexted-session-behaviour/#various-session-rules","title":"Various session rules","text":"<ul> <li>At the Go Site it is NOT possible to be logged in with an Adgangsplatformen   and a Unilogin session at the same time.</li> <li>The Unilogin session type only exists at the Go Site</li> <li>The Adgangsplatformen session type can be used on both sites and is shared   between the sites.</li> </ul>"},{"location":"DPL-Go/expexted-session-behaviour/#user-stories","title":"User stories","text":""},{"location":"DPL-Go/expexted-session-behaviour/#shared-adgangsplatformen-session-between-primary-library-and-go-site","title":"Shared Adgangsplatformen session between Primary Library and Go Site","text":"<p>The Adgangsplatformen session is shared between the Primary Library site and Go.</p> <p>Here are the various scenarios:</p>"},{"location":"DPL-Go/expexted-session-behaviour/#logging-into-primary-library-and-is-automatically-logged-into-go-site","title":"Logging into Primary Library and is automatically logged into Go Site","text":"<ul> <li>A user logs into to the Primary Library</li> <li>The user identifies that it is logged in</li> <li>The user navigates to the Go Site</li> <li>The user identifies that is logged in at the Go Site too</li> </ul>"},{"location":"DPL-Go/expexted-session-behaviour/#logging-into-go-site-and-is-automatically-logged-into-the-primary-library","title":"Logging into Go Site and is automatically logged into the Primary Library","text":"<ul> <li>A user logs into to the Go Site with Adgangsplatformen</li> <li>The user identifies that it is logged in</li> <li>The user navigates to the Primary Library</li> <li>The user identifies that is logged in at the Primary Library too</li> </ul>"},{"location":"DPL-Go/expexted-session-behaviour/#logging-into-go-site-with-either-adgangsplatformen-or-unilogin","title":"Logging into Go site with either Adgangsplatformen or Unilogin","text":"<p>This works similar for both of the Adgangsplatformen and Unilogin session types:</p> <ul> <li>A user logs into to the Go Site with Adgangsplatformen</li> <li>The user identifies that it is logged in</li> <li>The user clicks on the user icon in the header</li> <li>The user is now redirected to the user profile page</li> <li>The user is not able to switch to the Unilogin session before the use logs out   by clicking at the \"Log out\" button on the user profile page</li> </ul>"},{"location":"DPL-Go/expexted-session-behaviour/#two-different-sessions-between-go-site-and-the-primary-library","title":"Two different sessions between Go Site and the Primary Library","text":"<p>In one scenario it is possible to have two different sessions between the two sites:</p> <ul> <li>A user logs into to the Go Site with Unilogin</li> <li>The user identifies that it is logged in</li> <li>The user navigates to the Primary Library</li> <li>The user logs in to the Primary Library with Adgansgplatformen</li> <li>The user identifies that it is logged in</li> <li>The user navigates back to the Go Site</li> <li>The user identifies that it is logged in with Unilogin (?)</li> <li>The user navigates back to the Primary Library</li> <li>The user identifies that it is logged in with Adgangsplatformen</li> </ul> <p>(?) Unless the user waited too long with going back to the Go Site and the Unilogin session ran out.</p>"},{"location":"DPL-Go/architecture/adr-001-configuration/","title":"Configuration","text":""},{"location":"DPL-Go/architecture/adr-001-configuration/#context","title":"Context","text":"<p>We have several places where configuration come from:</p> <ul> <li>Configuration from external API's</li> <li>Static local configuration</li> </ul> <p>We wanted a unified way of handling configuration that addresses those (and possibly future) configuration sources. This does not include environment variables - they by their own helper functions from <code>env.ts</code>. The reasons why is listed in the \"Consequences\" section of this document.</p>"},{"location":"DPL-Go/architecture/adr-001-configuration/#decision","title":"Decision","text":"<p>We decided to make our own configuration system that uses a plugin-system we call resolvers. They are placed in our centralized directory <code>lib</code> directory: <code>/lib/config/resolvers</code>.</p> <p>A resolver can either be a flat value like:</p> <pre><code>const search = {\n  \"search.item.limit\": 12,\n  \"search.offset.initial\": 0,\n  \"search.param.initial\": 0,\n  \"search.facet.limit\": 100,\n...\n</code></pre> <p>...or even a asynchronous function:</p> <pre><code>{\n  \"service.unilogin.api.url\": async () =&gt; {\n    if (getEnv(\"UNILOGIN_API_URL\")) {\n      return getEnv(\"UNILOGIN_API_URL\")\n    }\n\n    const config = await getDplCmsUniloginConfig()\n    if (config?.unilogin_api_url) {\n      return config?.unilogin_api_url\n    }\n  },\n}\n</code></pre> <p>NOTE:</p> <p>With configuration that is coming from an asynchronous resolver function and is dependant on external systems you MUST specify an environment variable that possibly can overwrite what is coming from outside (like in the async example).</p> <p>In that way it is possible in eg. in tests, development and CI to overwrite the configuration.</p>"},{"location":"DPL-Go/architecture/adr-001-configuration/#alternatives-considered","title":"Alternatives considered","text":"<p>We did not look into alternatives.</p>"},{"location":"DPL-Go/architecture/adr-001-configuration/#consequences","title":"Consequences","text":"<p>With the new configuration system we:</p> <ul> <li>Do not need to know where the configuration comes from when we refer it.</li> <li>Can have one place where we control the error handling</li> <li>The configuration is typed so we know what is available</li> </ul>"},{"location":"DPL-Go/architecture/adr-002-xstate/","title":"Xstate","text":""},{"location":"DPL-Go/architecture/adr-002-xstate/#context","title":"Context","text":"<p>We wanted a methodology of handling state when we have a feature set with a growing number of different states, transitions and context.</p> <p>Also we wanted to get acquainted with Xstate both because of its principle of using a state tree and because of it's possibility of visualizing the various scenarios/flows a user could go through.</p>"},{"location":"DPL-Go/architecture/adr-002-xstate/#decision","title":"Decision","text":"<p>The search page was growing into being a problematic scenario as described above with multiple elements and connected states to be managed:</p> <ul> <li>Searching</li> <li>Loading more results</li> <li>Filtering and loading possible filters</li> <li>Linking to a search/filtering</li> </ul> <p>So we decided to implement the current early version of the search in Xstate in order to get a transparent state tree controlling the different states and transition to other states.</p>"},{"location":"DPL-Go/architecture/adr-002-xstate/#alternatives-considered","title":"Alternatives considered","text":"<p>Other state handlers where considered:</p> <ul> <li>Zustand</li> <li>Redux</li> </ul>"},{"location":"DPL-Go/architecture/adr-002-xstate/#consequences","title":"Consequences","text":"<p>Common to the alternatives considered is the fact that they do have the concept of a state tree controlling which transitions are available at the various levels.</p> <p>By having all the states and possible transitions between them in a Xstate machine we have a predictable way of treating the various cases/flows a user can go through.</p> <p>Also Xstate has powerful tools in order to handle side effects of the machine/actor. One example is the event handlers which we use for listening if a filter was toggled. When a filter is toggled we can either set or remove a query parameter accordingly.</p> <p>Even machine/actors can interact with each other, but let's see if we will ever need that complexity.</p>"},{"location":"DPL-Go/architecture/adr-003-error-boundaries/","title":"Error boundaries","text":""},{"location":"DPL-Go/architecture/adr-003-error-boundaries/#context","title":"Context","text":"<p>We wanted to implement error boundaries in our GO application to achieve the graceful error handling, component isolation, debugging and logging, improved user experience.</p>"},{"location":"DPL-Go/architecture/adr-003-error-boundaries/#decision","title":"Decision","text":"<p>We decided to implement an error boundary wrapper specifically for paragraphs, named <code>ParagraphErrorBoundary</code>. This ensures that if a paragraph encounters an error, the issue is contained within that specific section of the site, preventing it from affecting the rest of the application.</p> <p>The implementation of error boundaries will be an ongoing process. We will integrate them progressively, adding them to components where they provide the most benefit as we continue to develop and enhance the site.</p>"},{"location":"DPL-Go/architecture/adr-003-error-boundaries/#alternatives-considered","title":"Alternatives considered","text":"<p>none</p>"},{"location":"DPL-Go/architecture/adr-003-error-boundaries/#consequences","title":"Consequences","text":""},{"location":"DPL-Go/architecture/adr-003-error-boundaries/#graceful-error-handling","title":"Graceful Error Handling","text":"<p>Error boundaries allows us to catch JavaScript errors anywhere in the component tree, log those errors, and display a fallback UI instead of crashing the entire application. This ensures a better user experience by preventing the app from breaking completely.</p>"},{"location":"DPL-Go/architecture/adr-003-error-boundaries/#component-isolation","title":"Component Isolation","text":"<p>By wrapping specific components with error boundaries, we can isolate errors to those components. This means that an error in one part of the UI won't affect other parts, allowing the rest of the application to continue functioning normally.</p>"},{"location":"DPL-Go/architecture/adr-003-error-boundaries/#debugging-and-logging","title":"Debugging and Logging","text":"<p>Error boundaries provide a way to log errors for debugging purposes. We can at some point use them to send error reports to a logging service, which helps in monitoring and fixing issues more efficiently.</p>"},{"location":"DPL-Go/architecture/adr-003-error-boundaries/#fallback-ui","title":"Fallback UI","text":"<p>When an error is caught by an error boundary, we can display a fallback UI. This could be a simple error message, a \"Something went wrong\" page, or any custom UI that informs the user about the issue without exposing technical details.</p>"},{"location":"DPL-Go/architecture/adr-003-error-boundaries/#improved-user-experience","title":"Improved User Experience","text":"<p>By handling errors gracefully and providing meaningful feedback to users, error boundaries contribute to a smoother and more reliable user experience. Users are less likely to encounter a completely broken application.</p>"},{"location":"DPL-Go/architecture/adr-004-env-variables/","title":"Env variables","text":""},{"location":"DPL-Go/architecture/adr-004-env-variables/#context","title":"Context","text":"<p>We needed unified way to handle type safe environment variables in our application. We often ran into issues where the application would break because of missing environment variables, but it was not obvious which one. By using our own configuration system we can validate the environment variables on startup and get a clear error message with the name of the environment variable that is missing. This also gives autocompletion and we can catch any accidental renaming of environment variables.</p>"},{"location":"DPL-Go/architecture/adr-004-env-variables/#decision","title":"Decision","text":"<p>We decided to make two helpers for getting environment variables:</p> <ul> <li><code>getEnv</code> for getting public environment variables on both the server and the client</li> <li><code>getServerEnv</code> for getting sensitive server-only environment variables</li> </ul> <p>By using Zod we can define a schema for the environment variables and what content we expect of them.</p> <p>A new environment variable can be added by adding it to the <code>getEnvs</code> function and then extending the <code>EnvSchema</code> or <code>EnvServerSchema</code> with the new variable.</p> <p>Example:</p> <pre><code>function getEnvs() {\n  return {\n    NEW_ENV: process.env.NEXT_PUBLIC_NEW_ENV,\n    SOME_API_KEY: process.env.SOME_API_KEY,\n  }\n}\n</code></pre> <pre><code>const EnvSchema = z.object({\n  NEW_ENV: z.string().refine(validateUrl), // validates that the env is a valid url\n})\n</code></pre> <p>and</p> <pre><code>const EnvServerSchema = z.object({\n  SOME_API_KEY: z.string().optional(), // use optinal if the variable can be omitted - like in cases where we override fetched config\n})\n</code></pre> <p>Then use it in your code like this:</p> <pre><code>const apiKey = getServerEnv(\"SOME_API_KEY\")\n\nconst result = await fetchDataFromExternalAPI(apiKey)\n</code></pre> <p>and</p> <pre><code>console.log(getEnv(\"NEW_ENV\"))\n</code></pre>"},{"location":"DPL-Go/architecture/adr-004-env-variables/#alternatives-considered","title":"Alternatives considered","text":"<p>We did not look into alternatives.</p>"},{"location":"DPL-Go/architecture/adr-004-env-variables/#consequences","title":"Consequences","text":"<p>With the new helper functions it is crucial that we no longer use <code>process.env</code> directly in our code. As this would defeat the purpose of having a type safe system.</p>"},{"location":"DPL-Go/architecture/adr-005-dual-login-and-publizon-adapter/","title":"Dual Login and Publizon Adapter","text":""},{"location":"DPL-Go/architecture/adr-005-dual-login-and-publizon-adapter/#context","title":"Context","text":"<p>Our application supports two different authentication mechanisms for users:</p> <ul> <li>Adgangsplatformen \u2014 used by library patrons.</li> <li>Unilogin \u2014 a login used by students, particularly children in schools.</li> </ul> <p>Depending on which method is used, the session is established differently and includes a session token that is used to authorize requests to external services.</p> <p>A key integration in our system is the Publizon Adapter (see documentation here), a service that acts as a middle layer between us and another Publizon API that only works server-side as a SOAP service. See documentation here. We found out that SOAP services are used in this server side API by reverse-engineering the Ereolen GO (current version of the app that our project will be replacing) functionality using Unilogin users. The adapter provides endpoints we use to achieve various things, and proxys requests further to the server side SOAP API. An example of Publizon adapter endpoints that we use in our app:</p> <ul> <li><code>GET /v1/user/loans</code> \u2014 check current user loans and quotas.</li> <li><code>GET /v1/products/:identifier</code> \u2014 determine whether a material is cost-free (thus not affecting quotas).</li> </ul> <p>Originally, the Publizon Adapter was only built to work with tokens from Adgangsplatformen. As such, when a request to the Publizon Adapter is made using a bearer token from a Unilogin user session, they are rejected.</p> <p>We needed a way to support Publizon calls for both login types and ensure our app could keep same functionality in either case.</p>"},{"location":"DPL-Go/architecture/adr-005-dual-login-and-publizon-adapter/#decision","title":"Decision","text":"<p>We decided to maintain two parallel integrations Publizon services:</p> <ul> <li>For Adgangsplatformen users, we use the official external Publizon Adapter hosted by DBC at <code>https://pubhub-openplatform.dbc.dk/</code>.</li> <li>For Unilogin users, we created a local Publizon Adapter, hosted internally in our Next.js backend. This adapter forwards requests to Publizon Pubhub on behalf of the user, as that SOAP service only supports server-to-server communication.</li> </ul> <p>To facilitate this, we:</p> <ul> <li>Generated a second set of client code for our local adapter using Orval, matching the shape of the existing external adapter.</li> <li>Created utility hooks that conditionally select the correct query/mutation hook based on session type.</li> </ul> <p>For example:</p> <pre><code>const useGetV1UserLoans = withSessionType((cookieType: TSessionType) =&gt; {\n  if (cookieType === \"unilogin\") {\n    return useGetV1UserLoansLocalAdapter()\n  }\n  return useGetV1UserLoansAdapter()\n})\n</code></pre> <p>These wrapper hooks are used in components when calling endpoints like <code>/v1/user/loans</code>, so that the correct adapter (local or external) is queried transparently depending on the session.</p> <p>Note: <code>withSessionType</code> is a custom utility that internally uses <code>useSession()</code> to extract the login type (\"unilogin\" or \"adgangsplatform\") and then returns the result of the appropriate hook.</p> <p>This structure allows us to:</p> <ul> <li>Separate external and internal adapter concerns.</li> <li>Maintain type safety by relying on generated hooks for both services.</li> <li>Avoid dynamic logic in the <code>fetcher</code>, where <code>useSession()</code> cannot be called (hooks are not usable outside React).</li> </ul>"},{"location":"DPL-Go/architecture/adr-005-dual-login-and-publizon-adapter/#alternatives-considered","title":"Alternatives considered","text":""},{"location":"DPL-Go/architecture/adr-005-dual-login-and-publizon-adapter/#dynamic-endpoint-switching-inside-fetcher","title":"Dynamic endpoint switching inside <code>fetcher</code>","text":"<p>We considered dynamically switching between the external and internal adapter URLs within a custom <code>fetcher</code> function based on session type. However, this proved infeasible, because the session type can only be determined by calling <code>useSession()</code>, which is a React hook and cannot be used outside components/hooks.</p>"},{"location":"DPL-Go/architecture/adr-005-dual-login-and-publizon-adapter/#consequences","title":"Consequences","text":"<ul> <li>We now maintain two sets of generated code for Publizon Adapter calls: one for the external adapter and one for the local adapter.</li> <li>All usage of these adapters must go through a wrapper hook (like <code>useGetV1UserLoans</code>) that abstracts away the logic of which session type is active.</li> <li>There is a slight duplication in generated code, but it ensures clarity, type safety, and avoids runtime conditionals in awkward places.</li> <li>This setup is extensible if more adapters or session types ever need to be supported \u2014 the pattern of routing based on session type can scale.</li> <li>We avoid violating React hook rules by containing all session-related logic within components or custom hooks.</li> </ul>"},{"location":"DPL-Go/architecture/adr-006-mock-data/","title":"Data mocking","text":""},{"location":"DPL-Go/architecture/adr-006-mock-data/#context","title":"Context","text":"<p>We needed to find a unified way to use mocked data inside Cypress tests and API-requests in Storybook. The solution had to offer type safety as our generated graphql services have shown to change sporadically. Going forward the usage of simple .json files would not suffice, as issues within would only showup when running into React errors, as neither Cypress or Storybook \"knows\" what the data is supposed to look like.</p>"},{"location":"DPL-Go/architecture/adr-006-mock-data/#decision","title":"Decision","text":"<p>The decision fell upon Fishery which is a small JS tool that allows the creation of data factories, where atomic objects are created against a given type and inherited to create a given data structure, that matches responses of endpoints in the application. Fishery offers a well-documented API and some powerful chaining abilities, which makes it very flexible for defining advanced data factories.</p>"},{"location":"DPL-Go/architecture/adr-006-mock-data/#usage","title":"Usage","text":"<pre><code>// A simple userFactory defined with mocked data\n// while building we can provide overrides for edge cases\n// factories/user.ts\nimport { Factory } from \"fishery\"\n\nimport { User } from \"../my-types\"\nimport postFactory from \"./post\"\n\nconst userFactory = Factory.define&lt;User&gt;(({ sequence }) =&gt; ({\n  id: sequence,\n  name: \"Rosa\",\n  address: { city: \"Austin\", state: \"TX\", country: \"USA\" },\n  posts: postFactory.buildList(2),\n}))\n\nconst user = userFactory.build({\n  name: \"Susan\",\n  address: { city: \"El Paso\" },\n})\n\nuser.name // Susan\nuser.address.city // El Paso\nuser.address.state // TX (from factory)\n</code></pre>"},{"location":"DPL-Go/architecture/adr-006-mock-data/#alternatives-considered","title":"Alternatives considered","text":"<p>We did not actively look into other JS based alternatives, but briefly considered if it would cause too much overhead, as it requires more time to define a Factory than a simple .json file</p>"},{"location":"DPL-Go/architecture/adr-006-mock-data/#consequences","title":"Consequences","text":"<p>Advanced API responses WILL take more time to define as factories, because we can only benefit from Fishery, if data is split up into reusable atomic objects. But it is required that each developer takes their time to define a Factory, as it will benefit adding changes in the future.</p>"},{"location":"DPL-Go/architecture/adr-007-server-mocking/","title":"Server side mocking","text":""},{"location":"DPL-Go/architecture/adr-007-server-mocking/#context","title":"Context","text":"<p>We needed to find a good way to handle server side mocking as a part of Cypress tests in Nextjs. The goal was to have a tool which could intercept requests made from server side functions, which could not be intercepted with cy.intercept() It should be possible to write custom intercept commands directly in a Cypress test and use a Fishery factory for its response data. It was a must that the tool could intercept both REST, SOAP and GraphQL requests.</p>"},{"location":"DPL-Go/architecture/adr-007-server-mocking/#decision","title":"Decision","text":"<p>MockTTP is a byproduct of the Open Source software HTTP Toolkit, which allows very advanced request intercept and rewrites. MockTTP can be run inside Node.js and change response output on the fly, which makes it a good option, because it allows different responses to the same endpoint while running a Cypress test.</p>"},{"location":"DPL-Go/architecture/adr-007-server-mocking/#usage","title":"Usage","text":"<p>While some tools listens for all requests in the mocked application, MockTTP rely on requests to be routed through its HTTP server, which is available on port 9000 after starting the server.</p> <p>Using the mock server directly (MockApiServer()) should not be needed, as a range custom Cypress commands has already been defined. These should cover most use cases when used together with Cypress.</p> <p>MockTTP will startup together with Cypress on <code>yarn cypress:open</code>, and any handled or unhandled request will be logged to the console.</p> <p>Example:</p> <pre><code>// intercept server side GraphQL Query with a matching operation name\n// return response as Fishery factory\ncy.mockServerGraphQLQuery({\n  operationName: \"getAdgangsplatformenLibraryToken\",\n  data: GetAdgangsplatformenLibraryToken.build(),\n})\n</code></pre> <pre><code>// intercept server side POST request on matching path (ignores any query params)\n// return response as Fishery factory\ncy.mockServerRest({\n  method: \"POST\",\n  path: \"/introspect\",\n  data: introspection.build(),\n})\n</code></pre>"},{"location":"DPL-Go/architecture/adr-007-server-mocking/#alternatives-considered","title":"Alternatives considered","text":"<p>As a part of choosing MockTTP, the more widely used and known package MSW was also tested. After discovering that MSW works very poorly with Nextjs, as it only intercept some requests, we decided to look elsewhere. https://github.com/mswjs/examples/pull/101</p>"},{"location":"DPL-Go/architecture/adr-007-server-mocking/#consequences","title":"Consequences","text":"<p>MockTTP is unfortunately not a very well-documented tool, as its intended use is mostly as a part of HTTP Toolkit. It does have some well-defined functions and type safety, but don't expect any thorough API documentation.</p>"},{"location":"DPL-Go/architecture/adr-008-third-party-service-caching/","title":"3rd Party Service Cache Handling in Go","text":""},{"location":"DPL-Go/architecture/adr-008-third-party-service-caching/#context","title":"Context","text":"<p>We wanted to address performance concerns by speeding up the delivery of data and thereby improve the user experience.</p>"},{"location":"DPL-Go/architecture/adr-008-third-party-service-caching/#decision","title":"Decision","text":"<p>Go fetches data from several 3rd party services, and all the requests uses the React Query QueryClient - either server side or client side.</p> <p>Both query clients make use of the function <code>getQueryClientStaleTime()</code> which is, at the moment of this writing, set to 1 minute. Unless the queryclient is told to invalidate certain query keys the cache will live for that amount of time.</p>"},{"location":"DPL-Go/architecture/adr-008-third-party-service-caching/#usage","title":"Usage","text":""},{"location":"DPL-Go/architecture/adr-008-third-party-service-caching/#implementation-examples","title":"Implementation Examples","text":""},{"location":"DPL-Go/architecture/adr-008-third-party-service-caching/#react-query-setup","title":"React Query Setup","text":"<pre><code>// Server-side QueryClient\nconst getQueryClient = cache(\n  () =&gt;\n    new QueryClient({\n      defaultOptions: {\n        queries: {\n          staleTime: getQueryClientStaleTime(), // 1 minute in production, 0 in dev\n        },\n      },\n    })\n)\n\n// Client-side QueryClient\nconst [client] = useState(\n  new QueryClient({\n    defaultOptions: {\n      queries: {\n        staleTime: getQueryClientStaleTime(),\n      },\n    },\n  })\n)\n</code></pre>"},{"location":"DPL-Go/architecture/adr-008-third-party-service-caching/#3rd-party-service-queries","title":"3rd Party Service Queries","text":"<pre><code>// Publizon get user loans example\nconst { data: dataLoans, isLoading: isLoadingLoans } = useGetV1UserLoans()\n</code></pre>"},{"location":"DPL-Go/architecture/adr-008-third-party-service-caching/#cache-invalidation","title":"Cache Invalidation","text":""},{"location":"DPL-Go/architecture/adr-008-third-party-service-caching/#manual-invalidation","title":"Manual Invalidation","text":"<pre><code>// In React Query\nqueryClient.invalidateQueries({ queryKey: getGetV1UserLoansAdapterQueryKey() })\n</code></pre>"},{"location":"DPL-Go/architecture/adr-008-third-party-service-caching/#alternatives-considered","title":"Alternatives considered","text":"<p>We could make use of the cache mechanisms in NextJs. But we like the ease of defining the cache once at the Query Client level and let React Query handle the rest.</p>"},{"location":"DPL-Go/architecture/adr-008-third-party-service-caching/#consequences","title":"Consequences","text":"<p>The lifetime of the cache implemented in React Query is very low. The cache is primarily added in order to improve the performance for the end user because we do not have specific demands from the service providers to keep the request frequency low.</p> <p>Although we have a short cache life time on the 3rd party services, we should be aware of invalidating the relevant cache(s) upon data mutations and on log out, to keep data fresh.</p>"},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/","title":"FB CMS Content and Configuration Cache Handling in Go","text":""},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#context","title":"Context","text":"<p>We wanted to address performance concerns specifically related to FB CMS that delivers page data and configuration for Go. The FB CMS serves as the primary content management system providing:</p> <ul> <li>Dynamic page content (category pages, articles, etc.)</li> <li>Application configuration data</li> </ul> <p>Reducing load on FB CMS while ensuring content freshness when editors make changes was a key requirement.</p>"},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#decision","title":"Decision","text":"<p>Using cache in an application is not a trivial matter. The caching layer can, by accident, save a malformed state. Glitches, eg. network errors, can result in the cache not being renewed.</p> <p>By having a simple model and clear rules for the caching strategy it can help troubleshooting/debugging when things go wrong.</p> <p>So in our model caching is implemented at the request level using Next.js's \"use cache\" directive in functions. This approach allows for targeted cache invalidation strategies using cache tags.</p> <p>In order to make use of \"use cache\" we had to use Next 15's Canary version track that is used for experimental features. More about that in the \"Consequences\" section.</p>"},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#page-caching","title":"Page Caching","text":"<p>When requesting page data for the first time, eg. a category page, data is being stored server side, in Go's (Next.js's) cache. The cache is saved in memory runtime and is stored infinitely. The cache is marked with cache tags that are delivered as a part of the response from FB CMS. If an editor followingly makes a change to the category a revalidation request is sent from FB CMS to the Go application, which removes the data from the cache.</p>"},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#configuration-caching","title":"Configuration Caching","text":"<p>The application relies on configuration that is administered in FB CMS. We do NOT make use of cache tags in the configuration fetching but still we make use of the \"use cache\" directive. According to the Next.js documentation the default cache lifetime should be around 15 minutes before it is revalidated.</p>"},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#usage","title":"Usage","text":""},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#implementation-examples","title":"Implementation Examples","text":""},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#page-caching-with-cache-tags","title":"Page Caching with Cache Tags","text":"<pre><code>// Category page example\nasync function getPage(slug: string[]) {\n  \"use cache\"\n  const {\n    go: { cacheTags },\n    ...data\n  } = await loadPageData({\n    contentPath: slug.join(\"/\"),\n    type: \"category\",\n  })\n\n  if (cacheTags) {\n    cacheTag(...cacheTags)\n  }\n\n  return { go: { cacheTags }, ...data }\n}\n</code></pre>"},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#configuration-caching_1","title":"Configuration Caching","text":"<pre><code>const getDplCmsPrivateConfigData = async () =&gt; {\n  \"use cache\"\n  // Default cache time: ~15 minutes\n  try {\n    const data = await queryDplCmsPrivateConfig()\n    return privateConfigSchema.parse(data)\n  } catch {\n    // Fallback configuration\n    return defaultConfig\n  }\n}\n</code></pre>"},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#cache-invalidation","title":"Cache Invalidation","text":""},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#revalidation-endpoint","title":"Revalidation Endpoint","text":"<p>FB CMS triggers cache invalidation by calling:</p> <pre><code>GET /api/cache/revalidate?secret={DRUPAL_REVALIDATE_SECRET}&amp;tags=tag1,tag2,tag3\n</code></pre>"},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#manual-invalidation","title":"Manual Invalidation","text":"<pre><code>// Next.js cache tags\nrevalidateTag(\"category-123\")\n</code></pre>"},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#alternatives-considered","title":"Alternatives considered","text":"<p>There are loads of other possibilities in cache land. Other cache stores could be used eg Redis, but of course it introduces another complexity.</p> <p>By sticking to cache handling at the fetching level it should make it easier to understand the flow of cached data. That being said we could also consider caching the FB CMS content at the rendering level, which would prevent logic being executed and thereby improve performance (it is unclear how much though).</p>"},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#consequences","title":"Consequences","text":"<p>By using cache tags we can store the content infinitely which is obviously good for the performance of the FB CMS and the FB CMS GraphQL API, because it makes sure that we have a minimal amount of requests sent to it.</p> <p>NOTE: The configuration cache could be added to the cache tag handling. It would demand invalidation requests sent when ever configuration is updated at the FB CMS - so additional coding is required in both the Go and in the FB CMS application.</p>"},{"location":"DPL-Go/architecture/adr-009-fb-cms-caching/#choosing-use-cache-and-dynamicio","title":"Choosing \"use cache\" and \"dynamicIo\"","text":"<p>In order to make use of the \"use cache\" and cache tags functionality in Nextjs 15 you have to use the Canary versions.</p> <p>It was a bit of a surprise for us because we thought those features came out of the box. Although we did not experience major flaws yet, it is not a nice feeling to choose an ustable track with experimental features that portentially can go wrong in production or be removed entirely in the future.</p> <p>But we thought that the fact that we could make use of cache tags and invalidation outweighted the instability/uncertanties of being on the Canary track.</p>"},{"location":"DPL-Go/guides/create-new-test/","title":"Creating a new Cypress test","text":"<p>The following guide will walk you through the process of creating a new Cypress test, using factories and mocking.</p>"},{"location":"DPL-Go/guides/create-new-test/#steps","title":"Steps","text":"<ol> <li>Create a new test file in the <code>cypress/e2e</code> directory and give it a descriptive name.</li> <li>Add a <code>describe</code> block with a descriptive name for the test section.</li> <li>Then add a <code>beforeEach</code> block to set up the test environment, this should include a <code>cy.visit</code> to the page you want to test.</li> <li>Run <code>yarn cypress:open</code> to open the Cypress test runner.</li> <li>Select the test file you just created and run it.</li> <li>The chosen page path should load, but might fail, as it probably requires some mocking.</li> <li>Look in the terminal for any unhandled request thrown by MockTTP - these are the server side requests which needs to be mocked.</li> <li>Look for any failed network requests inside Cypress - these are the client side requests which needs to be mocked.</li> <li>Create a new factory in <code>cypress/factories/#matching-service-name#/#data-name#.ts</code>.</li> <li>Use a matching data type for the factory. This would often come from our generated graphql types in <code>/lib/graphql/generated/fbi/graphql</code>.</li> <li>Create your mocked data in the factory, and make sure it matches the incoming data type.</li> <li>Then we can import the factory in the test file and use it to mock the request. This is done by using <code>cy.mockServerGraphQLQuery</code> or <code>cy.mockServerRest</code>.</li> <li>Run the test again, and look in the Cypress terminal, you should see the requests being intercepted by MockTTP.</li> <li>Now for the client side requests, we can do the exact same thing, but use <code>cy.interceptGraphQL</code> or <code>cy.interceptRest</code> to mock the client side request.</li> <li>Continue until all requests are mocked, and the test passes.</li> <li>We now have all the required data to run render the page, and we can continue adding tests for functionality and user interaction.</li> </ol>"},{"location":"DPL-Go/guides/create-new-test/#example","title":"Example","text":"<pre><code>describe(\"This block has multiple tests\", () =&gt; {\n  beforeEach(() =&gt; {\n    // Visit the page for each test\n    cy.visit(\"/some-path\")\n\n    // Mock the required data for the page, here it will be set for each test\n    // intercept server side graphql request\n    cy.mockServerGraphQLQuery({\n      operationName: \"someDefinedOperationName\",\n      data: someDefinedFactory.build(),\n    })\n\n    // intercept client side graphql request\n    cy.interceptGraphQL({\n      operationName: \"someOtherDefinedOperationName\",\n      data: someOtherDefinedFactory.build(),\n    })\n  })\n\n  it(\"This test does something specific\", () =&gt; {\n    // Do some testing on the page\n    cy.get(\"some-element\").should(\"be.visible\")\n  })\n\n  it(\"This test does something else, but with different mocked data\", () =&gt; {\n    // override mocked data for this test\n    cy.mockServerGraphQLQuery({\n      operationName: \"someOtherDefinedOperationName\",\n      data: someOtherDefinedFactory.build({\n        someKey: \"someValue\",\n        someOtherKey: \"someOtherValue\",\n      }),\n    })\n\n    // Do some testing on the page\n    cy.get(\"some-other-element\").should(\"be.visible\")\n  })\n})\n</code></pre>"},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/","title":"Runbook: Git Branch Strategy and workflow for Development and Demoing","text":""},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/#overview","title":"Overview","text":"<p>This runbook documents the feature development and demoing Git workflow for the GO project, including branch creation, pull request process, tagging, and deployment procedures for testing environments.</p>"},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/#when-to-use-this-runbook","title":"When to Use This Runbook","text":"<ul> <li>Starting work on a new feature or bug fix</li> <li>Creating releases for customer testing</li> <li>Deploying to test environments (playground, demo)</li> <li>Managing feature integration back to develop branch</li> </ul>"},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to GO repository</li> <li>JIRA access for ticket creation/management</li> <li>Access to DPL-CMS repository for deployment coordination</li> <li>Lagoon CLI access for manual deployments</li> <li>Understanding of Git branching workflows</li> </ul>"},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/#workflow-overview","title":"Workflow Overview","text":"<ol> <li>Create JIRA task</li> <li>Create feature branch</li> <li>Develop</li> <li>Create PR</li> <li>Create release tag</li> <li>Update DPL-CMS</li> </ol>"},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/#step-by-step-procedure","title":"Step-by-Step Procedure","text":""},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/#step-1-create-jira-task","title":"Step 1: Create JIRA Task","text":"<ol> <li>Navigate to JIRA project</li> <li>Create a new task with format: <code>DDF-[number]-[brief-description]</code></li> <li>Fill in task details and acceptance criteria</li> <li>Assign to appropriate developer</li> </ol> <p>Example: <code>DDF-123-test-issue</code></p>"},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/#step-2-create-feature-branch","title":"Step 2: Create Feature Branch","text":"<ol> <li>Ensure you're on the latest develop branch:</li> </ol> <pre><code>git checkout develop\ngit pull origin develop\n</code></pre> <ol> <li>Create new feature branch from develop:</li> </ol> <pre><code>git checkout -b DDF-123-test-issue\n</code></pre> <ol> <li>Push the new branch to remote:</li> </ol> <pre><code>git push -u origin DDF-123-test-issue\n</code></pre>"},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/#step-3-develop-feature","title":"Step 3: Develop Feature","text":"<ol> <li>Implement the feature or fix</li> <li>Make regular commits with descriptive messages</li> <li>Follow existing code patterns and conventions</li> <li>Ensure code quality and testing standards</li> </ol>"},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/#step-4-create-pull-request","title":"Step 4: Create Pull Request","text":"<ol> <li>Push final changes to feature branch:</li> </ol> <pre><code>git push origin DDF-123-test-issue\n</code></pre> <ol> <li>Create pull request from feature branch to <code>develop</code></li> <li>Fill in PR description with:</li> <li>Link to JIRA ticket</li> <li>Description of changes</li> <li>Testing instructions</li> <li> <p>Any breaking changes or considerations</p> </li> <li> <p>Request code review from team members</p> </li> <li>Wait for PR approval</li> </ol>"},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/#step-5-create-release-tag-for-testing","title":"Step 5: Create Release Tag for Testing","text":"<p>After PR approval, create a release tag for customer testing:</p> <ol> <li>Tag Naming Convention: <code>[environment]-[year][month][day].[sequence_number]</code></li> </ol> <p>Available Test Environments:    - <code>playground</code> - Playground testing environment    - <code>demo</code> - Demo testing environment</p> <ol> <li>Create the tag from your feature branch:</li> </ol> <pre><code>git tag playground-20250829.1\ngit push origin playground-20250829.1\n</code></pre> <p>Examples:    - <code>demo-20250701.1</code>    - <code>playground-20250701.1</code>    - <code>playground-20250701.2</code> (if second release same day)</p>"},{"location":"DPL-Go/guides/git-branch-development-demo-workflow/#step-6-update-dpl-cms-with-release-tag","title":"Step 6: Update DPL-CMS with Release Tag","text":"<ol> <li>Navigate to DPL-CMS repository</li> <li>Switch to the appropriate environment branch:</li> <li>For playground: <code>dpl-go-reload-playground</code></li> <li> <p>For demo: <code>dpl-go-demo</code></p> </li> <li> <p>Open <code>node.dockerfile</code></p> </li> <li>Update the <code>dpl-go-node</code> version number with your release tag</li> <li>Commit and push the change:</li> </ol> <pre><code>git add node.dockerfile\ngit commit -m \"Update dpl-go-node to [tag-name]\"\ngit push origin [environment-branch]\n</code></pre>"},{"location":"DPL-Go/guides/grafana-lookups/","title":"Grafana Lookups Runbook","text":""},{"location":"DPL-Go/guides/grafana-lookups/#overview","title":"Overview","text":"<p>This runbook describes the various Grafana queries that can be used in order to identify errors or troubleshoot things gone wrong.</p>"},{"location":"DPL-Go/guides/grafana-lookups/#synchronisation-problems-between-bnf-delingstjenesten-and-library-sites","title":"Synchronisation problems between BNF (Delingstjenesten) and library sites","text":""},{"location":"DPL-Go/guides/grafana-lookups/#see-all-bnf-actions-on-a-specific-library-sites-filtered-by-uuid","title":"See all bnf actions on a specific library sites filtered by UUID","text":"<p>If you want to see what happened to a certain piece of conntent you can filter it like this:</p> <pre><code>{app=\"cli-persistent\", namespace=\"herning-main\"} | json | subtype = `bnf` | message =~ `.*9dfc9d54-7087-41d5-ae11-469aa003c262.*`\n</code></pre> <p>Grafana link</p>"},{"location":"DPL-Go/guides/grafana-lookups/#cache-revalidation-monitoring","title":"Cache Revalidation Monitoring","text":""},{"location":"DPL-Go/guides/grafana-lookups/#track-revalidation-of-specific-piece-of-content-on-a-specific-site","title":"Track revalidation of specific piece of content on a specific site","text":"<p>If you want to see if a library site sent a revalidation requests to its Go site, you can query like this:</p> <pre><code>{namespace=\"ingress-nginx\"} | json | path = `/cache/revalidate` | request_query =~ `^tags=node%3A1192.*` | vhost=`www.go.herningbib.dk`\n</code></pre> <p>Grafana Link</p> <p>Note: In the near future it will be possible to follow the UUID as a cache tag as well, when the feature has been merged.</p>"},{"location":"DPL-Go/guides/grafana-lookups/#all-graphql-requests-sent-to-a-specific-library-site","title":"All graphql requests sent to a specific library site","text":"<p>If you want to track all graphql requests sent from Go to a specific library site, you can query like this:</p> <pre><code>{namespace=\"ingress-nginx\"} | json | path = `/graphql` | http_user_agent = `node` | vhost=`nginx.main.silkeborg.dplplat01.dpl.reload.dk`\n</code></pre> <p>Grafana Link</p>"},{"location":"DPL-Go/guides/grafana-lookups/#follow-build-process-of-a-site-being-deployed","title":"Follow build process of a site being deployed","text":"<p>The Lagoon deployment UI is ok. But if you want to keep track of everything happening in the deployment process you can query like this:</p> <pre><code>{namespace=\"dpl-cms-pr-1707\", container=\"lagoon-build\"}\n</code></pre> <p>Grafana Link</p>"},{"location":"DPL-Go/guides/grafana-lookups/#general-query-patterns","title":"General Query Patterns","text":""},{"location":"DPL-Go/guides/grafana-lookups/#label-selectors","title":"Label Selectors","text":"<ul> <li><code>{app=\"...\", namespace=\"...\"}</code> - Target specific applications in specific namespaces</li> <li><code>{namespace=\"...\", container=\"...\"}</code> - Target specific containers within a namespace</li> </ul>"},{"location":"DPL-Go/guides/grafana-lookups/#log-processing","title":"Log Processing","text":"<ul> <li><code>| json</code> - Parse JSON-formatted log entries</li> <li><code>| message =~ \"pattern\"</code> - Filter by message content using regex</li> <li><code>| path = \"/endpoint\"</code> - Filter by HTTP path</li> <li><code>| http_user_agent = \"agent\"</code> - Filter by user agent</li> <li><code>| vhost=\"domain\"</code> - Filter by virtual host</li> </ul>"},{"location":"DPL-Go/guides/lagoon-environment-variables/","title":"Lagoon Environment Variables Runbook","text":""},{"location":"DPL-Go/guides/lagoon-environment-variables/#overview","title":"Overview","text":"<p>This runbook provides instructions for retrieving and managing environment variables for Lagoon projects using the Lagoon CLI. This is essential for debugging configuration issues or setting up local development environments.</p>"},{"location":"DPL-Go/guides/lagoon-environment-variables/#when-to-use-this-runbook","title":"When to Use This Runbook","text":"<ul> <li>Debugging environment-specific configuration issues</li> <li>Setting up local development environment to match production</li> <li>Verifying environment variable values</li> </ul>"},{"location":"DPL-Go/guides/lagoon-environment-variables/#prerequisites","title":"Prerequisites","text":"<ul> <li>Lagoon CLI installed and configured</li> <li>Valid authentication with Lagoon platform</li> </ul>"},{"location":"DPL-Go/guides/lagoon-environment-variables/#step-by-step-procedure","title":"Step-by-Step Procedure","text":""},{"location":"DPL-Go/guides/lagoon-environment-variables/#step-1-list-environment-variables","title":"Step 1: List Environment Variables","text":"<ol> <li>Open terminal</li> <li>Run the following command to retrieve environment variables:</li> </ol> <pre><code>lagoon list variables -p [PROJECT_NAME] --reveal\n</code></pre> <p>Replace <code>[PROJECT_NAME]</code> with the actual project name</p>"},{"location":"DPL-Go/guides/lagoon-environment-variables/#step-2-review-output","title":"Step 2: Review Output","text":"<ol> <li>The command will display all environment variables for the project</li> <li>Variables will be shown with their actual values (due to <code>--reveal</code> flag)</li> </ol>"},{"location":"DPL-Go/guides/lagoon-environment-variables/#examples","title":"Examples","text":""},{"location":"DPL-Go/guides/lagoon-environment-variables/#basic-usage","title":"Basic Usage","text":"<p>If you need to check variables across multiple projects:</p> <pre><code># Check pr environment variables\nlagoon list variables -p dpl-cms --reveal\n</code></pre> <p>Or a specific environment:</p> <pre><code># Check kobenhavn production (main) environment\nlagoon list variables -p kobenhavn -e main --reveal\n</code></pre>"},{"location":"DPL-Go/guides/nextjs-cache-invalidation/","title":"NextJS Cache Invalidation Runbook","text":""},{"location":"DPL-Go/guides/nextjs-cache-invalidation/#overview","title":"Overview","text":"<p>This runbook provides step-by-step instructions for invalidating NextJS cache in the GO application when content changes in DPL-CMS are not reflecting on the frontend.</p>"},{"location":"DPL-Go/guides/nextjs-cache-invalidation/#when-to-use-this-runbook","title":"When to Use This Runbook","text":"<ul> <li>Content changes in DPL-CMS are not appearing on the GO frontend</li> <li>Specific pages or content sections appear stale</li> </ul>"},{"location":"DPL-Go/guides/nextjs-cache-invalidation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to the GO site's cache invalidation endpoint</li> <li>Knowledge of the specific content that needs cache invalidation</li> <li>Cache invalidation secret for the environment</li> </ul>"},{"location":"DPL-Go/guides/nextjs-cache-invalidation/#step-by-step-procedure","title":"Step-by-Step Procedure","text":""},{"location":"DPL-Go/guides/nextjs-cache-invalidation/#step-1-identify-cache-tags","title":"Step 1: Identify Cache Tags","text":"<ol> <li>Make a request to the DPL-CMS <code>/graphql</code> endpoint for the content you want to    invalidate.</li> <li>Check the response headers for <code>x-dpl-graphql-cache-tags</code></li> <li>Look for cache tags that start with <code>node:[id]</code> where <code>[id]</code> is the content identifier</li> </ol> <p>When you want to get the cache tags you can find a query by either looking through the dpl-go repository for queries or construct one by using the graphql explorer tool in dpl-cms. Here is a query that probably will not change in the future:</p> <pre><code>query getDplCmsPublicConfiguration {\n  goConfiguration {\n    public {\n      libraryInfo {\n        name\n      }\n    }\n  }\n}\n</code></pre> <p>Example Response Header:</p> <pre><code>x-dpl-graphql-cache-tags: node:597,category:library,content:article\n</code></pre>"},{"location":"DPL-Go/guides/nextjs-cache-invalidation/#step-2-invalidate-cache","title":"Step 2: Invalidate Cache","text":"<ol> <li>Construct the cache invalidation URL:</li> </ol> <pre><code>POST /cache/invalidate?tags=node:[id]&amp;secret=[secret_key]\n</code></pre> <ol> <li>Make a POST request to the GO site's cache invalidation endpoint</li> </ol> <p>Example for T\u00e5rnby Library:</p> <pre><code>curl -X POST \"https://go.taarnbybib.dk/cache/revalidate?secret=JYTgIdNLkl3RBip7zJAW3K1qqiAb7fWmp2M5NgoMrowPq2u3iFhZi5t9i2Y53j32askdnasdj/w17chieeYvw==&amp;tags=node:597\"\n</code></pre>"},{"location":"DPL-Go/guides/nextjs-cache-invalidation/#step-3-verify-success","title":"Step 3: Verify Success","text":"<ol> <li>Check the response status code</li> <li>A <code>200</code> status code indicates successful cache invalidation</li> <li>Verify the content change is now visible on the frontend</li> </ol>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/","title":"Unilogin &amp; Publizon_- models and relations","text":""},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#session-handling","title":"Session handling","text":"<pre><code>erDiagram\n  UniloginAccessToken {\n      string accessToken\n      int expires\n      string refreshToken\n      int values__refresh_expires_in\n      int values__id_token\n  }</code></pre>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#dinguniloginuser","title":"DingUniloginUser","text":"<pre><code>erDiagram\n    DingUniloginUser {\n        string name\n        string authname\n        string wsuser\n        string wspass\n        string_array institutionsIds\n        int_array municipalityIds\n        object_array institutions\n    }</code></pre> <p><code>name</code> is the same as the Unilogin username</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#publizon-libraries-local-db-table","title":"Publizon Libraries (local db table)","text":"<pre><code>erDiagram\n    publizon_libraries {\n      string retailer_id\n      string library_name\n      string unilogin_id\n      string retailer_key_code\n      string subscribed_users\n      string municipality_id\n    }</code></pre>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#institutions-og-municipalities","title":"Institutions og municipalities","text":"<pre><code>erDiagram\n    institution {\n      string instnr\n      string instnavn\n      string type\n      string typenavn\n      string adresse\n      string bynavn\n      string postnr\n      string telefonnr\n      string mailadresse\n      string www\n      int kommunenr\n    }</code></pre>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#institutionids","title":"institutionIds","text":"<p>Institution ids are collected via the <code>DingUniloginWSIbruger</code> client via <code>hentBrugersInstitutionsTilknytninger</code> SOAP service</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#municipalitiesids","title":"municipalitiesIds","text":"<p>Municipalities is only represented as id's.</p> <p>Municipalities are fetched via the <code>DingUniloginUser::getInstitutionMunicipalities(institutionIds)</code> method:</p> <ul> <li>Fake municipalities are established via <code>_ding_unilogin_get_nonstitution_municipalities</code> that gets its data from: <code>_ding_unilogin_get_nonstitutions</code> that gets the institutions from a setting (variable) called: <code>ding_unilogin_nonstitutions</code></li> <li>InstitutionIds are iterated and every institution is fetched by the <code>DingUniloginWSIinst</code> client via the <code>hentInstitution</code> service</li> <li>If a fake institution exists with current institutionId then use the <code>kommunenr</code> from that</li> <li>(side effect) <code>DingUniloginUser.institutions</code> are being populated in the iteration with the institution</li> <li><code>insititution.kommunenr</code> is added to the <code>municipalityIds</code> array</li> <li>The <code>municipalityIds</code> array is set on <code>DingUniloginUser.municipalityIds</code></li> <li>The <code>municipalityIds</code> array is being returned</li> </ul>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#librarylibraries","title":"Library/libraries","text":"<pre><code>erDiagram\n    PublizonConfiguredLibrary {\n      int retailer_id\n      string library_name\n      string unilogin_id\n      string municipality_id\n      string retailer_key_code\n      string subscribed_users\n    }</code></pre> <p>Libraries (<code>PublizonConfiguredLibrary</code>) are stored in the db in the table: <code>publizon_libraries</code> they are fetched by the function: <code>publizon_get_libraries()</code> which keys them by <code>retailer_id</code>.</p> <p>Afterwards the libraries are being filtered by the municipalities connected to the user.</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#the-usagemeaning-of-a-library","title":"The usage/meaning of a library","text":"<p>If there is multiple libraries connected to the user, only the first one is being used.</p> <p>The unilogin_id from the library is being tied to the Drupal user and is being used in future contexts.</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#services","title":"Services","text":""},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#unilogin-services","title":"Unilogin services","text":""},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#base-class-dinguniloginservicebase","title":"Base class: DingUniloginServiceBase","text":"<p>Uses two methods: <code>call</code> and <code>callWithAuth</code> depending on if the request needs authentication</p> <p><code>callWithAuth</code> uses wsuser and wspasswd tied to the <code>DingUniloginUser</code> object</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#dinguniloginwsibruger-unilogin-api","title":"DingUniloginWSIbruger (Unilogin API)","text":""},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#wsdl-url","title":"WSDL Url","text":"<pre><code>https://wsibruger.unilogin.dk/wsibruger-v6/ws?WSDL\n</code></pre>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#actions","title":"Actions","text":"<p>hentBrugersInstitutionsTilknytninger: Finds institutions connected to a user.</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#dinguniloginwsiinst-unilogin-api","title":"DingUniloginWSIinst (Unilogin API)","text":""},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#wsdl-url_1","title":"WSDL Url","text":"<pre><code>https://wsiinst.unilogin.dk/wsiinst-v5/ws?WSDL\n</code></pre>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#actions_1","title":"Actions","text":"<p>hentInstitution: Loads a institution given its id.</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#publizon-services","title":"Publizon services","text":""},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#base-class-publizonclient","title":"Base class: PublizonClient","text":"<p>Makes sure that following params are used as well:</p> <ul> <li>':languagecode': empty string</li> <li>':clientid': publizon client id</li> <li>':retailerid': retailer id tied to user (from library)</li> <li>':retailerkeycode': retailer id tied to user (from library)</li> </ul>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#publizonuserclient-publizon-api","title":"PublizonUserClient (Publizon API)","text":""},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#methods","title":"Methods","text":""},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#getsupportid","title":"getSupportId","text":"<p>Endpoint: get_friendly_cardnumber</p> <p>Action: GetFriendlyCardnumber</p> <p>Params: <code>pub:cardnumber</code> (uniid), <code>retailer_id</code> (from library)</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#publizonloanclient-publizon-api","title":"PublizonLoanClient (Publizon API)","text":""},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#methods_1","title":"Methods","text":""},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#createloan","title":"createLoan","text":"<p>Endpoint: createloan.asmx</p> <p>Action: CreateLoan</p> <p>Params: <code>pub:ebookid</code> (isbn), <code>retailer_id</code> (from library), <code>pub:cardnumber</code> (uniid (username)), <code>pub:pincode</code> (empty), <code>pub:institutionid</code>, <code>pub:format</code> (empty string), <code>pub:mobipocketid</code> (empty string), <code>pub:institutionTags</code> (array of <code>pub:string</code> =&gt; $institution_tags (often empty))</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#getlibraryuserorderlist","title":"getLibraryUserOrderList","text":"<p>Endpoint: getlibraryuserorderlist.asmx</p> <p>Action: GetLibraryUserOrderList</p> <p>Params: <code>retailer_id</code> (from library), <code>pub:cardnumber</code> (uniid (username))</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#login-flows","title":"Login flows","text":""},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#unilogin","title":"Unilogin","text":"<p>Login flow at Unilogin</p> <pre><code>sequenceDiagram\n    participant US as Unilogin Services\n    participant EG as Ereolen GO\n\n    EG-&gt;&gt;US: User clicks login at Ereolen GO\n    US-&gt;&gt;EG: User authenticated\n    EG--&gt;&gt;US: Get access token *1\n    EG--&gt;&gt;US: Get user info\n    Note over EG: Access token sent as argument\n    Note over US: Most important data her is: uniid\n    EG-&gt;&gt;EG: DingUniloginUser instantiated\n    Note over EG: Username = Unilogin uniid\n    EG--&gt;&gt;US: Get institution ids tied to the user\n    EG--&gt;&gt;EG: Get Nonstitution Municipalities\n    Note over EG: Municipalites are stored in a variable called:&lt;br/&gt; ding_unilogin_nonstitutions *2\n    EG--&gt;&gt;EG: An array of municpalities is tied to the user *3\n    EG--&gt;&gt;US: Get Institution\n    Note over EG: For each institution ids tied to the user\n    EG--&gt;&gt;EG: Get library\n    Note over EG: Lookup in local DB table (\"publizon_libraries\") *4\n    EG--&gt;&gt;EG: Ding User Authenticate\n    Note over EG: library.unilogin_id is connected to the&lt;br/&gt; Ding user object in the extra.attributes.unilogin property\n    EG-&gt;&gt;EG: publizon_auth_single_sign_on\n    Note over EG: Retailer id and support id is added to Drupal user.data\n\n</code></pre>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#1","title":"1)","text":"<p>It seems like the access token is primarily used to fetch userinfo about user in not in subsequent calls like: getUserLoans performLoan etc.</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#2","title":"2)","text":"<p>Only one nonstitution municipality is stored:</p> <pre><code>ding_unilogin_nonstitutions:\n  A04441:\n    institution_id: A04441\n    municipality_kommunenr: '101'\n    municipality_kommune: '101'\n</code></pre> <p>The municipality is represented in the <code>publizon_libraries</code> table:</p> <pre><code>+-------------+--------------+-------------+-------------------+------------------+-----------------+\n| retailer_id | library_name | unilogin_id | retailer_key_code | subscribed_users | municipality_id |\n+-------------+--------------+-------------+-------------------+------------------+-----------------+\n| 810         | K\u00f8benhavn    | 101         | xxxxxxxx          | 26915            | 101             |\n+-------------+--------------+-------------+-------------------+------------------+-----------------+\n</code></pre>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#3","title":"3)","text":"<p>For every institution stored on user there is a <code>kommunenr</code> and is collected into a municipality array:</p> <pre><code> $municipalityIds[] = $institution-&gt;kommunenr;\n</code></pre> <p>However if an user institution is represented in <code>ding_unilogin_nonstitutions</code> the <code>kommunenr</code> is used from that.</p>"},{"location":"DPL-Go/research/ereolengo/unilogin_publizon_models_and_relations/#4","title":"4)","text":"<p>Get library by looking at the kommunenr of the first institution of the user</p>"}]}